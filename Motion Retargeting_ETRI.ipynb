{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import plotly\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SGN.model import SGN\n",
    "from SGN.data import NTUDataLoaders, AverageMeter\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import mlflow\n",
    "import time\n",
    "_=mlflow.set_experiment(\"Privacy Retargeting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "only_use_pos = True # True uses SGN preprocessing, False uses my preprocessing\n",
    "remove_two_actor_actions = True\n",
    "one_dimension_conv = False\n",
    "ntu_120 = True\n",
    "only_ntu_120 = False\n",
    "seperate_train_test = True\n",
    "sgn_eval_after_each_stage = False\n",
    "binary_data = False\n",
    "train_cameras = [2,3,5,7]\n",
    "test_cameras = [1,4,6,8]\n",
    "train_actors = [84,24,51,12,66,58,1,56,54,62,22,42,30,64,\n",
    "                     41,69,3,80,21,35,90,89,63,46,32,2,26,72,\n",
    "                     50,91,16,57,36,71,31,59,78,53,9,27,7,95,\n",
    "                     4,83,65,48,75,5,44,100]\n",
    "T = 75\n",
    "k = 5\n",
    "setting = 'cv'\n",
    "dataset = 'NTU'\n",
    "metric = 'val_utility_acc_coop'\n",
    "matric_minimize = False\n",
    "device = torch.device('cuda:0')\n",
    "seg = 20\n",
    "lr = 1e-5\n",
    "adv_lr = 1e-5\n",
    "util_classifier_alpha = 10\n",
    "priv_classifier_alpha = .1\n",
    "if ntu_120:\n",
    "    utility_classes = 120\n",
    "    privacy_classes = 106\n",
    "else:\n",
    "    utility_classes = 60\n",
    "    privacy_classes = 40\n",
    "validation_acc_freq = -1 #-1 to disable\n",
    "emb_clf_update_per_epoch_paired = 1\n",
    "emb_clf_update_per_epoch_unpaired = 3\n",
    "encoded_channels = (128, 16) # default\n",
    "dmr_encoded_channels = (256, 32) # dmr\n",
    "batch_size = 32\n",
    "workers=0\n",
    "cross_samples_train = 50000\n",
    "cross_samples_test = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate parameters\n",
    "assert len(train_cameras) > 0 and len(test_cameras) > 0\n",
    "assert emb_clf_update_per_epoch_paired > 0 and emb_clf_update_per_epoch_unpaired > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Organize\n",
    "\n",
    "X = (frames, joints, pos + orientation)\n",
    "    \n",
    "    (frames, 25, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# if only_use_pos:\n",
    "#     with open('ntu/SGN/X_full.pkl', 'rb') as f:\n",
    "#         X = pickle.load(f)\n",
    "# else:\n",
    "#     with open('ntu/X.pkl', 'rb') as f:\n",
    "#         X = pickle.load(f)\n",
    "\n",
    "with open('ETRI.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "# pad/trim data to T frames and convert to tensor\n",
    "for file, value in X.items():\n",
    "    # If SGN preprocessing, remove zero padding\n",
    "    if only_use_pos:\n",
    "        first_zero_index = value.shape[0]\n",
    "        for i in range(value.shape[0]):\n",
    "            if np.all(value[i] == 0):\n",
    "                first_zero_index = i\n",
    "                break\n",
    "        value = value[:first_zero_index]\n",
    "\n",
    "    num_frames = value.shape[0]\n",
    "\n",
    "    # Pad or trim\n",
    "    if num_frames < T:\n",
    "        if only_use_pos: padding = np.repeat(value[-1][np.newaxis, :], T - num_frames, axis=0)\n",
    "        else: padding = np.repeat(value[-1][np.newaxis, :, :], T - num_frames, axis=0)\n",
    "        value = np.concatenate((value, padding), axis=0)\n",
    "    elif num_frames > T:\n",
    "        # Randomly sample T frames\n",
    "        start = random.randint(0, num_frames - T)\n",
    "        value = value[start:start+T]\n",
    "    \n",
    "    # Convert to tensor and store back\n",
    "    X[file] = torch.from_numpy(value).float()\n",
    "\n",
    "\n",
    "# chop off second actor and convert to 3d\n",
    "if only_use_pos:\n",
    "    for file, value in X.items():\n",
    "        value = value[:, :75]\n",
    "        value = value.view(-1, 25, 3)\n",
    "        X[file] = value\n",
    "\n",
    "# remove two actor actions\n",
    "# two_action_files = set([50,51,52,53,54,55,56,57,58,59,60,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120])\n",
    "# if remove_two_actor_actions:\n",
    "#     to_rem = []\n",
    "#     for file in X.keys():\n",
    "#         if int(str(file).split('A')[1][:3]) in two_action_files:\n",
    "#             to_rem.append(file)\n",
    "#     for file in to_rem:\n",
    "#         del X[file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Data Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_name(file_name):\n",
    "    \"\"\"Parses the filename into a dictionary of parts.\"\"\"\n",
    "    file_name = str(file_name)\n",
    "    if file_name[0] == 'b': # SGN preprocessing\n",
    "        A = int(file_name[3:6])\n",
    "        P = int(file_name[7:10])\n",
    "        G = int(file_name[11:14])\n",
    "        C = int(file_name[15:18])\n",
    "    else:\n",
    "        pass\n",
    "        # S = int(file_name[1:4])\n",
    "        # C = int(file_name[5:8])\n",
    "        # P = int(file_name[9:12])\n",
    "        # R = int(file_name[13:16])\n",
    "        # A = int(file_name[17:20])\n",
    "    return {'C': C, 'P': P, 'G': G, 'A': A}\n",
    "\n",
    "def organize_data(data):\n",
    "    train_data = defaultdict(list)\n",
    "    test_data = defaultdict(list)\n",
    "\n",
    "    organized_data = defaultdict(list)\n",
    "    for file_name, content in data.items():\n",
    "        parts = parse_file_name(file_name)\n",
    "        if not ntu_120: # NTU 60\n",
    "            if int(parts['A']) > 60:\n",
    "                continue\n",
    "        organized_data[parts['C']].append((parts['P'], parts['A'], content))\n",
    "        if setting == 'cs':\n",
    "            if parts['P'] in train_actors:\n",
    "                train_data[parts['C']].append((parts['P'], parts['A'], content))\n",
    "            else:\n",
    "                test_data[parts['C']].append((parts['P'], parts['A'], content))\n",
    "\n",
    "        \n",
    "    if setting == 'cv':\n",
    "        for camera in train_cameras:\n",
    "            train_data[camera].extend(organized_data[camera])\n",
    "\n",
    "        for camera in test_cameras:\n",
    "            test_data[camera].extend(organized_data[camera])\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def sample_data(organized_data):\n",
    "    # Pick a random C pair\n",
    "    C = random.choice(list(organized_data.keys()))\n",
    "\n",
    "    # Get all (P, A, content) tuples for this C \n",
    "    pa_list = organized_data[C]\n",
    "\n",
    "    # Pick 2 unique P values, find two overlapping A's\n",
    "    \n",
    "\n",
    "    # Pick 2 unique P values and 2 unique A values\n",
    "    random.shuffle(pa_list)\n",
    "    unique_p = set()\n",
    "    unique_a = set()\n",
    "    for p, a, _ in pa_list:\n",
    "        if len(unique_p) < 2:\n",
    "            unique_p.add(p)\n",
    "        if len(unique_a) < 2:\n",
    "            unique_a.add(a)\n",
    "        if len(unique_p) == 2 and len(unique_a) == 2:\n",
    "            break\n",
    "\n",
    "    if len(unique_p) < 2 or len(unique_a) < 2:\n",
    "        raise Exception(f'Not enough unique P or A values for C pair {C}')\n",
    "\n",
    "    # Form all four (P, A) pairs and get the corresponding content\n",
    "    sampled_data = [] #(p1, a1) (p1, a2) (p2, a1) (p2, a2)\n",
    "    for p in unique_p:\n",
    "        for a in unique_a:\n",
    "            for pa_content in pa_list:\n",
    "                if pa_content[0] == p and pa_content[1] == a:\n",
    "                    sampled_data.append(pa_content)\n",
    "                    break\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "def gen_samples(samples, data):\n",
    "    d = []\n",
    "    unique_samples = set()  # Set to track unique samples\n",
    "    for _ in range(samples):\n",
    "        failed = 0\n",
    "        while True:\n",
    "            d_ = sample_data(data)\n",
    "            d_tuple = tuple(tuple(x) for x in d_)\n",
    "            if d_tuple not in unique_samples and len(d_tuple) == 4:\n",
    "                unique_samples.add(d_tuple)  # Add the unique sample to the set\n",
    "                d.append(d_)  # Add the unique sample to the dataset\n",
    "                break\n",
    "            failed += 1\n",
    "            if failed > 100:\n",
    "                print('failed to sample data')\n",
    "                break\n",
    "    return np.array(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rec_data(X):\n",
    "    # Remove NTU 120 if needed\n",
    "    if not ntu_120:\n",
    "        X = {k: v for k, v in X.items() if int(parse_file_name(k)['A']) <= 60}\n",
    "\n",
    "    # Split data into train and test\n",
    "    # X_train_keys, X_test_keys = train_test_split(list(X.keys()), test_size=0.2, random_state=42)\n",
    "\n",
    "    # Split by camera views\n",
    "    X_train_keys = []\n",
    "    X_test_keys = []\n",
    "    if setting == 'cs':\n",
    "        for key in X.keys():\n",
    "            if parse_file_name(key)['P'] in train_actors:\n",
    "                X_train_keys.append(key)\n",
    "            else:\n",
    "                X_test_keys.append(key)\n",
    "    elif setting == 'cv':\n",
    "        for key in X.keys():\n",
    "            if parse_file_name(key)['C'] in train_cameras:\n",
    "                X_train_keys.append(key)\n",
    "            else:\n",
    "                X_test_keys.append(key)\n",
    "    \n",
    "    # Create train and test sets\n",
    "    X_train = np.zeros((len(X_train_keys), T, 25, 3 if only_use_pos else 7))\n",
    "    X_test = np.zeros((len(X_test_keys), T, 25, 3 if only_use_pos else 7))\n",
    "    for i, key in enumerate(X_train_keys):\n",
    "        X_train[i] = X[key]\n",
    "    for i, key in enumerate(X_test_keys):\n",
    "        X_test[i] = X[key]\n",
    "\n",
    "    # Get actor and action names\n",
    "    train_actors = [parse_file_name(key)['P'] for key in X_train_keys]\n",
    "    test_actors = [parse_file_name(key)['P'] for key in X_test_keys]\n",
    "    train_actions = [parse_file_name(key)['A'] for key in X_train_keys]\n",
    "    test_actions = [parse_file_name(key)['A'] for key in X_test_keys]\n",
    "    \n",
    "    return X_train, X_test, train_actors, train_actions, test_actors, test_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (50000, 4, 3) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Cross Data\u001b[39;00m\n\u001b[1;32m     40\u001b[0m organized_data_train, organized_data_test \u001b[38;5;241m=\u001b[39m organize_data(X)\n\u001b[0;32m---> 41\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mgen_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_samples_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganized_data_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seperate_train_test: val_data \u001b[38;5;241m=\u001b[39m gen_samples(cross_samples_test, organized_data_test)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: val_data \u001b[38;5;241m=\u001b[39m train_data\n",
      "Cell \u001b[0;32mIn[5], line 97\u001b[0m, in \u001b[0;36mgen_samples\u001b[0;34m(samples, data)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to sample data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (50000, 4, 3) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "class Cross_Data(Dataset):\n",
    "    def __init__(self, sampled_data):\n",
    "        self.data = sampled_data # the tuple is actor, action, frames\n",
    "        self.x1 = sampled_data[:, 0, 2] # P1, A1\n",
    "        self.x2 = sampled_data[:, 3, 2] # P2, A2\n",
    "        self.y1 = sampled_data[:, 1, 2] # P1, A2\n",
    "        self.y2 = sampled_data[:, 2, 2] # P2, A1\n",
    "        self.none = torch.zeros(1)\n",
    "\n",
    "    def __getitem__(self, index): # data, actors, actions\n",
    "        if only_use_pos:\n",
    "            return self.x1[index], self.none,\\\n",
    "                    self.x2[index], self.none,\\\n",
    "                    self.y1[index], self.none,\\\n",
    "                    self.y2[index], self.none,\\\n",
    "                    [float(self.data[index][0][0]), float(self.data[index][3][0])], [float(self.data[index][0][1]), float(self.data[index][3][1])]\n",
    "        return self.x1[index][:, :, 0:3], self.x1[index][:, :, 3:7],\\\n",
    "                self.x2[index][:, :, 0:3], self.x2[index][:, :, 3:7],\\\n",
    "                self.y1[index][:, :, 0:3], self.y1[index][:, :, 3:7],\\\n",
    "                self.y2[index][:, :, 0:3], self.y2[index][:, :, 3:7],\\\n",
    "                [float(self.data[index][0][0]), float(self.data[index][3][0])], [float(self.data[index][0][1]), float(self.data[index][3][1])]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class Rec_Data(Dataset):\n",
    "    def __init__(self, X, Actor, Action):\n",
    "        self.X = X\n",
    "        self.Actor = Actor\n",
    "        self.Action = Action\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], float(self.Actor[index]), float(self.Action[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "# Cross Data\n",
    "organized_data_train, organized_data_test = organize_data(X)\n",
    "train_data = gen_samples(cross_samples_train, organized_data_train)\n",
    "if seperate_train_test: val_data = gen_samples(cross_samples_test, organized_data_test)\n",
    "else: val_data = train_data\n",
    "train_dataset = Cross_Data(train_data)\n",
    "val_dataset = Cross_Data(val_data)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_ssize=batch_size, shuffle=True)\n",
    "\n",
    "# Rec Data\n",
    "rec_train_data, rec_val_data, t_actors, t_actions, v_actors, v_actions = sample_rec_data(X)\n",
    "rec_train_dataset = Rec_Data(rec_train_data, t_actors, t_actions)\n",
    "rec_val_dataset = Rec_Data(rec_val_data, v_actors, v_actions)\n",
    "rec_train_dl = DataLoader(rec_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "rec_val_dl = DataLoader(rec_val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gather stats on data\n",
    "# def print_data(d):\n",
    "#     print(f'Number of samples: {len(d)}')\n",
    "#     unique_actors = set()\n",
    "#     unique_actions = set()\n",
    "#     for d in d:\n",
    "#         for i in range(4):\n",
    "#             unique_actors.add(d[i][0])\n",
    "#             unique_actions.add(d[i][1])\n",
    "#     print(f'Number of unique actors: {len(unique_actors)}')\n",
    "#     print(f'Number of unique actions: {len(unique_actions)}')\n",
    "# print('Train Data:')\n",
    "# print_data(train_data)\n",
    "# print('Val Data:')\n",
    "# print_data(val_data)\n",
    "# print('Rec Train Data:')\n",
    "# print(len(rec_train_data))\n",
    "# print('Rec Val Data:')\n",
    "# print(len(rec_val_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is size of latent space\n",
    "class Adversary_Emb(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Adversary_Emb, self).__init__()\n",
    "        self.channels = [encoded_channels[0], 128, 256, 512]\n",
    "        self.conv1 = nn.ConvTranspose1d(self.channels[0], self.channels[1], 3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(self.channels[1], self.channels[2], 3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv3 = nn.ConvTranspose1d(self.channels[2], self.channels[3], 3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.channels[1])\n",
    "        self.bn2 = nn.BatchNorm1d(self.channels[2])\n",
    "        self.bn3 = nn.BatchNorm1d(self.channels[3])\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(self.channels[3], 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=0.5, training=self.training)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), p=0.5, training=self.training)\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        # x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module): # 1 = real, 0 = fake\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv1d(in_channels=T, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    "        self.fc1 = nn.Linear(80, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        #flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Retargeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv1d(in_channels=T, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv1d(in_channels=512, out_channels=encoded_channels[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(encoded_channels[0], encoded_channels[0] * encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1) \n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder1D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose1d(in_channels=encoded_channels[0]*2, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(in_channels=96, out_channels=T, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    " \n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder2D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv2d(in_channels=T, out_channels=12, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv2d(in_channels=32, out_channels=encoded_channels[0], kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(encoded_channels[0], encoded_channels[0] * encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder2D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose2d(in_channels=encoded_channels[0]*2, out_channels=256, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(in_channels=128, out_channels=96, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose2d(in_channels=96, out_channels=75, kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(3)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMR_Encoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Encoder1D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv1d(in_channels=T, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv1d(in_channels=512, out_channels=dmr_encoded_channels[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(dmr_encoded_channels[0], dmr_encoded_channels[0] * dmr_encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1) \n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *dmr_encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DMR_Decoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Decoder1D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose1d(in_channels=dmr_encoded_channels[0]*2, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(in_channels=96, out_channels=T, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    " \n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        return x\n",
    "    \n",
    "class DMR_Encoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Encoder2D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv2d(in_channels=T, out_channels=12, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv2d(in_channels=32, out_channels=dmr_encoded_channels[0], kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(dmr_encoded_channels[0], dmr_encoded_channels[0] * dmr_encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *dmr_encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DMR_Decoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Decoder2D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose2d(in_channels=dmr_encoded_channels[0]*2, out_channels=256, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(in_channels=128, out_channels=96, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose2d(in_channels=96, out_channels=75, kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(3)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DMR(nn.Module):\n",
    "    def __init__(self, adv_lr=1e-4, use_adv=False):\n",
    "        super(DMR, self).__init__()\n",
    "\n",
    "        # AutoEncoder Models\n",
    "        if one_dimension_conv:\n",
    "            self.static_encoder = DMR_Encoder1D()\n",
    "            self.advamic_encoder = DMR_Encoder1D()\n",
    "            self.decoder = DMR_Decoder1D()\n",
    "        else:\n",
    "            self.static_encoder = DMR_Encoder2D()\n",
    "            self.dynamic_encoder = DMR_Encoder2D()\n",
    "            self.decoder = DMR_Decoder1D()\n",
    "\n",
    "        # Adversarial Models\n",
    "        self.use_adv = use_adv\n",
    "        if use_adv:\n",
    "            self.priv_adv = Adversary_Emb(privacy_classes).to(device) # input = dynamic embedding, output = privacy class\n",
    "            self.priv_coop = Adversary_Emb(privacy_classes).to(device) # input = static embedding, output = privacy class\n",
    "            self.util_adv = Adversary_Emb(utility_classes).to(device) # input = static embedding, output = utility class\n",
    "            self.util_coop = Adversary_Emb(utility_classes).to(device) # input = dynamic embedding, output = utility class\n",
    "            self.discriminator = Discriminator().to(device)\n",
    "\n",
    "            self.priv_optim = torch.optim.AdamW(self.priv_adv.parameters(), lr=adv_lr)\n",
    "            self.priv_coop_optim = torch.optim.AdamW(self.priv_coop.parameters(), lr=adv_lr)\n",
    "            self.util_optim = torch.optim.AdamW(self.util_adv.parameters(), lr=adv_lr)\n",
    "            self.util_coop_optim = torch.optim.AdamW(self.util_coop.parameters(), lr=adv_lr)\n",
    "            self.discriminator_optim = torch.optim.AdamW(self.discriminator.parameters(), lr=adv_lr)\n",
    "\n",
    "            # Freeze Adversarial Models\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "\n",
    "        # Loss Functions\n",
    "        self.triplet_loss = nn.TripletMarginLoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Info for loss functions\n",
    "        self.end_effectors = torch.tensor([19, 15, 23, 24, 21, 22, 3]).to(device) * 3\n",
    "        self.chain_lengths = torch.tensor([5, 5, 8, 8, 8, 8, 5]).to(device)\n",
    "\n",
    "        # Lambdas for discounted loss\n",
    "        self.lambda_rec = 2\n",
    "        self.lambda_cross = 0.1\n",
    "        self.lambda_ee = 1\n",
    "        self.lambda_smoothing = 3\n",
    "        self.lambda_trip = 1\n",
    "        self.lambda_latent = 10\n",
    "        self.lambda_adv_util_coop = util_classifier_alpha\n",
    "        self.lambda_adv_priv_coop = priv_classifier_alpha\n",
    "        self.lambda_adv_util_adv = util_classifier_alpha\n",
    "        self.lambda_adv_priv_adv = priv_classifier_alpha\n",
    "        self.lambda_adv_disc = 1\n",
    "\n",
    "        # Loss Toggles\n",
    "        self.use_rec_loss = True\n",
    "        self.use_cross_loss = True\n",
    "        self.use_ee_loss = True \n",
    "        self.use_trip_loss_paired = True \n",
    "        self.use_trip_loss_unpaired = True\n",
    "        self.use_smoothing_loss = True\n",
    "        self.use_latent_consistency = True\n",
    "\n",
    "    def get_loss_params(self):\n",
    "        return {\n",
    "            'lambda_rec': self.lambda_rec,\n",
    "            'lambda_cross': self.lambda_cross,\n",
    "            'lambda_ee': self.lambda_ee,\n",
    "            'lambda_trip': self.lambda_trip,\n",
    "            'lambda_latent': self.lambda_latent,\n",
    "            'lambda_adv_util_coop': self.lambda_adv_util_coop,\n",
    "            'lambda_adv_priv_coop': self.lambda_adv_priv_coop,\n",
    "            'lambda_adv_util_adv': self.lambda_adv_util_adv,\n",
    "            'lambda_adv_priv_adv': self.lambda_adv_priv_adv,\n",
    "            'lambda_adv_disc': self.lambda_adv_disc,\n",
    "            'use_rec_loss': self.use_rec_loss,\n",
    "            'use_cross_loss': self.use_cross_loss,\n",
    "            'use_ee_loss': self.use_ee_loss,\n",
    "            'use_trip_loss_paired': self.use_trip_loss_paired,\n",
    "            'use_trip_loss_unpaired': self.use_trip_loss_unpaired,\n",
    "            'use_smoothing_loss': self.use_smoothing_loss,\n",
    "            'use_latent_consistency': self.use_latent_consistency\n",
    "        }\n",
    "\n",
    "    def cross(self, x1, x1_rot, x2, x2_rot):\n",
    "        d1 = self.dynamic_encoder(x1_rot)\n",
    "        d2 = self.dynamic_encoder(x2_rot)\n",
    "        s1 = self.static_encoder(x1)\n",
    "        s2 = self.static_encoder(x2)\n",
    "        \n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1))\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1))\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1))\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1))\n",
    "\n",
    "        return x1_hat, x2_hat, y1_hat, y2_hat\n",
    "    \n",
    "    def eval(self, x1_rot, x2):\n",
    "        dynamic = self.dynamic_encoder(x1_rot)\n",
    "        static = self.static_encoder(x2)\n",
    "        return self.decoder(torch.cat((dynamic, static), dim=1))\n",
    "\n",
    "    def rec_loss(self, x, x_rot):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "        if not one_dimension_conv:\n",
    "            x_ = x.reshape(x.size(0), T, -1)\n",
    "        return self.reconstruction_loss(x_, x_hat)\n",
    "    \n",
    "    def loss_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, cross = True, reconstruction = True, emb_adv = True, discrim_adv = True, verbose = False):\n",
    "        d1 = self.dynamic_encoder(x1_rot) # A1\n",
    "        d2 = self.dynamic_encoder(x2_rot) # A2\n",
    "        s1 = self.static_encoder(x1) # P1\n",
    "        s2 = self.static_encoder(x2) # P2\n",
    "\n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1)) # P1, A1\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1)) # P2, A2\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1)) # P2, A1\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1)) # P1, A2\n",
    "\n",
    "        d12 = self.dynamic_encoder(y1_rot) # A1\n",
    "        d21 = self.dynamic_encoder(y2_rot) # A2\n",
    "        s12 = self.static_encoder(y1) # P2\n",
    "        s21 = self.static_encoder(y2) # P1\n",
    "\n",
    "        x1_hat_ = self.decoder(torch.cat((d12, s21), dim=1)) # P1, A1\n",
    "        x2_hat_ = self.decoder(torch.cat((d21, s12), dim=1)) # P2, A2\n",
    "        y1_hat_ = self.decoder(torch.cat((d12, s12), dim=1)) # P2, A1\n",
    "        y2_hat_ = self.decoder(torch.cat((d21, s21), dim=1)) # P1, A2\n",
    "\n",
    "        # x1_hat is reconstruction of x1\n",
    "        # x2_hat is reconstruction of x2\n",
    "        # y1_hat is cross reconstruction from x1 and x2\n",
    "        # y2_hat is cross reconstruction from x2 and x1\n",
    "        # x1_hat_ is cross reconstruction from y1 and y2\n",
    "        # x2_hat_ is cross reconstruction from y2 and y1\n",
    "        # y1_hat_ is reconstruction of y1\n",
    "        # y2_hat_ is reconstruction of y2\n",
    "        # d1 = A1\n",
    "        # d2 = A2\n",
    "        # d12 = A1\n",
    "        # d21 = A2\n",
    "        # s1 = P1\n",
    "        # s2 = P2\n",
    "        # s12 = P2\n",
    "        # s21 = P1\n",
    "\n",
    "        # flatten data if 2D\n",
    "        if not one_dimension_conv:\n",
    "            x1 = x1.view(x1.size(0), T, -1)\n",
    "            x2 = x2.view(x2.size(0), T, -1)\n",
    "            y1 = y1.view(y1.size(0), T, -1)\n",
    "            y2 = y2.view(y2.size(0), T, -1)\n",
    "        \n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        cross_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        latent_consistency_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "                        \n",
    "        # reconstruction loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = (self.reconstruction_loss(x1, x1_hat) + self.reconstruction_loss(x2, x2_hat) + self.reconstruction_loss(y1, y1_hat_) + self.reconstruction_loss(y2, y2_hat_)) / 4\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "        \n",
    "        # cross reconstruction loss\n",
    "        if self.use_cross_loss and cross:\n",
    "            # could move this to its own function, but since cross is basically reconstruction, its fine like this\n",
    "            cross_loss = (self.reconstruction_loss(y1, y1_hat) + self.reconstruction_loss(y2, y2_hat) + self.reconstruction_loss(x1, x1_hat_) + self.reconstruction_loss(x2, x2_hat_)) / 4\n",
    "            if verbose: print('Cross Reconstruction Loss: ', cross_loss.item())\n",
    "        \n",
    "        # end effector loss\n",
    "        if self.use_ee_loss:\n",
    "            if reconstruction:\n",
    "                end_effector_loss += (self.end_effector_loss(x1_hat, x1) + self.end_effector_loss(x2_hat, x2)) / 2\n",
    "            if cross:\n",
    "                end_effector_loss += (self.end_effector_loss(y1_hat, y1) + self.end_effector_loss(y2_hat, y2)) / 2\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # triplet loss\n",
    "        if self.use_trip_loss_paired: # anchor, positive, negative\n",
    "            # d1 = A1, d2 = A2, d12 = A1, d21 = A2\n",
    "            # s1 = P1, s2 = P2, s12 = P2, s21 = P1\n",
    "            # d12,s12 = y1, d21,s21 = y2\n",
    "            # y1 = jk, y2 = il\n",
    "            triplet_loss = self.triplet_loss(d12, d1, d2) \\\n",
    "                            + self.triplet_loss(d21, d2, d1) \\\n",
    "                            + self.triplet_loss(s12, s2, s1) \\\n",
    "                            + self.triplet_loss(s21, s1, s2) \n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = (self.smoothing_loss(x1, x1_hat) + self.smoothing_loss(x2, x2_hat) + self.smoothing_loss(y1, y1_hat_) + self.smoothing_loss(y2, y2_hat_) + \\\n",
    "                                self.smoothing_loss(x1, x1_hat_) + self.smoothing_loss(x2, x2_hat_) + self.smoothing_loss(y1, y1_hat) + self.smoothing_loss(y2, y2_hat)) / 8\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # latent consistency loss\n",
    "        if self.use_latent_consistency:\n",
    "            latent_consistency_loss = (self.latent_consistency_loss(d1, d12) + self.latent_consistency_loss(d2, d21) + self.latent_consistency_loss(s1, s21) + self.latent_consistency_loss(s2, s12)) / 4\n",
    "            if verbose: print('Latent Consistency Loss: ', latent_consistency_loss.item())\n",
    "\n",
    "        # adversarial loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y1, actor_y2 = actors[0] - 1, actors[1] - 1\n",
    "            actor_y1, actor_y2 = torch.eye(privacy_classes)[actor_y1.long()].to(device), torch.eye(privacy_classes)[actor_y2.long()].to(device)\n",
    "            action_y1, action_y2 = actions[0] - 1, actions[1] - 1\n",
    "            action_y1, action_y2 = torch.eye(utility_classes)[action_y1.long()].to(device), torch.eye(utility_classes)[action_y2.long()].to(device)\n",
    "\n",
    "            # x1 => d1 s1\n",
    "            # x2 => d2 s2\n",
    "\n",
    "            # d1 => p1\n",
    "            # d2 => p2\n",
    "            # s1 => a1\n",
    "            # s2 => a2\n",
    "            \n",
    "            # actor_y1 = p1\n",
    "            # actor_y2 = p2\n",
    "\n",
    "            # action_y1 = a1\n",
    "            # action_y2 = a2\n",
    "\n",
    "\n",
    "            # privacy loss (adversarial)\n",
    "            privacy_loss_adv = (-self.adv_loss(self.priv_adv, d1, actor_y1) -self.adv_loss(self.priv_adv, d2, actor_y2))/2\n",
    "            privacy_acc_adv = (self.adv_accuracy(self.priv_adv, d1, actor_y1) + self.adv_accuracy(self.priv_adv, d2, actor_y2))/2\n",
    "\n",
    "            # privacy loss (coop)\n",
    "            privacy_loss_coop = (self.adv_loss(self.priv_coop, s1, actor_y1) + self.adv_loss(self.priv_coop, s2, actor_y2))/2\n",
    "            privacy_acc_coop = (self.adv_accuracy(self.priv_coop, s1, actor_y1) + self.adv_accuracy(self.priv_coop, s2, actor_y2))/2\n",
    "\n",
    "            # utility loss (adversarial)\n",
    "            utility_loss_adv = (-self.adv_loss(self.util_adv, s1, action_y1) -self.adv_loss(self.util_adv, s2, action_y2))/2\n",
    "            utility_acc_adv = (self.adv_accuracy(self.util_adv, s1, action_y1) + self.adv_accuracy(self.util_adv, s2, action_y2))/2\n",
    "\n",
    "            # utility loss (coop)\n",
    "            utility_loss_coop = (self.adv_loss(self.util_coop, d1, action_y1) + self.adv_loss(self.util_coop, d2, action_y2))/2\n",
    "            utility_acc_coop = (self.adv_accuracy(self.util_coop, d1, action_y1) + self.adv_accuracy(self.util_coop, d2, action_y2))/2\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss (Adversarial): ', privacy_loss_adv.item(), '\\tPrivacy Loss (Coop): ', privacy_loss_coop.item())\n",
    "                print('Utility Loss (Adversarial): ', utility_loss_adv.item(), '\\tUtility Loss (Coop): ', utility_loss_coop.item())\n",
    "                print('Privacy Accuracy (Adversarial): ', privacy_acc_adv.item(), '\\tPrivacy Accuracy (Coop): ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy (Adversarial): ', utility_acc_adv.item(), '\\tUtility Accuracy (Coop): ', utility_acc_coop.item())\n",
    "            \n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat, x1_hat_, x2_hat_, y1_hat_, y2_hat_)))\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (8 * batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'cross_loss': cross_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'latent_consistency_loss': latent_consistency_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + cross_loss * self.lambda_cross \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + latent_consistency_loss * self.lambda_latent \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x1_hat, x2_hat, y1_hat, y2_hat, losses\n",
    "\n",
    "    def loss_unpaired(self, x_pos, x_rot, actors, actions, reconstruction = True, emb_adv = False, discrim_adv = False, ee = False, triplet = False, verbose = False):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x_pos)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "        if not one_dimension_conv:\n",
    "            x = x_pos.reshape(x_pos.size(0), T, -1)\n",
    "\n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = self.reconstruction_loss(x, x_hat)\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "\n",
    "        # End Effector Loss\n",
    "        if self.use_ee_loss and ee:\n",
    "            end_effector_loss = self.end_effector_loss(x_hat, x)\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # Triplet Loss\n",
    "        if self.use_trip_loss_unpaired and triplet: # anchor, positive, negative\n",
    "            triplet_loss = (self.triplet_loss(d, d, s) + self.triplet_loss(s, s, d)) / 2\n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        # Smoothing Loss\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = self.smoothing_loss(x, x_hat)\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # Adversarial Loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y = actors - 1\n",
    "            actor_y = torch.eye(privacy_classes)[actor_y.long()].to(device)\n",
    "            action_y = actions - 1\n",
    "            action_y = torch.eye(utility_classes)[action_y.long()].to(device)\n",
    "\n",
    "            # latent privacy loss (adv)\n",
    "            privacy_loss_adv = -self.adv_loss(self.priv_adv, d, actor_y)\n",
    "            privacy_acc_adv = self.adv_accuracy(self.priv_adv, d, actor_y)\n",
    "\n",
    "            # latent privacy loss (coop)\n",
    "            privacy_loss_coop = self.adv_loss(self.priv_coop, s, actor_y)\n",
    "            privacy_acc_coop = self.adv_accuracy(self.priv_coop, s, actor_y)\n",
    "\n",
    "            # latent utility loss (adv)\n",
    "            utility_loss_adv = -self.adv_loss(self.util_adv, s, action_y)\n",
    "            utility_acc_adv = self.adv_accuracy(self.util_adv, s, action_y)\n",
    "\n",
    "            # latent utility loss (coop)\n",
    "            utility_loss_coop = self.adv_loss(self.util_coop, d, action_y)\n",
    "            utility_acc_coop = self.adv_accuracy(self.util_coop, d, action_y)\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss Adv: ', privacy_loss_adv.item(), '\\tPrivacy Loss Coop: ', privacy_loss_coop.item(), '\\tPrivacy Loss: ', privacy_loss.item())\n",
    "                print('Utility Loss Adv: ', utility_loss_adv.item(), '\\tUtility Loss Coop: ', utility_loss_coop.item(), '\\tUtility Loss: ', utility_loss.item())\n",
    "                print('Privacy Accuracy Adv: ', privacy_acc_adv.item(), '\\tPrivacy Accuracy Coop: ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy Adv: ', utility_acc_adv.item(), '\\tUtility Accuracy Coop: ', utility_acc_coop.item())\n",
    "\n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x_hat, losses\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        # return F.mse_loss(x, y)\n",
    "        return torch.square(torch.norm(x - y, dim=1)).mean()\n",
    "    \n",
    "    def latent_consistency_loss(self, x, y):\n",
    "        return F.mse_loss(x, y)\n",
    "    \n",
    "    def end_effector_loss(self, x, y):\n",
    "        # slice to get the end effector joints\n",
    "        x_ee = x[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)] \n",
    "        y_ee = y[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)]\n",
    "\n",
    "        # calculate velocities\n",
    "        x_vel = torch.norm(x_ee[:, 1:] - x_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        y_vel = torch.norm(y_ee[:, 1:] - y_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        \n",
    "        # compute mse loss for each joint\n",
    "        losses = F.mse_loss(x_vel, y_vel, reduction='none')\n",
    "\n",
    "        # take sum over end effectors\n",
    "        loss = losses.sum(dim=1)\n",
    "\n",
    "        # take mean over batch\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def smoothing_loss(self, y, y_pred):\n",
    "        # (batch, T, 75)\n",
    "        # Calculate the squared sum of differences for y and y_pred\n",
    "        diff_y = torch.sum(y[:, :-1] - y[:, 1:], dim=2) ** 2\n",
    "        diff_y_pred = torch.sum(y_pred[:, :-1] - y_pred[:, 1:], dim=2) ** 2\n",
    "\n",
    "        # Calculate the absolute difference\n",
    "        abs_diff = torch.abs(diff_y - diff_y_pred)\n",
    "\n",
    "        # Sum over all batches and sequence elements\n",
    "        loss = torch.sum(abs_diff)\n",
    "\n",
    "        # Normalize by the total number of elements (batch_size * sequence_length)\n",
    "        total_loss = torch.sqrt(loss) / (y.size(0) * y.size(1))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def adv_loss(self, model, x, y):\n",
    "        return self.cross_entropy(model(x), y)#.long().to(device))\n",
    "    \n",
    "    def adv_accuracy(self, model, x, y):\n",
    "        return (model(x).argmax(dim=1) == y.argmax(dim=1).to(device)).float().mean()\n",
    "\n",
    "    def forward(self, x, x_rot):\n",
    "        dyn = self.dynamic_encoder(x_rot)\n",
    "        sta = self.static_encoder(x)\n",
    "        x = self.decoder(torch.cat((dyn, sta), dim=1))\n",
    "        return x\n",
    "    \n",
    "    def set_eval(self, eval=True):\n",
    "        if eval:\n",
    "            self.static_encoder.eval()\n",
    "            self.dynamic_encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "        else:\n",
    "            self.static_encoder.train()\n",
    "            self.dynamic_encoder.train()\n",
    "            self.decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, adv_lr=1e-4, use_adv=True):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # AutoEncoder Models\n",
    "        if one_dimension_conv:\n",
    "            self.static_encoder = Encoder1D()\n",
    "            self.advamic_encoder = Encoder1D()\n",
    "            self.decoder = Decoder1D()\n",
    "        else:\n",
    "            self.static_encoder = Encoder2D()\n",
    "            self.dynamic_encoder = Encoder2D()\n",
    "            self.decoder = Decoder1D()\n",
    "\n",
    "        # Adversarial Models\n",
    "        self.use_adv = use_adv\n",
    "        if use_adv:\n",
    "            self.priv_adv = Adversary_Emb(privacy_classes).to(device) # input = dynamic embedding, output = privacy class\n",
    "            self.priv_coop = Adversary_Emb(privacy_classes).to(device) # input = static embedding, output = privacy class\n",
    "            self.util_adv = Adversary_Emb(utility_classes).to(device) # input = static embedding, output = utility class\n",
    "            self.util_coop = Adversary_Emb(utility_classes).to(device) # input = dynamic embedding, output = utility class\n",
    "            self.discriminator = Discriminator().to(device)\n",
    "\n",
    "            self.priv_optim = torch.optim.AdamW(self.priv_adv.parameters(), lr=adv_lr)\n",
    "            self.priv_coop_optim = torch.optim.AdamW(self.priv_coop.parameters(), lr=adv_lr)\n",
    "            self.util_optim = torch.optim.AdamW(self.util_adv.parameters(), lr=adv_lr)\n",
    "            self.util_coop_optim = torch.optim.AdamW(self.util_coop.parameters(), lr=adv_lr)\n",
    "            self.discriminator_optim = torch.optim.AdamW(self.discriminator.parameters(), lr=adv_lr)\n",
    "\n",
    "            # Freeze Adversarial Models\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "\n",
    "        # Loss Functions\n",
    "        self.triplet_loss = nn.TripletMarginLoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Info for loss functions\n",
    "        self.end_effectors = torch.tensor([19, 15, 23, 24, 21, 22, 3]).to(device) * 3\n",
    "        self.chain_lengths = torch.tensor([5, 5, 8, 8, 8, 8, 5]).to(device)\n",
    "\n",
    "        # Lambdas for discounted loss\n",
    "        self.lambda_rec = 2\n",
    "        self.lambda_cross = 0.1\n",
    "        self.lambda_ee = 1\n",
    "        self.lambda_smoothing = 3\n",
    "        self.lambda_trip = 1\n",
    "        self.lambda_latent = 10\n",
    "        self.lambda_adv_util_coop = util_classifier_alpha\n",
    "        self.lambda_adv_priv_coop = priv_classifier_alpha\n",
    "        self.lambda_adv_util_adv = util_classifier_alpha\n",
    "        self.lambda_adv_priv_adv = priv_classifier_alpha\n",
    "        self.lambda_adv_disc = 1\n",
    "\n",
    "        # Loss Toggles\n",
    "        self.use_rec_loss = True\n",
    "        self.use_cross_loss = True\n",
    "        self.use_ee_loss = True \n",
    "        self.use_trip_loss_paired = True \n",
    "        self.use_trip_loss_unpaired = True\n",
    "        self.use_smoothing_loss = True\n",
    "        self.use_latent_consistency = True\n",
    "\n",
    "    def get_loss_params(self):\n",
    "        return {\n",
    "            'lambda_rec': self.lambda_rec,\n",
    "            'lambda_cross': self.lambda_cross,\n",
    "            'lambda_ee': self.lambda_ee,\n",
    "            'lambda_trip': self.lambda_trip,\n",
    "            'lambda_latent': self.lambda_latent,\n",
    "            'lambda_adv_util_coop': self.lambda_adv_util_coop,\n",
    "            'lambda_adv_priv_coop': self.lambda_adv_priv_coop,\n",
    "            'lambda_adv_util_adv': self.lambda_adv_util_adv,\n",
    "            'lambda_adv_priv_adv': self.lambda_adv_priv_adv,\n",
    "            'lambda_adv_disc': self.lambda_adv_disc,\n",
    "            'use_rec_loss': self.use_rec_loss,\n",
    "            'use_cross_loss': self.use_cross_loss,\n",
    "            'use_ee_loss': self.use_ee_loss,\n",
    "            'use_trip_loss_paired': self.use_trip_loss_paired,\n",
    "            'use_trip_loss_unpaired': self.use_trip_loss_unpaired,\n",
    "            'use_smoothing_loss': self.use_smoothing_loss,\n",
    "            'use_latent_consistency': self.use_latent_consistency\n",
    "        }\n",
    "\n",
    "    def cross(self, x1, x1_rot, x2, x2_rot):\n",
    "        d1 = self.dynamic_encoder(x1_rot)\n",
    "        d2 = self.dynamic_encoder(x2_rot)\n",
    "        s1 = self.static_encoder(x1)\n",
    "        s2 = self.static_encoder(x2)\n",
    "        \n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1))\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1))\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1))\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1))\n",
    "\n",
    "        return x1_hat, x2_hat, y1_hat, y2_hat\n",
    "    \n",
    "    def eval(self, x1_rot, x2):\n",
    "        dynamic = self.dynamic_encoder(x1_rot)\n",
    "        static = self.static_encoder(x2)\n",
    "        return self.decoder(torch.cat((dynamic, static), dim=1))\n",
    "\n",
    "    def rec_loss(self, x, x_rot):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "        if not one_dimension_conv:\n",
    "            x_ = x.reshape(x.size(0), T, -1)\n",
    "        return self.reconstruction_loss(x_, x_hat)\n",
    "    \n",
    "    def loss_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, cross = True, reconstruction = True, emb_adv = True, discrim_adv = True, verbose = False):\n",
    "        d1 = self.dynamic_encoder(x1_rot) # A1\n",
    "        d2 = self.dynamic_encoder(x2_rot) # A2\n",
    "        s1 = self.static_encoder(x1) # P1\n",
    "        s2 = self.static_encoder(x2) # P2\n",
    "\n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1)) # P1, A1\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1)) # P2, A2\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1)) # P2, A1\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1)) # P1, A2\n",
    "\n",
    "        d12 = self.dynamic_encoder(y1_rot) # A1\n",
    "        d21 = self.dynamic_encoder(y2_rot) # A2\n",
    "        s12 = self.static_encoder(y1) # P2\n",
    "        s21 = self.static_encoder(y2) # P1\n",
    "\n",
    "        x1_hat_ = self.decoder(torch.cat((d12, s21), dim=1)) # P1, A1\n",
    "        x2_hat_ = self.decoder(torch.cat((d21, s12), dim=1)) # P2, A2\n",
    "        y1_hat_ = self.decoder(torch.cat((d12, s12), dim=1)) # P2, A1\n",
    "        y2_hat_ = self.decoder(torch.cat((d21, s21), dim=1)) # P1, A2\n",
    "\n",
    "        # x1_hat is reconstruction of x1\n",
    "        # x2_hat is reconstruction of x2\n",
    "        # y1_hat is cross reconstruction from x1 and x2\n",
    "        # y2_hat is cross reconstruction from x2 and x1\n",
    "        # x1_hat_ is cross reconstruction from y1 and y2\n",
    "        # x2_hat_ is cross reconstruction from y2 and y1\n",
    "        # y1_hat_ is reconstruction of y1\n",
    "        # y2_hat_ is reconstruction of y2\n",
    "        # d1 = A1\n",
    "        # d2 = A2\n",
    "        # d12 = A1\n",
    "        # d21 = A2\n",
    "        # s1 = P1\n",
    "        # s2 = P2\n",
    "        # s12 = P2\n",
    "        # s21 = P1\n",
    "\n",
    "        # flatten data if 2D\n",
    "        if not one_dimension_conv:\n",
    "            x1 = x1.view(x1.size(0), T, -1)\n",
    "            x2 = x2.view(x2.size(0), T, -1)\n",
    "            y1 = y1.view(y1.size(0), T, -1)\n",
    "            y2 = y2.view(y2.size(0), T, -1)\n",
    "        \n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        cross_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        latent_consistency_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "                        \n",
    "        # reconstruction loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = (self.reconstruction_loss(x1, x1_hat) + self.reconstruction_loss(x2, x2_hat) + self.reconstruction_loss(y1, y1_hat_) + self.reconstruction_loss(y2, y2_hat_)) / 4\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "        \n",
    "        # cross reconstruction loss\n",
    "        if self.use_cross_loss and cross:\n",
    "            # could move this to its own function, but since cross is basically reconstruction, its fine like this\n",
    "            cross_loss = (self.reconstruction_loss(y1, y1_hat) + self.reconstruction_loss(y2, y2_hat) + self.reconstruction_loss(x1, x1_hat_) + self.reconstruction_loss(x2, x2_hat_)) / 4\n",
    "            if verbose: print('Cross Reconstruction Loss: ', cross_loss.item())\n",
    "        \n",
    "        # end effector loss\n",
    "        if self.use_ee_loss:\n",
    "            if reconstruction:\n",
    "                end_effector_loss += (self.end_effector_loss(x1_hat, x1) + self.end_effector_loss(x2_hat, x2)) / 2\n",
    "            if cross:\n",
    "                end_effector_loss += (self.end_effector_loss(y1_hat, y1) + self.end_effector_loss(y2_hat, y2)) / 2\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # triplet loss\n",
    "        if self.use_trip_loss_paired: # anchor, positive, negative\n",
    "            # d1 = A1, d2 = A2, d12 = A1, d21 = A2\n",
    "            # s1 = P1, s2 = P2, s12 = P2, s21 = P1\n",
    "            # d12,s12 = y1, d21,s21 = y2\n",
    "            # y1 = jk, y2 = il\n",
    "            triplet_loss = self.triplet_loss(d12, d1, d2) \\\n",
    "                            + self.triplet_loss(d21, d2, d1) \\\n",
    "                            + self.triplet_loss(s12, s2, s1) \\\n",
    "                            + self.triplet_loss(s21, s1, s2) \n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = (self.smoothing_loss(x1, x1_hat) + self.smoothing_loss(x2, x2_hat) + self.smoothing_loss(y1, y1_hat_) + self.smoothing_loss(y2, y2_hat_) + \\\n",
    "                                self.smoothing_loss(x1, x1_hat_) + self.smoothing_loss(x2, x2_hat_) + self.smoothing_loss(y1, y1_hat) + self.smoothing_loss(y2, y2_hat)) / 8\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # latent consistency loss\n",
    "        if self.use_latent_consistency:\n",
    "            latent_consistency_loss = (self.latent_consistency_loss(d1, d12) + self.latent_consistency_loss(d2, d21) + self.latent_consistency_loss(s1, s21) + self.latent_consistency_loss(s2, s12)) / 4\n",
    "            if verbose: print('Latent Consistency Loss: ', latent_consistency_loss.item())\n",
    "\n",
    "        # adversarial loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y1, actor_y2 = actors[0] - 1, actors[1] - 1\n",
    "            actor_y1, actor_y2 = torch.eye(privacy_classes)[actor_y1.long()].to(device), torch.eye(privacy_classes)[actor_y2.long()].to(device)\n",
    "            action_y1, action_y2 = actions[0] - 1, actions[1] - 1\n",
    "            action_y1, action_y2 = torch.eye(utility_classes)[action_y1.long()].to(device), torch.eye(utility_classes)[action_y2.long()].to(device)\n",
    "\n",
    "            # x1 => d1 s1\n",
    "            # x2 => d2 s2\n",
    "\n",
    "            # d1 => p1\n",
    "            # d2 => p2\n",
    "            # s1 => a1\n",
    "            # s2 => a2\n",
    "            \n",
    "            # actor_y1 = p1\n",
    "            # actor_y2 = p2\n",
    "\n",
    "            # action_y1 = a1\n",
    "            # action_y2 = a2\n",
    "\n",
    "\n",
    "            # privacy loss (adversarial)\n",
    "            privacy_loss_adv = (-self.adv_loss(self.priv_adv, d1, actor_y1) -self.adv_loss(self.priv_adv, d2, actor_y2))/2\n",
    "            privacy_acc_adv = (self.adv_accuracy(self.priv_adv, d1, actor_y1) + self.adv_accuracy(self.priv_adv, d2, actor_y2))/2\n",
    "\n",
    "            # privacy loss (coop)\n",
    "            privacy_loss_coop = (self.adv_loss(self.priv_coop, s1, actor_y1) + self.adv_loss(self.priv_coop, s2, actor_y2))/2\n",
    "            privacy_acc_coop = (self.adv_accuracy(self.priv_coop, s1, actor_y1) + self.adv_accuracy(self.priv_coop, s2, actor_y2))/2\n",
    "\n",
    "            # utility loss (adversarial)\n",
    "            utility_loss_adv = (-self.adv_loss(self.util_adv, s1, action_y1) -self.adv_loss(self.util_adv, s2, action_y2))/2\n",
    "            utility_acc_adv = (self.adv_accuracy(self.util_adv, s1, action_y1) + self.adv_accuracy(self.util_adv, s2, action_y2))/2\n",
    "\n",
    "            # utility loss (coop)\n",
    "            utility_loss_coop = (self.adv_loss(self.util_coop, d1, action_y1) + self.adv_loss(self.util_coop, d2, action_y2))/2\n",
    "            utility_acc_coop = (self.adv_accuracy(self.util_coop, d1, action_y1) + self.adv_accuracy(self.util_coop, d2, action_y2))/2\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss (Adversarial): ', privacy_loss_adv.item(), '\\tPrivacy Loss (Coop): ', privacy_loss_coop.item())\n",
    "                print('Utility Loss (Adversarial): ', utility_loss_adv.item(), '\\tUtility Loss (Coop): ', utility_loss_coop.item())\n",
    "                print('Privacy Accuracy (Adversarial): ', privacy_acc_adv.item(), '\\tPrivacy Accuracy (Coop): ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy (Adversarial): ', utility_acc_adv.item(), '\\tUtility Accuracy (Coop): ', utility_acc_coop.item())\n",
    "            \n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat, x1_hat_, x2_hat_, y1_hat_, y2_hat_)))\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (8 * batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'cross_loss': cross_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'latent_consistency_loss': latent_consistency_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + cross_loss * self.lambda_cross \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + latent_consistency_loss * self.lambda_latent \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x1_hat, x2_hat, y1_hat, y2_hat, losses\n",
    "\n",
    "    def loss_unpaired(self, x_pos, x_rot, actors, actions, reconstruction = True, emb_adv = False, discrim_adv = False, ee = False, triplet = False, verbose = False):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x_pos)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "        if not one_dimension_conv:\n",
    "            x = x_pos.reshape(x_pos.size(0), T, -1)\n",
    "\n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = self.reconstruction_loss(x, x_hat)\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "\n",
    "        # End Effector Loss\n",
    "        if self.use_ee_loss and ee:\n",
    "            end_effector_loss = self.end_effector_loss(x_hat, x)\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # Triplet Loss\n",
    "        if self.use_trip_loss_unpaired and triplet: # anchor, positive, negative\n",
    "            triplet_loss = (self.triplet_loss(d, d, s) + self.triplet_loss(s, s, d)) / 2\n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        # Smoothing Loss\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = self.smoothing_loss(x, x_hat)\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # Adversarial Loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y = actors - 1\n",
    "            actor_y = torch.eye(privacy_classes)[actor_y.long()].to(device)\n",
    "            action_y = actions - 1\n",
    "            action_y = torch.eye(utility_classes)[action_y.long()].to(device)\n",
    "\n",
    "            # latent privacy loss (adv)\n",
    "            privacy_loss_adv = -self.adv_loss(self.priv_adv, d, actor_y)\n",
    "            privacy_acc_adv = self.adv_accuracy(self.priv_adv, d, actor_y)\n",
    "\n",
    "            # latent privacy loss (coop)\n",
    "            privacy_loss_coop = self.adv_loss(self.priv_coop, s, actor_y)\n",
    "            privacy_acc_coop = self.adv_accuracy(self.priv_coop, s, actor_y)\n",
    "\n",
    "            # latent utility loss (adv)\n",
    "            utility_loss_adv = -self.adv_loss(self.util_adv, s, action_y)\n",
    "            utility_acc_adv = self.adv_accuracy(self.util_adv, s, action_y)\n",
    "\n",
    "            # latent utility loss (coop)\n",
    "            utility_loss_coop = self.adv_loss(self.util_coop, d, action_y)\n",
    "            utility_acc_coop = self.adv_accuracy(self.util_coop, d, action_y)\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss Adv: ', privacy_loss_adv.item(), '\\tPrivacy Loss Coop: ', privacy_loss_coop.item(), '\\tPrivacy Loss: ', privacy_loss.item())\n",
    "                print('Utility Loss Adv: ', utility_loss_adv.item(), '\\tUtility Loss Coop: ', utility_loss_coop.item(), '\\tUtility Loss: ', utility_loss.item())\n",
    "                print('Privacy Accuracy Adv: ', privacy_acc_adv.item(), '\\tPrivacy Accuracy Coop: ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy Adv: ', utility_acc_adv.item(), '\\tUtility Accuracy Coop: ', utility_acc_coop.item())\n",
    "\n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x_hat, losses\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        # return F.mse_loss(x, y)\n",
    "        return torch.square(torch.norm(x - y, dim=1)).mean()\n",
    "    \n",
    "    def latent_consistency_loss(self, x, y):\n",
    "        return F.mse_loss(x, y)\n",
    "    \n",
    "    def end_effector_loss(self, x, y):\n",
    "        # slice to get the end effector joints\n",
    "        x_ee = x[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)] \n",
    "        y_ee = y[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)]\n",
    "\n",
    "        # calculate velocities\n",
    "        x_vel = torch.norm(x_ee[:, 1:] - x_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        y_vel = torch.norm(y_ee[:, 1:] - y_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        \n",
    "        # compute mse loss for each joint\n",
    "        losses = F.mse_loss(x_vel, y_vel, reduction='none')\n",
    "\n",
    "        # take sum over end effectors\n",
    "        loss = losses.sum(dim=1)\n",
    "\n",
    "        # take mean over batch\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def smoothing_loss(self, y, y_pred):\n",
    "        # (batch, T, 75)\n",
    "        # Calculate the squared sum of differences for y and y_pred\n",
    "        diff_y = torch.sum(y[:, :-1] - y[:, 1:], dim=2) ** 2\n",
    "        diff_y_pred = torch.sum(y_pred[:, :-1] - y_pred[:, 1:], dim=2) ** 2\n",
    "\n",
    "        # Calculate the absolute difference\n",
    "        abs_diff = torch.abs(diff_y - diff_y_pred)\n",
    "\n",
    "        # Sum over all batches and sequence elements\n",
    "        loss = torch.sum(abs_diff)\n",
    "\n",
    "        # Normalize by the total number of elements (batch_size * sequence_length)\n",
    "        total_loss = torch.sqrt(loss) / (y.size(0) * y.size(1))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def adv_loss(self, model, x, y):\n",
    "        return self.cross_entropy(model(x), y)#.long().to(device))\n",
    "    \n",
    "    def adv_accuracy(self, model, x, y):\n",
    "        return (model(x).argmax(dim=1) == y.argmax(dim=1).to(device)).float().mean()\n",
    "\n",
    "    def train_adv_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, train_emb = True, train_discrim = True):\n",
    "        if not self.use_adv: return 0,0\n",
    "        # freeze encoders/decoder\n",
    "        self.dynamic_encoder.eval()\n",
    "        self.static_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        # unfreeze adversaries\n",
    "        self.priv_adv.train()\n",
    "        self.util_adv.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        # zero out gradients\n",
    "        self.priv_optim.zero_grad()\n",
    "        self.util_optim.zero_grad()\n",
    "        self.discriminator_optim.zero_grad()\n",
    "\n",
    "        # encode\n",
    "        d1 = self.dynamic_encoder(x1_rot) # A1\n",
    "        d2 = self.dynamic_encoder(x2_rot) # A2\n",
    "        d3 = self.dynamic_encoder(y1_rot) # A2\n",
    "        d4 = self.dynamic_encoder(y2_rot) # A1\n",
    "        s1 = self.static_encoder(x1) # P1\n",
    "        s2 = self.static_encoder(x2) # P2\n",
    "        s3 = self.static_encoder(y1) # P1\n",
    "        s4 = self.static_encoder(y2) # P2\n",
    "\n",
    "        # decode\n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1)) # P1, A1\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1)) # P2, A2\n",
    "        y1_hat = self.decoder(torch.cat((d3, s3), dim=1)) # P1, A2\n",
    "        y2_hat = self.decoder(torch.cat((d4, s4), dim=1)) # P2, A1\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            # train privacy adversary\n",
    "            p1, p2 = actors[0] - 1, actors[1] - 1\n",
    "            p1, p2 = torch.eye(privacy_classes)[p1.long()].to(device), torch.eye(privacy_classes)[p2.long()].to(device)\n",
    "            priv_loss = (self.cross_entropy(self.priv_adv(d1), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d2), p2) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d3), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d4), p2)) / 4\n",
    "            priv_acc = (self.adv_accuracy(self.priv_adv, d1, p1) + self.adv_accuracy(self.priv_adv, d2, p2) + self.adv_accuracy(self.priv_adv, d3, p1) + self.adv_accuracy(self.priv_adv, d4, p2)) / 4\n",
    "            priv_loss.backward(retain_graph=True)\n",
    "            self.priv_optim.step()\n",
    "\n",
    "            # train privacy cooperative\n",
    "            priv_coop_loss = (self.cross_entropy(self.priv_coop(s1), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s2), p2) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s3), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s4), p2)) / 4\n",
    "            priv_coop_acc = (self.adv_accuracy(self.priv_coop, s1, p1) + self.adv_accuracy(self.priv_coop, s2, p2) + self.adv_accuracy(self.priv_coop, s3, p1) + self.adv_accuracy(self.priv_coop, s4, p2)) / 4\n",
    "            priv_coop_loss.backward(retain_graph=True)\n",
    "            self.priv_coop_optim.step()\n",
    "                        \n",
    "            # train utility adversary\n",
    "            a1, a2 = actions[0] - 1, actions[1] - 1\n",
    "            a1, a2 = torch.eye(utility_classes)[a1.long()].to(device), torch.eye(utility_classes)[a2.long()].to(device)\n",
    "            util_loss = (self.cross_entropy(self.util_adv(s1), a1) + \\\n",
    "                        self.cross_entropy(self.util_adv(s2), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s3), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s4), a1)) / 4\n",
    "            util_acc = (self.adv_accuracy(self.util_adv, s1, a1) + self.adv_accuracy(self.util_adv, s2, a2) + self.adv_accuracy(self.util_adv, s3, a2) + self.adv_accuracy(self.util_adv, s4, a1)) / 4\n",
    "            util_loss.backward(retain_graph=True)\n",
    "            self.util_optim.step()\n",
    "\n",
    "            # train utility cooperative\n",
    "            util_coop_loss = (self.cross_entropy(self.util_coop(d1), a1) + \\\n",
    "                            self.cross_entropy(self.util_coop(d2), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d3), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d4), a1)) / 4\n",
    "            util_coop_acc = (self.adv_accuracy(self.util_coop, d1, a1) + self.adv_accuracy(self.util_coop, d2, a2) + self.adv_accuracy(self.util_coop, d3, a2) + self.adv_accuracy(self.util_coop, d4, a1)) / 4\n",
    "            util_coop_loss.backward(retain_graph=True)\n",
    "            self.util_coop_optim.step()\n",
    "\n",
    "\n",
    "        if train_discrim:\n",
    "            # train discriminator\n",
    "            output_real = self.discriminator(torch.cat((x1.view(x1.size(0), T, -1), x2.view(x2.size(0), T, -1), y1.view(y1.size(0), T, -1), y2.view(y1.size(0), T, -1))))\n",
    "            output_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat)))\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / (4 * batch_size)) + (torch.sum(torch.round(output_real) == 1).float() / (4 * batch_size))) / 2\n",
    "            discriminator_loss.backward()\n",
    "            self.discriminator_optim.step()\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.dynamic_encoder.train()\n",
    "        self.static_encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        # freeze adversaries\n",
    "        self.priv_adv.eval()\n",
    "        self.priv_coop.eval()\n",
    "        self.util_adv.eval()\n",
    "        self.util_coop.eval()\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def train_adv_unpaired(self, x_pos, x_rot, actor, action, train_emb = True, train_discrim = True):\n",
    "        # ensure one training method is enabled\n",
    "        assert train_emb or train_discrim, 'At least one training method must be enabled'\n",
    "\n",
    "        # freeze encoders/decoder\n",
    "        self.dynamic_encoder.eval()\n",
    "        self.static_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        # unfreeze adversaries\n",
    "        self.priv_adv.train()\n",
    "        self.priv_coop.train()\n",
    "        self.util_adv.train()\n",
    "        self.util_coop.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        # zero out gradients\n",
    "        self.priv_optim.zero_grad()\n",
    "        self.priv_coop_optim.zero_grad()\n",
    "        self.util_optim.zero_grad()\n",
    "        self.util_coop_optim.zero_grad()\n",
    "        self.discriminator_optim.zero_grad()\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            p = actor - 1\n",
    "            p = torch.eye(privacy_classes)[p.long()].to(device)\n",
    "            a = action - 1\n",
    "            a = torch.eye(utility_classes)[a.long()].to(device)\n",
    "\n",
    "            # train privacy adversary\n",
    "            priv_loss = self.adv_loss(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "            priv_acc = self.adv_accuracy(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "            priv_loss.backward()\n",
    "            self.priv_optim.step()\n",
    "\n",
    "            # tain privacy cooperative\n",
    "            priv_coop_loss = self.adv_loss(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            priv_coop_acc = self.adv_accuracy(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            priv_coop_loss.backward()\n",
    "            self.priv_coop_optim.step()\n",
    "            \n",
    "            # train utility adversary\n",
    "            util_loss = self.adv_loss(self.util_adv, self.static_encoder(x_pos), a)\n",
    "            util_acc = self.adv_accuracy(self.util_adv, self.static_encoder(x_pos), a)\n",
    "            util_loss.backward()\n",
    "            self.util_optim.step()\n",
    "\n",
    "            # train utility cooperative\n",
    "            util_coop_loss = self.adv_loss(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "            util_coop_acc = self.adv_accuracy(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "            util_coop_loss.backward()\n",
    "            self.util_coop_optim.step()\n",
    "\n",
    "        if train_discrim:\n",
    "            # encode\n",
    "            d = self.dynamic_encoder(x_rot)\n",
    "            s = self.static_encoder(x_pos)\n",
    "\n",
    "            # decode\n",
    "            x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "            # train discriminator\n",
    "            output_real = self.discriminator(x_pos.reshape(x_pos.size(0), T, -1))\n",
    "            output_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / batch_size) + (torch.sum(torch.round(output_real) == 1).float() / batch_size)) / 2\n",
    "            discriminator_loss.backward()\n",
    "            self.discriminator_optim.step()\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.dynamic_encoder.train()\n",
    "        self.static_encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        # freeze adversaries\n",
    "        self.priv_adv.eval()\n",
    "        self.priv_coop.eval()\n",
    "        self.util_adv.eval()\n",
    "        self.util_coop.eval()\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def val_adv_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, train_emb = True, train_discrim = True):\n",
    "        if not self.use_adv: return 0,0\n",
    "\n",
    "        # freeze encoders/decoder\n",
    "        self.set_eval()\n",
    "\n",
    "        # Encode\n",
    "        d1, d2, d3, d4 = [self.dynamic_encoder(x) for x in [x1_rot, x2_rot, y1_rot, y2_rot]]\n",
    "        s1, s2, s3, s4 = [self.static_encoder(x) for x in [x1, x2, y1, y2]]\n",
    "\n",
    "        # Decode\n",
    "        x1_hat, x2_hat, y1_hat, y2_hat = [self.decoder(torch.cat((d, s), dim=1)) for d, s in zip([d1, d2, d3, d4], [s1, s2, s3, s4])]\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            # privacy adversary\n",
    "            p1, p2 = actors[0] - 1, actors[1] - 1\n",
    "            p1, p2 = torch.eye(privacy_classes)[p1.long()].to(device), torch.eye(privacy_classes)[p2.long()].to(device)\n",
    "            priv_loss = (self.cross_entropy(self.priv_adv(d1), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d2), p2) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d3), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d4), p2)) / 4\n",
    "            priv_acc = (self.adv_accuracy(self.priv_adv, d1, p1) + self.adv_accuracy(self.priv_adv, d2, p2) + self.adv_accuracy(self.priv_adv, d3, p1) + self.adv_accuracy(self.priv_adv, d4, p2)) / 4\n",
    "\n",
    "            # privacy cooperative\n",
    "            priv_coop_loss = (self.cross_entropy(self.priv_coop(s1), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s2), p2) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s3), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s4), p2)) / 4\n",
    "            priv_coop_acc = (self.adv_accuracy(self.priv_coop, s1, p1) + self.adv_accuracy(self.priv_coop, s2, p2) + self.adv_accuracy(self.priv_coop, s3, p1) + self.adv_accuracy(self.priv_coop, s4, p2)) / 4\n",
    "                        \n",
    "            # utility adversary\n",
    "            a1, a2 = actions[0] - 1, actions[1] - 1\n",
    "            a1, a2 = torch.eye(utility_classes)[a1.long()].to(device), torch.eye(utility_classes)[a2.long()].to(device)\n",
    "            util_loss = (self.cross_entropy(self.util_adv(s1), a1) + \\\n",
    "                        self.cross_entropy(self.util_adv(s2), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s3), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s4), a1)) / 4\n",
    "            util_acc = (self.adv_accuracy(self.util_adv, s1, a1) + self.adv_accuracy(self.util_adv, s2, a2) + self.adv_accuracy(self.util_adv, s3, a2) + self.adv_accuracy(self.util_adv, s4, a1)) / 4\n",
    "\n",
    "            # utility cooperative\n",
    "            util_coop_loss = (self.cross_entropy(self.util_coop(d1), a1) + \\\n",
    "                            self.cross_entropy(self.util_coop(d2), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d3), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d4), a1)) / 4\n",
    "            util_coop_acc = (self.adv_accuracy(self.util_coop, d1, a1) + self.adv_accuracy(self.util_coop, d2, a2) + self.adv_accuracy(self.util_coop, d3, a2) + self.adv_accuracy(self.util_coop, d4, a1)) / 4\n",
    "\n",
    "\n",
    "        if train_discrim:\n",
    "            # discriminator\n",
    "            output_real = self.discriminator(torch.cat((x1.view(x1.size(0), T, -1), x2.view(x2.size(0), T, -1), y1.view(y1.size(0), T, -1), y2.view(y1.size(0), T, -1))))\n",
    "            output_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat)))\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / (4 * batch_size)) + (torch.sum(torch.round(output_real) == 1).float() / (4 * batch_size))) / 2\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.set_eval(False)\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def val_adv_unpaired(self, x_pos, x_rot, actor, action, train_emb = True, train_discrim = True):\n",
    "        # ensure one training method is enabled\n",
    "        assert train_emb or train_discrim, 'At least one training method must be enabled'\n",
    "\n",
    "        # freeze encoders/decoder\n",
    "        self.set_eval()\n",
    "\n",
    "        # Encode\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x_pos)\n",
    "\n",
    "        # Decode\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            p = actor - 1\n",
    "            p = torch.eye(privacy_classes)[p.long()].to(device)\n",
    "            a = action - 1\n",
    "            a = torch.eye(utility_classes)[a.long()].to(device)\n",
    "\n",
    "            # privacy adversary\n",
    "            priv_loss = self.adv_loss(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "            priv_acc = self.adv_accuracy(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "\n",
    "            # privacy cooperative\n",
    "            priv_coop_loss = self.adv_loss(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            priv_coop_acc = self.adv_accuracy(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            \n",
    "            # utility adversary\n",
    "            util_loss = self.adv_loss(self.util_adv, self.static_encoder(x_pos), a)\n",
    "            util_acc = self.adv_accuracy(self.util_adv, self.static_encoder(x_pos), a)\n",
    "\n",
    "            # utility cooperative\n",
    "            util_coop_loss = self.adv_loss(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "            util_coop_acc = self.adv_accuracy(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "\n",
    "        if train_discrim:\n",
    "            # encode\n",
    "            d = self.dynamic_encoder(x_rot)\n",
    "            s = self.static_encoder(x_pos)\n",
    "\n",
    "            # decode\n",
    "            x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "            # train discriminator\n",
    "            output_real = self.discriminator(x_pos.reshape(x_pos.size(0), T, -1))\n",
    "            output_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / batch_size) + (torch.sum(torch.round(output_real) == 1).float() / batch_size)) / 2\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.set_eval(False)\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def forward(self, x, x_rot):\n",
    "        dyn = self.dynamic_encoder(x_rot)\n",
    "        sta = self.static_encoder(x)\n",
    "        x = self.decoder(torch.cat((dyn, sta), dim=1))\n",
    "        return x\n",
    "    \n",
    "    def set_eval(self, eval=True):\n",
    "        if eval:\n",
    "            self.static_encoder.eval()\n",
    "            self.dynamic_encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "        else:\n",
    "            self.static_encoder.train()\n",
    "            self.dynamic_encoder.train()\n",
    "            self.decoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility/Privacy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, k=3):\n",
    "    acces = AverageMeter()\n",
    "    topk_acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "        topk_acc = top_k_accuracy(output.data, target.cuda(), k=k)\n",
    "        topk_acces.update(topk_acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "\n",
    "    label_index = np.argmax(label_output, axis=1)\n",
    "    pred_index = np.argmax(pred_output, axis=1)\n",
    "\n",
    "    f1 = f1_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "    precision = precision_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "    recall = recall_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "\n",
    "    return acces.avg, f1, precision, recall, topk_acces.avg\n",
    "    \n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)\n",
    "\n",
    "def top_k_accuracy(output, target, k=3):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(k, 1, True, True) \n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1) \n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "    return correct_k.mul_(100.0 / batch_size)\n",
    "    \n",
    "def run_sgn_eval(train_x, train_y, test_x, test_y, val_x, val_y, case, model, k=3):\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, case, seg=20, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    test_loader = ntu_loaders.get_test_loader(batch_size, 16)\n",
    "\n",
    "    # Test\n",
    "    return test(test_loader, model, k=k)\n",
    "\n",
    "def run_sgn_gender_eval(train_x, train_y, test_x, test_y, val_x, val_y, model, k=1):\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, 0, seg=20, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    test_loader = ntu_loaders.get_test_loader(batch_size, 16)\n",
    "\n",
    "    acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "\n",
    "    label_index = np.argmax(label_output, axis=1)\n",
    "    pred_index = np.argmax(pred_output, axis=1)\n",
    "\n",
    "    f1 = f1_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "\n",
    "    return acces.avg, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(adv_lr=adv_lr).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_24632\\273552988.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('pretrained/MR.pt'))\n"
     ]
    }
   ],
   "source": [
    "load_model = False\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load('pretrained/MR.pt'))\n",
    "    # model.load_state_dict(torch.load('final_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_24632\\471309301.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if remove_two_actor_actions: sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60_sgnpt_no_two_actor.pt')['state_dict'])\n",
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_24632\\471309301.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_60_sgnpt.pt')['state_dict'])\n"
     ]
    }
   ],
   "source": [
    "load_util = True\n",
    "sgn_ar = SGN(utility_classes, None, seg, batch_size, 0).to(device)\n",
    "sgn_priv = SGN(privacy_classes, None, seg, batch_size, 0).to(device)\n",
    "\n",
    "sgn_priv.load_state_dict(torch.load('SGN/pretrained/etri_ri.pt')['state_dict'])\n",
    "sgn_ar.load_state_dict(torch.load('SGN/pretrained/etri_ar.pt')['state_dict'])\n",
    "\n",
    "\n",
    "# if ntu_120:\n",
    "#     if only_use_pos: # Assumes SGN preprocessing\n",
    "#         raise NotImplementedError\n",
    "#         sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_sgnpt.pt')['state_dict'])\n",
    "#         sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_sgnpt.pt')['state_dict'])\n",
    "#     else:\n",
    "#         sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy.pt')['state_dict'])\n",
    "#         sgn_ar.load_state_dict(torch.load('SGN/pretrained/action.pt')['state_dict'])\n",
    "# else:\n",
    "#     if only_use_pos:\n",
    "#         if remove_two_actor_actions: sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60_sgnpt_no_two_actor.pt')['state_dict'])\n",
    "#         else: sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60_sgnpt.pt')['state_dict'])\n",
    "#         sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_60_sgnpt.pt')['state_dict'])\n",
    "#     else: \n",
    "#         sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_60.pt')['state_dict'])\n",
    "#         sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Motion Retargeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgn_train_x, sgn_train_y, sgn_val_x, sgn_val_y = np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1)), np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1))\n",
    "\n",
    "best_metric = float('inf')\n",
    "total_epochs = -1\n",
    "cur_tot_epoch = 0\n",
    "\n",
    "def train_paired(train_ae = True, train_cross = True, train_discrim = True, train_emb_adv = True, run_eval = True, use_emb_adv = True, use_discrim_adv = True, run_sgn_eval = False, save = True, k=3):\n",
    "    global best_metric\n",
    "    global total_epochs\n",
    "    global cur_tot_epoch\n",
    "    # Assertions\n",
    "    # assert train_ae or train_cross or train_discrim or train_emb_adv, \"At least one of the training objectives must be True\"\n",
    "    assert not (run_sgn_eval and not run_eval), \"If run_sgn_eval is True, then run_eval must be True\"\n",
    "    \n",
    "    # Store eval values for validation\n",
    "    eval_X_known, eval_Y_known_action, eval_Y_known_actor, eval_X_rec, eval_Y_rec_action, eval_Y_rec_actor, eval_X, eval_Y_action, eval_Y_actor, eval_Y_initial_actor = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    # Losses for printing\n",
    "    losses = []\n",
    "    rec_loss, cross_loss, end_effector_loss, smoothing_loss, triplet_loss, latent_consistency_loss, privacy_loss, privacy_loss_adv, privacy_loss_coop, privacy_acc_adv, privacy_acc_coop, priv_training_loss, utility_loss, utility_loss_adv, utility_loss_coop, utility_acc_adv, utility_acc_coop, util_training_loss, discriminator_loss, discriminator_train_losses, discriminator_training_acc, priv_coop_training_loss, priv_training_acc, priv_coop_training_acc, util_coop_training_loss, util_training_acc, util_coop_training_acc = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    # Determine if adversaries need to be trained\n",
    "    train_emb_this_epoch = True\n",
    "    if emb_clf_update_per_epoch_paired < 1:\n",
    "        if cur_tot_epoch % round(1 / emb_clf_update_per_epoch_paired) != 0:\n",
    "            train_emb_this_epoch = False\n",
    "\n",
    "    for (x1_pos, x1_rot, x2_pos, x2_rot, y1_pos, y1_rot, y2_pos, y2_rot, actors, actions) in train_dl:\n",
    "        # Move tensors to the configured device\n",
    "        x1_pos, x1_rot, x2_pos, x2_rot, y1_pos, y1_rot, y2_pos, y2_rot = x1_pos.float().to(device), x1_rot.float().to(device), x2_pos.float().to(device), x2_rot.float().to(device), y1_pos.float().to(device), y1_rot.float().to(device), y2_pos.float().to(device), y2_rot.float().to(device)\n",
    "        \n",
    "        # Remove rotation data if only using position data\n",
    "        if only_use_pos:\n",
    "            x1_rot, x2_rot, y1_rot, y2_rot = x1_pos, x2_pos, y1_pos, y2_pos\n",
    "\n",
    "        # For 1D convolutions, flatten the data\n",
    "        if one_dimension_conv:\n",
    "            x1_pos = x1_pos.view(x1_pos.size(0), T, -1)\n",
    "            x1_rot = x1_rot.view(x1_rot.size(0), T, -1)\n",
    "            x2_pos = x2_pos.view(x2_pos.size(0), T, -1)\n",
    "            x2_rot = x2_rot.view(x2_rot.size(0), T, -1)\n",
    "            y1_pos = y1_pos.view(y1_pos.size(0), T, -1)\n",
    "            y1_rot = y1_rot.view(y1_rot.size(0), T, -1)\n",
    "            y2_pos = y2_pos.view(y2_pos.size(0), T, -1)\n",
    "            y2_rot = y2_rot.view(y2_rot.size(0), T, -1)\n",
    "\n",
    "        \n",
    "        if train_discrim or train_emb_adv:\n",
    "            # Train the discriminator\n",
    "            if train_emb_this_epoch:\n",
    "                it = 1\n",
    "                if emb_clf_update_per_epoch_paired > 1: it = emb_clf_update_per_epoch_paired\n",
    "                for _ in range(int(it)):\n",
    "                    t_priv_loss, t_priv_coop_loss, t_util_loss, t_util_coop_loss, t_discriminator_loss, t_priv_acc, t_util_acc, t_priv_coop_acc, t_util_coop_acc, t_discriminator_acc  = model.train_adv_paired(x1_pos, x1_rot, x2_pos, x2_rot, y1_pos, y1_rot, y2_pos, y2_rot, actors, actions, train_emb=train_emb_adv, train_discrim=train_discrim)\n",
    "                \n",
    "                # Track the loss\n",
    "                priv_training_loss.append(t_priv_loss)\n",
    "                priv_coop_training_loss.append(t_priv_coop_loss)\n",
    "                priv_training_acc.append(t_priv_acc)\n",
    "                priv_coop_training_acc.append(t_priv_coop_acc)\n",
    "                util_training_loss.append(t_util_loss)\n",
    "                util_coop_training_loss.append(t_util_coop_loss)\n",
    "                util_training_acc.append(t_util_acc)\n",
    "                util_coop_training_acc.append(t_util_coop_acc)\n",
    "                discriminator_train_losses.append(t_discriminator_loss)\n",
    "                discriminator_training_acc.append(t_discriminator_acc)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Train the autoencoder/cross reconstruction\n",
    "        if train_ae or train_cross:\n",
    "            # Forward pass\n",
    "            loss, _, _, _, _, losses_ = model.loss_paired(x1_pos, x1_rot, x2_pos, x2_rot, y1_pos, y1_rot, y2_pos, y2_rot, actors, actions, cross=train_cross, reconstruction=train_ae, emb_adv=use_emb_adv, discrim_adv=use_discrim_adv)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the loss\n",
    "            losses.append(loss.item())\n",
    "            rec_loss.append(losses_['rec_loss'])\n",
    "            cross_loss.append(losses_['cross_loss'])\n",
    "            end_effector_loss.append(losses_['end_effector_loss'])\n",
    "            smoothing_loss.append(losses_['smoothing_loss'])\n",
    "            latent_consistency_loss.append(losses_['latent_consistency_loss'])\n",
    "            triplet_loss.append(losses_['triplet_loss'])\n",
    "            privacy_loss.append(losses_['privacy_loss'])\n",
    "            privacy_loss_adv.append(losses_['privacy_loss_adv'])\n",
    "            privacy_loss_coop.append(losses_['privacy_loss_coop'])\n",
    "            privacy_acc_adv.append(losses_['privacy_acc_adv'])\n",
    "            privacy_acc_coop.append(losses_['privacy_acc_coop'])\n",
    "            utility_loss.append(losses_['utility_loss'])\n",
    "            utility_loss_adv.append(losses_['utility_loss_adv'])\n",
    "            utility_loss_coop.append(losses_['utility_loss_coop'])\n",
    "            utility_acc_adv.append(losses_['utility_acc_adv'])\n",
    "            utility_acc_coop.append(losses_['utility_acc_coop'])\n",
    "            discriminator_loss.append(losses_['discriminator_loss'])\n",
    "            discriminator_training_acc.append(losses_['discriminator_acc'])\n",
    "        \n",
    "    # Decay learning rate (disabled for training stages)\n",
    "    # scheduler.step() \n",
    "\n",
    "    # Validation\n",
    "    if run_eval:\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            val_rec_loss, val_cross_loss, val_end_effector_loss, val_smoothing_loss, val_triplet_loss, val_latent_consistency_loss, val_privacy_loss, val_privacy_loss_adv, val_privacy_loss_coop, val_privacy_acc_adv, val_privacy_acc_coop, val_utility_loss, val_utility_loss_adv, val_utility_loss_coop, val_utility_acc_adv, val_utility_acc_coop, val_discriminator_loss, val_discriminator_acc = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "            \n",
    "            for (x1_pos, x1_rot, x2_pos, x2_rot, y1_pos, y1_rot, y2_pos, y2_rot, actors, actions) in val_dl:\n",
    "                x1_pos, x1_rot, x2_pos, x2_rot, y1_pos, y1_rot, y2_pos, y2_rot = x1_pos.float().to(device), x1_rot.float().to(device), x2_pos.float().to(device), x2_rot.float().to(device), y1_pos.float().to(device), y1_rot.float().to(device), y2_pos.float().to(device), y2_rot.float().to(device)\n",
    "\n",
    "                # Remove rotation data if only using position data\n",
    "                if only_use_pos:\n",
    "                    x1_rot, x2_rot, y1_rot, y2_rot = x1_pos, x2_pos, y1_pos, y2_pos\n",
    "\n",
    "                # For 1D convolutions, flatten the data\n",
    "                if one_dimension_conv:\n",
    "                    x1_pos = x1_pos.view(x1_pos.size(0), T, -1)\n",
    "                    x1_rot = x1_rot.view(x1_rot.size(0), T, -1)\n",
    "                    x2_pos = x2_pos.view(x2_pos.size(0), T, -1)\n",
    "                    x2_rot = x2_rot.view(x2_rot.size(0), T, -1)\n",
    "                    y1_pos = y1_pos.view(y1_pos.size(0), T, -1)\n",
    "                    y1_rot = y1_rot.view(y1_rot.size(0), T, -1)\n",
    "                    y2_pos = y2_pos.view(y2_pos.size(0), T, -1)\n",
    "                    y2_rot = y2_rot.view(y2_rot.size(0), T, -1)\n",
    "                \n",
    "                loss, x1_hat, x2_hat, y1_hat, y2_hat, losses_ = model.loss_paired(x1_pos, x1_rot, x2_pos, x2_rot, y1_pos, y1_rot, y2_pos, y2_rot, actors, actions, cross=train_cross, reconstruction=train_ae, emb_adv=use_emb_adv, discrim_adv=use_discrim_adv)\n",
    "                val_losses.append(loss.item())\n",
    "                val_rec_loss.append(losses_['rec_loss'])\n",
    "                val_cross_loss.append(losses_['cross_loss'])\n",
    "                val_end_effector_loss.append(losses_['end_effector_loss'])\n",
    "                val_smoothing_loss.append(losses_['smoothing_loss'])\n",
    "                val_triplet_loss.append(losses_['triplet_loss'])\n",
    "                val_latent_consistency_loss.append(losses_['latent_consistency_loss'])\n",
    "                val_privacy_loss.append(losses_['privacy_loss'])\n",
    "                val_privacy_loss_adv.append(losses_['privacy_loss_adv'])\n",
    "                val_privacy_loss_coop.append(losses_['privacy_loss_coop'])\n",
    "                val_privacy_acc_adv.append(losses_['privacy_acc_adv'])\n",
    "                val_privacy_acc_coop.append(losses_['privacy_acc_coop'])\n",
    "                val_utility_loss.append(losses_['utility_loss'])\n",
    "                val_utility_loss_adv.append(losses_['utility_loss_adv'])\n",
    "                val_utility_loss_coop.append(losses_['utility_loss_coop'])\n",
    "                val_utility_acc_adv.append(losses_['utility_acc_adv'])\n",
    "                val_utility_acc_coop.append(losses_['utility_acc_coop'])\n",
    "                val_discriminator_loss.append(losses_['discriminator_loss'])\n",
    "                val_discriminator_acc.append(losses_['discriminator_acc'])\n",
    "\n",
    "                if run_sgn_eval:\n",
    "                    if not one_dimension_conv:\n",
    "                        x1_pos = x1_pos.view(x1_pos.size(0), T, -1)\n",
    "                        x2_pos = x2_pos.view(x2_pos.size(0), T, -1)\n",
    "                        y1_pos = y1_pos.view(y1_pos.size(0), T, -1)\n",
    "                        y2_pos = y2_pos.view(y2_pos.size(0), T, -1)\n",
    "\n",
    "                    # x1 = P1, A1\n",
    "                    # x2 = P2, A2\n",
    "                    # y1 = P1, A2\n",
    "                    # y2 = P2, A1\n",
    "                    # actors = A1, A2\n",
    "                    # actions = P1, P2\n",
    "                    # x1hat = P1, A1\n",
    "                    # x2hat = P2, A2\n",
    "                    # y1hat = P2, A1\n",
    "                    # y2hat = P1, A2\n",
    "                        \n",
    "                    # Raw Data X\n",
    "                    eval_X_known.append(x1_pos.cpu().numpy())\n",
    "                    eval_X_known.append(x2_pos.cpu().numpy())\n",
    "                    eval_X_known.append(y1_pos.cpu().numpy())\n",
    "                    eval_X_known.append(y2_pos.cpu().numpy())\n",
    "\n",
    "                    # Raw Data Utility\n",
    "                    eval_Y_known_action.append(actions[0].cpu().numpy())\n",
    "                    eval_Y_known_action.append(actions[1].cpu().numpy())\n",
    "                    eval_Y_known_action.append(actions[1].cpu().numpy())\n",
    "                    eval_Y_known_action.append(actions[0].cpu().numpy())\n",
    "\n",
    "                    # Raw Data Privacy\n",
    "                    eval_Y_known_actor.append(actors[0].cpu().numpy())\n",
    "                    eval_Y_known_actor.append(actors[1].cpu().numpy())\n",
    "                    eval_Y_known_actor.append(actors[0].cpu().numpy())\n",
    "                    eval_Y_known_actor.append(actors[1].cpu().numpy())\n",
    "\n",
    "                    # Reconstruction X\n",
    "                    eval_X_rec.append(x1_hat.cpu().numpy())\n",
    "                    eval_X_rec.append(x2_hat.cpu().numpy())\n",
    "                    # Cross X\n",
    "                    eval_X.append(y1_hat.cpu().numpy()) # P2, A1\n",
    "                    eval_X.append(y2_hat.cpu().numpy()) # P1, A2\n",
    "\n",
    "                    # Reconstruction Utility\n",
    "                    eval_Y_rec_action.append(actions[0].cpu().numpy())\n",
    "                    eval_Y_rec_action.append(actions[1].cpu().numpy())\n",
    "                    # Cross Utility\n",
    "                    eval_Y_action.append(actions[0].cpu().numpy())\n",
    "                    eval_Y_action.append(actions[1].cpu().numpy())\n",
    "\n",
    "                    # Reconstruction Privacy\n",
    "                    eval_Y_rec_actor.append(actors[0].cpu().numpy())\n",
    "                    eval_Y_rec_actor.append(actors[1].cpu().numpy())\n",
    "                    # Cross Privacy\n",
    "                    eval_Y_actor.append(actors[1].cpu().numpy())\n",
    "                    eval_Y_actor.append(actors[0].cpu().numpy())\n",
    "                    # Initial Privacy\n",
    "                    eval_Y_initial_actor.append(actors[0].cpu().numpy())\n",
    "                    eval_Y_initial_actor.append(actors[1].cpu().numpy())\n",
    "\n",
    "    # Print loss/accuracy\n",
    "    print(f'--------------------\\nEpoch {cur_tot_epoch+1}/{total_epochs}\\n--------------------')\n",
    "    cur_tot_epoch += 1\n",
    "    \n",
    "    if train_ae or train_cross:\n",
    "        print(f'Training Loss:\\t\\t\\t{np.mean(losses)}')\n",
    "        if run_eval: print(f'Validation Loss:\\t\\t{np.mean(val_losses)}')\n",
    "        print('\\nTraining Losses:')\n",
    "        print(f'Reconstruction Loss:\\t\\t{np.mean(rec_loss)}\\nCross Reconstruction Loss:\\t{np.mean(cross_loss)}\\nEnd Effector Loss:\\t\\t{np.mean(end_effector_loss)}\\nSmoothing Loss:\\t\\t\\t{np.mean(smoothing_loss)}\\nTriplet Loss:\\t\\t\\t{np.mean(triplet_loss)}\\nLatent Consistency Loss:\\t{np.mean(latent_consistency_loss)}')\n",
    "        if use_emb_adv:\n",
    "            print(f'Privacy Loss:\\t\\t\\t{np.mean(privacy_loss)}\\nPrivacy Loss Dyn:\\t\\t{np.mean(privacy_loss_adv)}\\nPrivacy Loss Stat:\\t\\t{np.mean(privacy_loss_coop)}')\n",
    "            print(f'Utility Loss:\\t\\t\\t{np.mean(utility_loss)}\\nUtility Loss Dyn:\\t\\t{np.mean(utility_loss_adv)}\\nUtility Loss Stat:\\t\\t{np.mean(utility_loss_coop)}')\n",
    "        if use_discrim_adv: print(f'Discriminator Loss:\\t\\t{np.mean(discriminator_loss)}')\n",
    "\n",
    "    if run_eval:\n",
    "        print('\\nValidation Losses:')\n",
    "        print(f'Val Reconstruction Loss:\\t{np.mean(val_rec_loss)}\\nVal Cross Reconstruction Loss:\\t{np.mean(val_cross_loss)}\\nVal End Effector Loss:\\t\\t{np.mean(val_end_effector_loss)}\\nVal Smoothing Loss:\\t\\t{np.mean(val_smoothing_loss)}\\nVal Triplet Loss:\\t\\t{np.mean(val_triplet_loss)}\\nVal Latent Consistency Loss:\\t{np.mean(val_latent_consistency_loss)}')\n",
    "        if use_emb_adv:\n",
    "            print(f'Val Privacy Loss:\\t\\t{np.mean(val_privacy_loss)}\\nVal Privacy Loss Dyn:\\t\\t{np.mean(val_privacy_loss_adv)}\\nVal Privacy Loss Stat:\\t\\t{np.mean(val_privacy_loss_coop)}')\n",
    "            print(f'Val Utility Loss:\\t\\t{np.mean(val_utility_loss)}\\nVal Utility Loss Dyn:\\t\\t{np.mean(val_utility_loss_adv)}\\nVal Utility Loss Stat:\\t\\t{np.mean(val_utility_loss_coop)}')\n",
    "        if use_discrim_adv: print(f'Val Discriminator Loss:\\t\\t{np.mean(val_discriminator_loss)}')\n",
    "    \n",
    "    if train_emb_adv or train_discrim:\n",
    "        print('\\nEmbedding Classifers')\n",
    "        if train_emb_adv and train_emb_this_epoch:\n",
    "            print(f'Adv Privacy Training Loss:\\t\\t{np.mean(priv_training_loss)}\\nAdv Utility Training Loss:\\t\\t{np.mean(util_training_loss)}\\nCoop Privacy Training Loss:\\t{np.mean(priv_coop_training_loss)}\\nCoop Utility Training Loss:\\t{np.mean(util_coop_training_loss)}\\nDiscriminator Training Loss:\\t{np.mean(discriminator_train_losses)}')\n",
    "            print(f'Adv Privacy Training Acc:\\t\\t{np.mean(priv_training_acc)}\\nAdv Utility Training Acc:\\t\\t{np.mean(util_training_acc)}\\nCoop Privacy Training Acc:\\t\\t{np.mean(priv_coop_training_acc)}\\nCoop Utility Training Acc:\\t\\t{np.mean(util_coop_training_acc)}\\nDiscriminator Training Acc:\\t{np.mean(discriminator_training_acc)}')\n",
    "            if train_ae or train_cross: print(f'Privacy Acc Adv:\\t\\t{np.mean(privacy_acc_adv)}\\nPrivacy Acc Coop:\\t\\t{np.mean(privacy_acc_coop)}\\nUtility Acc Adv:\\t\\t{np.mean(utility_acc_adv)}\\nUtility Acc Coop:\\t\\t{np.mean(utility_acc_coop)}')\n",
    "            if run_eval: print(f'Val Privacy Acc Adv:\\t\\t{np.mean(val_privacy_acc_adv)}\\nVal Privacy Acc Coop:\\t\\t{np.mean(val_privacy_acc_coop)}\\nVal Utility Acc Adv:\\t\\t{np.mean(val_utility_acc_adv)}\\nVal Utility Acc Coop:\\t\\t{np.mean(val_utility_acc_coop)}')\n",
    "    \n",
    "    if train_ae or train_cross: print(f'Discriminator Acc:\\t\\t{np.mean(discriminator_training_acc)}')\n",
    "    if run_eval: print(f'Val Discriminator Acc:\\t\\t{np.mean(val_discriminator_acc)}')\n",
    "\n",
    "    # Test Accuracy\n",
    "    if run_sgn_eval and run_eval:\n",
    "        print('\\n')\n",
    "        sgn_acc_known_acc, sgn_acc_known_f1, sgn_acc_known_prec, sgn_acc_known_recall, sgn_acc_known_topk = sgn_eval(eval_X_known, eval_Y_known_action, 'Known Action', is_action=True, k=k)\n",
    "        sgn_acc_rec_acc, sgn_acc_rec_f1, sgn_acc_rec_prec, sgn_acc_rec_recall, sgn_acc_rec_topk = sgn_eval(eval_X_rec, eval_Y_rec_action, 'Reconstructed Action', is_action=True, k=k)\n",
    "        sgn_acc_cross_acc, sgn_acc_cross_f1, sgn_acc_cross_prec, sgn_acc_cross_recall, sgn_acc_cross_topk = sgn_eval(eval_X, eval_Y_action, 'Generated Action', is_action=True, k=k)\n",
    "        print('\\n')\n",
    "        sgn_priv_known_acc, sgn_priv_known_f1, sgn_priv_known_prec, sgn_priv_known_recall, sgn_priv_known_topk = sgn_eval(eval_X_known, eval_Y_known_actor, 'Known Actor', is_actor=True, k=k)\n",
    "        sgn_priv_rec_acc, sgn_priv_rec_f1, sgn_priv_rec_prec, sgn_priv_rec_recall, sgn_priv_rec_topk = sgn_eval(eval_X_rec, eval_Y_rec_actor, 'Reconstructed Actor', is_actor=True, k=k)\n",
    "        sgn_priv_cross_acc, sgn_priv_cross_f1, sgn_priv_cross_prec, sgn_priv_cross_recall, sgn_priv_cross_topk = sgn_eval(eval_X, eval_Y_actor, 'Generated Actor', is_actor=True, k=k)\n",
    "        sgn_priv_initial_acc, sgn_priv_initial_f1, sgn_priv_initial_prec, sgn_priv_initial_recall, sgn_priv_initial_topk = sgn_eval(eval_X, eval_Y_initial_actor, 'Initial Actor', is_actor=True, k=k)\n",
    "    else: print('\\n')\n",
    "\n",
    "    # Return dict with all losses and accuracies for plotting\n",
    "    losses_dict = {}\n",
    "    if train_ae or train_cross:\n",
    "        losses_dict['loss'] = np.mean(losses)\n",
    "        if run_eval: losses_dict['val_loss'] = np.mean(val_losses)\n",
    "        losses_dict['rec_loss'] = np.mean(rec_loss)\n",
    "        losses_dict['cross_loss'] = np.mean(cross_loss)\n",
    "        losses_dict['end_effector_loss'] = np.mean(end_effector_loss)\n",
    "        losses_dict['smoothing_loss'] = np.mean(smoothing_loss)\n",
    "        losses_dict['triplet_loss'] = np.mean(triplet_loss)\n",
    "        losses_dict['latent_consistency_loss'] = np.mean(latent_consistency_loss)\n",
    "        losses_dict['privacy_loss'] = np.mean(privacy_loss)\n",
    "        losses_dict['privacy_loss_adv'] = np.mean(privacy_loss_adv)\n",
    "        losses_dict['privacy_loss_coop'] = np.mean(privacy_loss_coop)\n",
    "        losses_dict['utility_loss'] = np.mean(utility_loss)\n",
    "        losses_dict['utility_loss_adv'] = np.mean(utility_loss_adv)\n",
    "        losses_dict['utility_loss_coop'] = np.mean(utility_loss_coop)\n",
    "        losses_dict['discriminator_loss'] = np.mean(discriminator_loss)\n",
    "    if run_eval:\n",
    "        losses_dict['val_rec_loss'] = np.mean(val_rec_loss)\n",
    "        losses_dict['val_cross_loss'] = np.mean(val_cross_loss)\n",
    "        losses_dict['val_end_effector_loss'] = np.mean(val_end_effector_loss)\n",
    "        losses_dict['val_smoothing_loss'] = np.mean(val_smoothing_loss)\n",
    "        losses_dict['val_triplet_loss'] = np.mean(val_triplet_loss)\n",
    "        losses_dict['val_latent_consistency_loss'] = np.mean(val_latent_consistency_loss)\n",
    "        losses_dict['val_privacy_loss'] = np.mean(val_privacy_loss)\n",
    "        losses_dict['val_privacy_loss_adv'] = np.mean(val_privacy_loss_adv)\n",
    "        losses_dict['val_privacy_loss_coop'] = np.mean(val_privacy_loss_coop)\n",
    "        losses_dict['val_utility_loss'] = np.mean(val_utility_loss)\n",
    "        losses_dict['val_utility_loss_adv'] = np.mean(val_utility_loss_adv)\n",
    "        losses_dict['val_utility_loss_coop'] = np.mean(val_utility_loss_coop)\n",
    "        losses_dict['val_discriminator_loss'] = np.mean(val_discriminator_loss)\n",
    "    if (train_emb_adv or train_discrim) and train_emb_this_epoch:\n",
    "        losses_dict['priv_training_loss'] = np.mean(priv_training_loss)\n",
    "        losses_dict['util_training_loss'] = np.mean(util_training_loss)\n",
    "        losses_dict['discriminator_train_loss'] = np.mean(discriminator_train_losses)\n",
    "        losses_dict['priv_training_acc'] = np.mean(priv_training_acc)\n",
    "        losses_dict['util_training_acc'] = np.mean(util_training_acc)\n",
    "        losses_dict['priv_coop_training_loss'] = np.mean(priv_coop_training_loss)\n",
    "        losses_dict['priv_coop_training_acc'] = np.mean(priv_coop_training_acc)\n",
    "        losses_dict['util_coop_training_loss'] = np.mean(util_coop_training_loss)\n",
    "        losses_dict['util_coop_training_acc'] = np.mean(util_coop_training_acc)\n",
    "        losses_dict['discriminator_training_acc'] = np.mean(discriminator_training_acc)\n",
    "        if train_ae or train_cross:\n",
    "            losses_dict['privacy_acc_adv'] = np.mean(privacy_acc_adv)\n",
    "            losses_dict['privacy_acc_coop'] = np.mean(privacy_acc_coop)\n",
    "            losses_dict['utility_acc_adv'] = np.mean(utility_acc_adv)\n",
    "            losses_dict['utility_acc_coop'] = np.mean(utility_acc_coop)\n",
    "        if run_eval:\n",
    "            losses_dict['val_privacy_acc_adv'] = np.mean(val_privacy_acc_adv)\n",
    "            losses_dict['val_privacy_acc_coop'] = np.mean(val_privacy_acc_coop)\n",
    "            losses_dict['val_utility_acc_adv'] = np.mean(val_utility_acc_adv)\n",
    "            losses_dict['val_utility_acc_coop'] = np.mean(val_utility_acc_coop)\n",
    "    if train_ae or train_cross:\n",
    "        losses_dict['discriminator_acc'] = np.mean(discriminator_training_acc)\n",
    "    if run_eval:\n",
    "        losses_dict['val_discriminator_acc'] = np.mean(val_discriminator_acc)\n",
    "    if run_sgn_eval and run_eval:\n",
    "        losses_dict['sgn_acc_known_acc'] = sgn_acc_known_acc\n",
    "        losses_dict['sgn_acc_known_f1'] = sgn_acc_known_f1\n",
    "        losses_dict['sgn_acc_known_prec'] = sgn_acc_known_prec\n",
    "        losses_dict['sgn_acc_known_recall'] = sgn_acc_known_recall\n",
    "        losses_dict['sgn_acc_rec_acc'] = sgn_acc_rec_acc\n",
    "        losses_dict['sgn_acc_rec_f1'] = sgn_acc_rec_f1\n",
    "        losses_dict['sgn_acc_rec_prec'] = sgn_acc_rec_prec\n",
    "        losses_dict['sgn_acc_rec_recall'] = sgn_acc_rec_recall\n",
    "        losses_dict['sgn_acc_cross_acc'] = sgn_acc_cross_acc\n",
    "        losses_dict['sgn_acc_cross_f1'] = sgn_acc_cross_f1\n",
    "        losses_dict['sgn_acc_cross_prec'] = sgn_acc_cross_prec\n",
    "        losses_dict['sgn_acc_cross_recall'] = sgn_acc_cross_recall\n",
    "        losses_dict['sgn_priv_known_acc'] = sgn_priv_known_acc\n",
    "        losses_dict['sgn_priv_known_f1'] = sgn_priv_known_f1\n",
    "        losses_dict['sgn_priv_known_prec'] = sgn_priv_known_prec\n",
    "        losses_dict['sgn_priv_known_recall'] = sgn_priv_known_recall\n",
    "        losses_dict['sgn_priv_rec_acc'] = sgn_priv_rec_acc\n",
    "        losses_dict['sgn_priv_rec_f1'] = sgn_priv_rec_f1\n",
    "        losses_dict['sgn_priv_rec_prec'] = sgn_priv_rec_prec\n",
    "        losses_dict['sgn_priv_rec_recall'] = sgn_priv_rec_recall\n",
    "        losses_dict['sgn_priv_cross_acc'] = sgn_priv_cross_acc\n",
    "        losses_dict['sgn_priv_cross_f1'] = sgn_priv_cross_f1\n",
    "        losses_dict['sgn_priv_cross_prec'] = sgn_priv_cross_prec\n",
    "        losses_dict['sgn_priv_cross_recall'] = sgn_priv_cross_recall\n",
    "        losses_dict['sgn_acc_known_topk'] = sgn_acc_known_topk\n",
    "        losses_dict['sgn_acc_rec_topk'] = sgn_acc_rec_topk\n",
    "        losses_dict['sgn_acc_cross_topk'] = sgn_acc_cross_topk\n",
    "        losses_dict['sgn_priv_known_topk'] = sgn_priv_known_topk\n",
    "        losses_dict['sgn_priv_rec_topk'] = sgn_priv_rec_topk\n",
    "        losses_dict['sgn_priv_cross_topk'] = sgn_priv_cross_topk\n",
    "        losses_dict['sgn_priv_initial_acc'] = sgn_priv_initial_acc\n",
    "        losses_dict['sgn_priv_initial_f1'] = sgn_priv_initial_f1\n",
    "        losses_dict['sgn_priv_initial_prec'] = sgn_priv_initial_prec\n",
    "        losses_dict['sgn_priv_initial_recall'] = sgn_priv_initial_recall\n",
    "        losses_dict['sgn_priv_initial_topk'] = sgn_priv_initial_topk\n",
    "    \n",
    "    # Save model\n",
    "    if save and metric in losses_dict and losses_dict[metric] > 0:\n",
    "        if matric_minimize:\n",
    "            if np.mean(val_losses) < best_metric:\n",
    "                best_metric = np.mean(val_losses)\n",
    "                torch.save(model.state_dict(), 'pretrained/MR.pt')\n",
    "        elif np.mean(val_losses) > best_metric:\n",
    "            best_metric = np.mean(val_losses)\n",
    "            torch.save(model.state_dict(), 'pretrained/MR.pt')\n",
    "\n",
    "    return losses_dict\n",
    "\n",
    "\n",
    "def sgn_eval(X, Y, label='Undefined', is_actor=False, is_action=False, k=3):\n",
    "    assert is_actor != is_action, \"is_actor and is_action cannot both be True\"\n",
    "    assert is_actor or is_action, \"Either is_actor or is_action must be True\"\n",
    "\n",
    "    if is_actor:\n",
    "        classes = privacy_classes\n",
    "        sgn = sgn_priv\n",
    "    elif is_action:\n",
    "        classes = utility_classes\n",
    "        sgn = sgn_ar\n",
    "\n",
    "    X = np.concatenate(X)\n",
    "    X = np.pad(X, ((0,0), (0,225), (0,75)), 'constant')\n",
    "\n",
    "    Y = np.concatenate(Y) - 1\n",
    "    Y = np.eye(classes)[Y.astype(int)]\n",
    "\n",
    "    acc, f1, prec, recall, topk = run_sgn_eval(sgn_train_x, sgn_train_y, X, Y, sgn_val_x, sgn_val_y, 1, sgn, k=k)\n",
    "    print(f'\\n{label} Accuracy:\\t\\t{acc}\\n{label} F1:\\t\\t\\t{f1*100}\\n{label} Precision:\\t\\t{prec*100}\\n{label} Recall:\\t\\t{recall*100}\\n{label} Top-{k} Accuracy:\\t{topk}\\n')\n",
    "    return acc, f1, prec, recall, topk\n",
    "\n",
    "# Simplified training loop for only AE\n",
    "def train_unpaired(run_eval=True, run_sgn_eval=True, save=True, ae=True, ee=False, triplet=False, use_emb_adv=False, use_discrim_adv=False, emb_adv=False, discrim_adv=False, k=3, smoothing=True):\n",
    "    global best_metric\n",
    "    global total_epochs\n",
    "    global cur_tot_epoch\n",
    "\n",
    "    # Store eval values for validation\n",
    "    eval_X_known, eval_Y_known_action, eval_Y_known_actor, eval_X_rec, eval_Y_rec_action, eval_Y_rec_actor = [], [], [], [], [], []\n",
    "    \n",
    "    # Losses for printing\n",
    "    rec_loss, end_effector_loss, smoothing_loss, triplet_loss, privacy_loss, privacy_loss_adv, privacy_loss_coop, privacy_acc_adv, privacy_acc_coop, priv_training_loss, utility_loss, utility_loss_adv, utility_loss_coop, utility_acc_adv, utility_acc_coop, util_training_loss, discriminator_loss, discriminator_train_losses, discriminator_acc, discriminator_train_accs, priv_coop_training_loss, priv_training_acc, priv_coop_training_acc, util_coop_training_loss, util_training_acc, util_coop_training_acc = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "    val_rec_loss, val_end_effector_loss, val_smoothing_loss, val_triplet_loss, val_privacy_loss, val_privacy_loss_adv, val_privacy_loss_coop, val_privacy_acc_adv, val_privacy_acc_coop, val_utility_loss, val_utility_loss_adv, val_utility_loss_coop, val_utility_acc_adv, val_utility_acc_coop, val_discriminator_loss, val_discriminator_acc = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    # Determine if adversaries need to be trained\n",
    "    train_emb_this_epoch = True\n",
    "    if emb_clf_update_per_epoch_unpaired < 1 and ae:\n",
    "        if cur_tot_epoch % round(1 / emb_clf_update_per_epoch_unpaired) != 0:\n",
    "            train_emb_this_epoch = False\n",
    "\n",
    "    for (x, actors, actions) in rec_train_dl:\n",
    "        # Move tensors to the configured device\n",
    "        x = x.float().to(device)\n",
    "\n",
    "        # Split into position and rotation\n",
    "        if only_use_pos:\n",
    "            x_pos = x\n",
    "            x_rot = x\n",
    "        else:\n",
    "            x_pos = x[:, :, :, :3]\n",
    "            x_rot = x[:, :, :, 3:]\n",
    "\n",
    "        # Train adversaries\n",
    "        if emb_adv or discrim_adv:\n",
    "            # Train the discriminator\n",
    "            if train_emb_this_epoch:\n",
    "                it = 1\n",
    "                if emb_clf_update_per_epoch_unpaired > 1: it = emb_clf_update_per_epoch_unpaired\n",
    "                for _ in range(int(it)):\n",
    "                    priv_train_loss, priv_train_coop_loss, util_train_loss, util_train_coop_loss, discriminator_train_loss, priv_acc, util_acc, priv_coop_acc, util_coop_acc, discriminator_train_acc = model.train_adv_unpaired(x_pos, x_rot, actors, actions, train_emb=emb_adv, train_discrim=discrim_adv)\n",
    "            \n",
    "                # Track the loss\n",
    "                priv_training_loss.append(priv_train_loss)\n",
    "                priv_coop_training_loss.append(priv_train_coop_loss)\n",
    "                priv_training_acc.append(priv_acc)\n",
    "                priv_coop_training_acc.append(priv_coop_acc)\n",
    "                util_training_loss.append(util_train_loss)\n",
    "                util_coop_training_loss.append(util_train_coop_loss)\n",
    "                util_training_acc.append(util_acc)\n",
    "                util_coop_training_acc.append(util_coop_acc)\n",
    "                discriminator_train_losses.append(discriminator_train_loss)\n",
    "                discriminator_train_accs.append(discriminator_train_acc)\n",
    "        \n",
    "        if not ae: continue\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss, _, losses_ = model.loss_unpaired(x_pos, x_rot, actors, actions, reconstruction=ae, emb_adv=use_emb_adv, discrim_adv=use_discrim_adv, ee=ee, triplet=triplet)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        rec_loss.append(losses_['rec_loss'])\n",
    "        end_effector_loss.append(losses_['end_effector_loss'])\n",
    "        smoothing_loss.append(losses_['smoothing_loss'])\n",
    "        triplet_loss.append(losses_['triplet_loss'])\n",
    "        privacy_loss.append(losses_['privacy_loss'])\n",
    "        privacy_loss_adv.append(losses_['privacy_loss_adv'])\n",
    "        privacy_loss_coop.append(losses_['privacy_loss_coop'])\n",
    "        privacy_acc_adv.append(losses_['privacy_acc_adv'])\n",
    "        privacy_acc_coop.append(losses_['privacy_acc_coop'])\n",
    "        utility_loss.append(losses_['utility_loss'])\n",
    "        utility_loss_adv.append(losses_['utility_loss_adv'])\n",
    "        utility_loss_coop.append(losses_['utility_loss_coop'])\n",
    "        utility_acc_adv.append(losses_['utility_acc_adv'])\n",
    "        utility_acc_coop.append(losses_['utility_acc_coop'])\n",
    "        discriminator_loss.append(losses_['discriminator_loss'])\n",
    "        discriminator_acc.append(losses_['discriminator_acc'])\n",
    "\n",
    "\n",
    "    # Decay learning rate\n",
    "    # scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    if run_eval:\n",
    "        with torch.no_grad():\n",
    "            for (x, actors, actions) in rec_val_dl:\n",
    "                x = x.float().to(device)\n",
    "\n",
    "                # Split into position and rotation\n",
    "                if only_use_pos:\n",
    "                    x_pos = x\n",
    "                    x_rot = x\n",
    "                else:\n",
    "                    x_pos = x[:, :, :, :3]\n",
    "                    x_rot = x[:, :, :, 3:]\n",
    "                \n",
    "                loss, _, losses_ = model.loss_unpaired(x_pos, x_rot, actors, actions, reconstruction=ae, emb_adv=use_emb_adv, discrim_adv=use_discrim_adv, ee=ee, triplet=triplet)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                if run_sgn_eval:\n",
    "                    eval_X_known.append(x_pos.contiguous().view(x_pos.size(0), T, -1).cpu().numpy())\n",
    "                    eval_Y_known_action.append(np.array(actions))\n",
    "                    eval_Y_known_actor.append(np.array(actors))\n",
    "\n",
    "                    eval_X_rec.append(model(x_pos, x_rot).cpu().numpy())\n",
    "                    eval_Y_rec_action.append(np.array(actions))\n",
    "                    eval_Y_rec_actor.append(np.array(actors))\n",
    "                \n",
    "                val_rec_loss.append(losses_['rec_loss'])\n",
    "                val_end_effector_loss.append(losses_['end_effector_loss'])\n",
    "                val_smoothing_loss.append(losses_['smoothing_loss'])\n",
    "                val_triplet_loss.append(losses_['triplet_loss'])\n",
    "                val_privacy_loss.append(losses_['privacy_loss'])\n",
    "                val_privacy_loss_adv.append(losses_['privacy_loss_adv'])\n",
    "                val_privacy_loss_coop.append(losses_['privacy_loss_coop'])\n",
    "                val_privacy_acc_adv.append(losses_['privacy_acc_adv'])\n",
    "                val_privacy_acc_coop.append(losses_['privacy_acc_coop'])\n",
    "                val_utility_loss.append(losses_['utility_loss'])\n",
    "                val_utility_loss_adv.append(losses_['utility_loss_adv'])\n",
    "                val_utility_loss_coop.append(losses_['utility_loss_coop'])\n",
    "                val_utility_acc_adv.append(losses_['utility_acc_adv'])\n",
    "                val_utility_acc_coop.append(losses_['utility_acc_coop'])\n",
    "                val_discriminator_loss.append(losses_['discriminator_loss'])\n",
    "                val_discriminator_acc.append(losses_['discriminator_acc'])\n",
    "\n",
    "    # Print loss/accuracy\n",
    "    print(f'--------------------\\nEpoch {cur_tot_epoch+1}/{total_epochs}\\n--------------------')\n",
    "    cur_tot_epoch += 1\n",
    "    if ae:\n",
    "        print(f'Training Loss:\\t\\t\\t{np.mean(losses)}\\nValidation Loss:\\t\\t{np.mean(val_losses)}\\n')\n",
    "        print('Training Losses:')\n",
    "        print(f'Reconstruction Loss:\\t\\t{np.mean(rec_loss)}\\nEnd Effector Loss:\\t\\t{np.mean(end_effector_loss)}\\nSmoothing Loss:\\t\\t\\t{np.mean(smoothing_loss)}\\nTriplet Loss:\\t\\t\\t{np.mean(triplet_loss)}')\n",
    "        if use_emb_adv:\n",
    "            print(f'Privacy Loss:\\t\\t\\t{np.mean(privacy_loss)}\\nPrivacy Loss Dyn:\\t\\t{np.mean(privacy_loss_adv)}\\nPrivacy Loss Stat:\\t\\t{np.mean(privacy_loss_coop)}')\n",
    "            print(f'Utility Loss:\\t\\t\\t{np.mean(utility_loss)}\\nUtility Loss Dyn:\\t\\t{np.mean(utility_loss_adv)}\\nUtility Loss Stat:\\t\\t{np.mean(utility_loss_coop)}')\n",
    "        if use_discrim_adv: print(f'Discriminator Loss:\\t\\t{np.mean(discriminator_loss)}')\n",
    "    if run_eval:\n",
    "        print('\\nValidation Losses:')\n",
    "        print(f'Val Reconstruction Loss:\\t{np.mean(val_rec_loss)}\\nVal End Effector Loss:\\t\\t{np.mean(val_end_effector_loss)}\\nVal Smoothing Loss:\\t\\t{np.mean(val_smoothing_loss)}\\nVal Triplet Loss:\\t\\t{np.mean(val_triplet_loss)}')\n",
    "        if use_emb_adv:\n",
    "            print(f'Val Privacy Loss:\\t\\t{np.mean(val_privacy_loss)}\\nVal Privacy Loss Dyn:\\t\\t{np.mean(val_privacy_loss_adv)}\\nVal Privacy Loss Stat:\\t\\t{np.mean(val_privacy_loss_coop)}')\n",
    "            print(f'Val Utility Loss:\\t\\t{np.mean(val_utility_loss)}\\nVal Utility Loss Dyn:\\t\\t{np.mean(val_utility_loss_adv)}\\nVal Utility Loss Stat:\\t\\t{np.mean(val_utility_loss_coop)}')\n",
    "        if use_discrim_adv: print(f'Val Discriminator Loss:\\t\\t{np.mean(val_discriminator_loss)}')\n",
    "    if (emb_adv or discrim_adv) and train_emb_this_epoch:\n",
    "        print('\\nAdversary Losses')\n",
    "        print(f'Privacy Training Loss:\\t\\t{np.mean(priv_training_loss)}\\nUtility Training Loss:\\t\\t{np.mean(util_training_loss)}\\nDiscriminator Training Loss:\\t{np.mean(discriminator_train_losses)}')\n",
    "        print(f'Privacy Training Acc:\\t\\t{np.mean(priv_training_acc)}\\nUtility Training Acc:\\t\\t{np.mean(util_training_acc)}\\nDiscriminator Training Acc:\\t{np.mean(discriminator_train_accs)}')\n",
    "        print(f'Privacy Training Coop Loss:\\t{np.mean(priv_coop_training_loss)}\\nUtility Training Coop Loss:\\t{np.mean(util_coop_training_loss)}')\n",
    "        print(f'Privacy Training Coop Acc:\\t{np.mean(priv_coop_training_acc)}\\nUtility Training Coop Acc:\\t{np.mean(util_coop_training_acc)}')\n",
    "        if emb_adv and ae:\n",
    "            print(f'Privacy Acc Adv:\\t\\t{np.mean(privacy_acc_adv)}\\nPrivacy Acc Coop:\\t\\t{np.mean(privacy_acc_coop)}\\nUtility Acc Adv:\\t\\t{np.mean(utility_acc_adv)}\\nUtility Acc Coop:\\t\\t{np.mean(utility_acc_coop)}')\n",
    "            if run_eval: print(f'Val Privacy Acc Adv:\\t\\t{np.mean(val_privacy_acc_adv)}\\nVal Privacy Acc Coop:\\t\\t{np.mean(val_privacy_acc_coop)}\\nVal Utility Acc Adv:\\t\\t{np.mean(val_utility_acc_adv)}\\nVal Utility Acc Coop:\\t\\t{np.mean(val_utility_acc_coop)}')\n",
    "        if discrim_adv and ae:\n",
    "            print(f'Discriminator Acc:\\t\\t{np.mean(discriminator_acc)}')\n",
    "            if run_eval: print(f'Val Discriminator Acc:\\t\\t{np.mean(val_discriminator_acc)}')\n",
    "\n",
    "\n",
    "    # Test Accuracy\n",
    "    if run_sgn_eval and run_eval:\n",
    "        print('\\n')\n",
    "        sgn_acc_known_acc, sgn_acc_known_f1, sgn_acc_known_prec, sgn_acc_known_recall, sgn_acc_known_topk = sgn_eval(eval_X_known, eval_Y_known_action, 'Known Action', is_action=True, k=k)\n",
    "        sgn_acc_rec_acc, sgn_acc_rec_f1, sgn_acc_rec_prec, sgn_acc_rec_recall, sgn_acc_rec_topk = sgn_eval(eval_X_rec, eval_Y_rec_action, 'Reconstructed Action', is_action=True, k=k)\n",
    "        print('\\n')\n",
    "        sgn_priv_known_acc, sgn_priv_known_f1, sgn_priv_known_prec, sgn_priv_known_recall, sgn_priv_known_topk = sgn_eval(eval_X_known, eval_Y_known_actor, 'Known Actor', is_actor=True, k=k)\n",
    "        sgn_priv_rec_acc, sgn_priv_rec_f1, sgn_priv_rec_prec, sgn_priv_rec_recall, sgn_priv_rec_topk = sgn_eval(eval_X_rec, eval_Y_rec_actor, 'Reconstructed Actor', is_actor=True, k=k)\n",
    "        print('\\n')\n",
    "    else: print('\\n')\n",
    "\n",
    "    losses_dict = {}\n",
    "    losses_dict['loss'] = np.mean(losses)\n",
    "\n",
    "    if ae: losses_dict['rec_loss'] = np.mean(rec_loss)\n",
    "    if ee: losses_dict['end_effector_loss'] = np.mean(end_effector_loss)\n",
    "    if smoothing: losses_dict['smoothing_loss'] = np.mean(smoothing_loss)\n",
    "    if triplet: losses_dict['triplet_loss'] = np.mean(triplet_loss)\n",
    "    if run_eval:\n",
    "        losses_dict['val_loss'] = np.mean(val_losses)\n",
    "        if ae: losses_dict['val_rec_loss'] = np.mean(val_rec_loss)\n",
    "        if ee: losses_dict['val_end_effector_loss'] = np.mean(val_end_effector_loss)\n",
    "        if smoothing: losses_dict['val_smoothing_loss'] = np.mean(val_smoothing_loss)\n",
    "        if triplet: losses_dict['val_triplet_loss'] = np.mean(val_triplet_loss)\n",
    "        if run_sgn_eval:\n",
    "            losses_dict['sgn_acc_known_acc'] = sgn_acc_known_acc\n",
    "            losses_dict['sgn_acc_known_f1'] = sgn_acc_known_f1\n",
    "            losses_dict['sgn_acc_known_prec'] = sgn_acc_known_prec\n",
    "            losses_dict['sgn_acc_known_recall'] = sgn_acc_known_recall\n",
    "            losses_dict['sgn_acc_rec_acc'] = sgn_acc_rec_acc\n",
    "            losses_dict['sgn_acc_rec_f1'] = sgn_acc_rec_f1\n",
    "            losses_dict['sgn_acc_rec_prec'] = sgn_acc_rec_prec\n",
    "            losses_dict['sgn_acc_rec_recall'] = sgn_acc_rec_recall\n",
    "            losses_dict['sgn_acc_known_topk'] = sgn_acc_known_topk\n",
    "            losses_dict['sgn_acc_rec_topk'] = sgn_acc_rec_topk\n",
    "            losses_dict['sgn_priv_known_acc'] = sgn_priv_known_acc\n",
    "            losses_dict['sgn_priv_known_f1'] = sgn_priv_known_f1\n",
    "            losses_dict['sgn_priv_known_prec'] = sgn_priv_known_prec\n",
    "            losses_dict['sgn_priv_known_recall'] = sgn_priv_known_recall\n",
    "            losses_dict['sgn_priv_rec_acc'] = sgn_priv_rec_acc\n",
    "            losses_dict['sgn_priv_rec_f1'] = sgn_priv_rec_f1\n",
    "            losses_dict['sgn_priv_rec_prec'] = sgn_priv_rec_prec\n",
    "            losses_dict['sgn_priv_rec_recall'] = sgn_priv_rec_recall\n",
    "            losses_dict['sgn_priv_known_topk'] = sgn_priv_known_topk\n",
    "            losses_dict['sgn_priv_rec_topk'] = sgn_priv_rec_topk\n",
    "    if use_emb_adv:\n",
    "        losses_dict['privacy_loss'] = np.mean(privacy_loss)\n",
    "        losses_dict['privacy_loss_adv'] = np.mean(privacy_loss_adv)\n",
    "        losses_dict['privacy_loss_coop'] = np.mean(privacy_loss_coop)\n",
    "        losses_dict['utility_loss'] = np.mean(utility_loss)\n",
    "        losses_dict['utility_loss_adv'] = np.mean(utility_loss_adv)\n",
    "        losses_dict['utility_loss_coop'] = np.mean(utility_loss_coop)\n",
    "        losses_dict['privacy_acc_adv'] = np.mean(privacy_acc_adv)\n",
    "        losses_dict['privacy_acc_coop'] = np.mean(privacy_acc_coop)\n",
    "        losses_dict['utility_acc_adv'] = np.mean(utility_acc_adv)\n",
    "        losses_dict['utility_acc_coop'] = np.mean(utility_acc_coop)\n",
    "        if run_eval:    \n",
    "            losses_dict['val_privacy_acc_adv'] = np.mean(val_privacy_acc_adv)\n",
    "            losses_dict['val_privacy_acc_coop'] = np.mean(val_privacy_acc_coop)\n",
    "            losses_dict['val_utility_acc_adv'] = np.mean(val_utility_acc_adv)\n",
    "            losses_dict['val_utility_acc_coop'] = np.mean(val_utility_acc_coop)\n",
    "            losses_dict['val_privacy_loss'] = np.mean(val_privacy_loss)\n",
    "            losses_dict['val_privacy_loss_adv'] = np.mean(val_privacy_loss_adv)\n",
    "            losses_dict['val_privacy_loss_coop'] = np.mean(val_privacy_loss_coop)\n",
    "            losses_dict['val_utility_loss'] = np.mean(val_utility_loss)\n",
    "            losses_dict['val_utility_loss_adv'] = np.mean(val_utility_loss_adv)\n",
    "            losses_dict['val_utility_loss_coop'] = np.mean(val_utility_loss_coop)\n",
    "    if emb_adv and train_emb_this_epoch:\n",
    "        losses_dict['priv_training_loss'] = np.mean(priv_training_loss)\n",
    "        losses_dict['priv_training_acc'] = np.mean(priv_training_acc)\n",
    "        losses_dict['priv_coop_training_loss'] = np.mean(priv_coop_training_loss)\n",
    "        losses_dict['priv_coop_training_acc'] = np.mean(priv_coop_training_acc)\n",
    "        losses_dict['util_training_loss'] = np.mean(util_training_loss)\n",
    "        losses_dict['util_training_acc'] = np.mean(util_training_acc)\n",
    "        losses_dict['util_coop_training_loss'] = np.mean(util_coop_training_loss)\n",
    "        losses_dict['util_coop_training_acc'] = np.mean(util_coop_training_acc)\n",
    "    if use_discrim_adv:\n",
    "        losses_dict['discriminator_loss'] = np.mean(discriminator_loss)\n",
    "        losses_dict['discriminator_acc'] = np.mean(discriminator_acc)\n",
    "        if run_eval:\n",
    "            losses_dict['val_discriminator_loss'] = np.mean(val_discriminator_loss)\n",
    "            losses_dict['val_discriminator_acc'] = np.mean(val_discriminator_acc)\n",
    "    if discrim_adv and train_emb_this_epoch:\n",
    "        losses_dict['discriminator_train_loss'] = np.mean(discriminator_train_losses)\n",
    "        losses_dict['discriminator_train_acc'] = np.mean(discriminator_train_accs)\n",
    "        \n",
    "    # Save model\n",
    "    if save and metric in losses_dict and losses_dict[metric] > 0:\n",
    "        if matric_minimize:\n",
    "            if np.mean(val_losses) < best_metric:\n",
    "                best_metric = np.mean(val_losses)\n",
    "                torch.save(model.state_dict(), 'pretrained/MR.pt')\n",
    "        elif np.mean(val_losses) > best_metric:\n",
    "            best_metric = np.mean(val_losses)\n",
    "            torch.save(model.state_dict(), 'pretrained/MR.pt')\n",
    "\n",
    "    return losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stages = [\n",
    "    # Pre-Train Cross to separate embeddings\n",
    "    {'epochs': 5, 'paired': True, 'ae': True, 'ee': True, 'cross': True, 'triplet': True, 'train_emb_adv': False, 'train_discrim_adv': False, 'emb_adv': False, 'discrim_adv': False, 'eval': False, 'sgn_eval': False, 'save': False},\n",
    "    \n",
    "    # Pre-Train AE\n",
    "    {'epochs': 20, 'paired': False, 'ae': True, 'ee': True, 'cross': False, 'triplet': True, 'train_emb_adv': False, 'train_discrim_adv': False, 'emb_adv': False, 'discrim_adv': False, 'eval': False, 'sgn_eval': False, 'save': False},\n",
    "    \n",
    "    # Pre-Train Adversaries (Paired)\n",
    "    {'epochs': 20, 'paired': True, 'ae': False, 'ee': False, 'cross': False, 'triplet': False, 'train_emb_adv': True, 'train_discrim_adv': True, 'emb_adv': False, 'discrim_adv': False, 'eval': False, 'sgn_eval': False, 'save': False},\n",
    "    \n",
    "    # Pre-Train Adversaries\n",
    "    {'epochs': 50, 'paired': False, 'ae': False, 'ee': False, 'cross': False, 'triplet': False, 'train_emb_adv': True, 'train_discrim_adv': True, 'emb_adv': False, 'discrim_adv': False, 'eval': False, 'sgn_eval': False, 'save': False},\n",
    "    \n",
    "    # Train AE and adversaries with adversary loss\n",
    "    {'epochs': 100, 'paired': False, 'ae': True, 'ee': True, 'cross': False, 'triplet': True, 'train_emb_adv': True, 'train_discrim_adv': True, 'emb_adv': True, 'discrim_adv': True, 'eval': True, 'sgn_eval': True, 'save': True},\n",
    "    {'epochs': 10, 'paired': False, 'ae': True, 'ee': True, 'cross': False, 'triplet': True, 'train_emb_adv': True, 'train_discrim_adv': True, 'emb_adv': True, 'discrim_adv': True, 'eval': True, 'sgn_eval': True, 'save': True},\n",
    "    # Paired Training (Crossing)\n",
    "    {'epochs': 100, 'paired': True, 'ae': True, 'ee': True, 'cross': True, 'triplet': True, 'train_emb_adv': True, 'train_discrim_adv': True, 'emb_adv': True, 'discrim_adv': True, 'eval': True, 'sgn_eval': False, 'save': True},\n",
    "]\n",
    "sgn_stage = {'epochs': 1, 'paired': True, 'ae': False, 'ee': False, 'cross': False, 'triplet': False, 'train_emb_adv': False, 'train_discrim_adv': False, 'emb_adv': True, 'discrim_adv': True, 'eval': True, 'sgn_eval': True, 'save': False}\n",
    "total_epochs = sum([stage['epochs'] for stage in training_stages])\n",
    "cur_tot_epoch = 0\n",
    "if sgn_eval_after_each_stage: total_epochs += len(training_stages)\n",
    "\n",
    "# Load AE pretrained model\n",
    "# uncomment training stages to use full training\n",
    "# pre_trained = 'pretrained/20240510-110342/stage_4.pt'\n",
    "# pre_trained = 'pretrained/20240405-130711/stage_5.pt'\n",
    "# model.load_state_dict(torch.load(pre_trained))\n",
    "\n",
    "# mlflow logging\n",
    "try: mlflow.end_run()\n",
    "except: pass\n",
    "mlflow.start_run()\n",
    "mlflow.log_param('total_epochs', total_epochs)\n",
    "mlflow.log_param('batch_size', batch_size)\n",
    "mlflow.log_param('learning_rate', lr)\n",
    "mlflow.log_param('one_dimension_conv', one_dimension_conv)\n",
    "mlflow.log_param('ntu120', ntu_120)\n",
    "mlflow.log_param('train_equal_test', str(not seperate_train_test))\n",
    "mlflow.log_param('only_use_pos', str(only_use_pos))\n",
    "mlflow.log_param('encoded_channels', str(encoded_channels))\n",
    "mlflow.log_param('cross_samples_train', cross_samples_train)\n",
    "mlflow.log_param('cross_samples_test', cross_samples_test)\n",
    "mlflow.log_param('T', T)\n",
    "mlflow.log_params(model.get_loss_params())\n",
    "\n",
    "# os.mkdir('training_stages_log')\n",
    "training_stage_name = f'training_stages_log/stages{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json'\n",
    "with open(training_stage_name, 'w') as f:\n",
    "    json.dump(training_stages, f)\n",
    "mlflow.log_artifact(training_stage_name)\n",
    "\n",
    "stages_save_path = f'pretrained/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "os.mkdir(stages_save_path)\n",
    "\n",
    "for i, stage in enumerate(training_stages):\n",
    "    print('\\nMoving to new stage')\n",
    "    print(stage, '\\n')\n",
    "    if stage['save']: assert stage['eval'], 'Cannot save model without evaluating'\n",
    "    for epoch in range(stage['epochs']):\n",
    "        if stage['sgn_eval']:\n",
    "            if validation_acc_freq > 0 and epoch % validation_acc_freq == 0: use_sgn = True\n",
    "            else: use_sgn = False\n",
    "        else: use_sgn = False\n",
    "        if not stage['paired']:\n",
    "            log_dict = train_unpaired(run_eval=stage['eval'], run_sgn_eval= use_sgn, save=stage['save'], ae=stage['ae'], ee=stage['ee'], triplet=stage['triplet'], use_emb_adv=stage['emb_adv'], use_discrim_adv=stage['discrim_adv'], emb_adv=stage['train_emb_adv'], discrim_adv=stage['train_discrim_adv'], k=k)\n",
    "        else: \n",
    "            log_dict = train_paired(train_ae=stage['ae'], train_cross=stage['cross'], train_discrim=stage['train_discrim_adv'], train_emb_adv=stage['train_emb_adv'], run_eval=stage['eval'], use_emb_adv=stage['emb_adv'], use_discrim_adv=stage['discrim_adv'], run_sgn_eval= use_sgn, save=stage['save'], k=k)\n",
    "        \n",
    "        for key, value in log_dict.items():\n",
    "            mlflow.log_metric(key, value, step=cur_tot_epoch-1)\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), f'{stages_save_path}/stage_{i}.pt')\n",
    "\n",
    "    if sgn_eval_after_each_stage:\n",
    "        print('\\nEvaluating Stage\\n')\n",
    "        stage = sgn_stage\n",
    "        log_dict = train_paired(train_ae=stage['ae'], train_cross=stage['cross'], train_discrim=stage['train_discrim_adv'], train_emb_adv=stage['train_emb_adv'], run_eval=stage['eval'], use_emb_adv=stage['emb_adv'], use_discrim_adv=stage['discrim_adv'], run_sgn_eval= use_sgn, save=stage['save'], k=k)\n",
    "        cur_tot_epoch += 1\n",
    "        for key, value in log_dict.items():\n",
    "            mlflow.log_metric(key, value, step=cur_tot_epoch-1)\n",
    "\n",
    "mlflow.pytorch.log_state_dict(model.state_dict(), 'final_model')\n",
    "mlflow.end_run()\n",
    "\n",
    "torch.save(model.state_dict(), 'ETRI.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'NTU60_1.3_priv.5_util10_5.18.24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run sgn evals for the stages\n",
    "# run = 'pretrained/20240226-132853/stage_'\n",
    "# model.set_eval()\n",
    "# for i in range(6):\n",
    "#     model.load_state_dict(torch.load(run + str(i) + '.pt'))\n",
    "#     stage = sgn_stage\n",
    "#     log_dict = train_paired(train_ae=stage['ae'], train_cross=stage['cross'], train_discrim=stage['train_discrim_adv'], train_emb_adv=stage['train_emb_adv'], run_eval=stage['eval'], use_emb_adv=stage['emb_adv'], use_discrim_adv=stage['discrim_adv'], run_sgn_eval= use_sgn, save=stage['save'], k=k)\n",
    "#     print(log_dict)\n",
    "# model.set_eval(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_video(val_data[0][0][2][:, :, :3], gif='test', show_render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sk = val_data[0][0][2][:, :, :3].unsqueeze(0).to(device)\n",
    "# render_video(model(sk, sk).cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retarget Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = val_data[0][0][2][:, :, :3].unsqueeze(0).to(device)\n",
    "# x2 = val_data[0][1][2][:, :, :3].unsqueeze(0).to(device)\n",
    "# y1 = val_data[0][2][2][:, :, :3].unsqueeze(0).to(device)\n",
    "# y2 = val_data[0][3][2][:, :, :3].unsqueeze(0).to(device)\n",
    "# print(f'Actors: {val_data[0][0][0]}, {val_data[0][2][0]}')\n",
    "# print(f'Actions: {val_data[0][0][1]}, {val_data[0][1][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_video(model(x1, x2).cpu().detach().numpy()[0])\n",
    "# render_video(model(x2, x1).cpu().detach().numpy()[0])\n",
    "# render_video(model(y1, y2).cpu().detach().numpy()[0])\n",
    "# render_video(model(y2, y1).cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retargeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmr = DMR().to(device)\n",
    "dmr.load_state_dict(torch.load('pretrained/DMR.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model = AutoEncoder(use_adv=False).to(device)\n",
    "weights = torch.load('pretrained/MR.pt')\n",
    "keys_to_rem = [\"priv_adv.conv1.weight\", \"priv_adv.conv1.bias\", \"priv_adv.conv2.weight\", \"priv_adv.conv2.bias\", \"priv_adv.conv3.weight\", \"priv_adv.conv3.bias\", \"priv_adv.bn1.weight\", \"priv_adv.bn1.bias\", \"priv_adv.bn1.running_mean\", \"priv_adv.bn1.running_var\", \"priv_adv.bn1.num_batches_tracked\", \"priv_adv.bn2.weight\", \"priv_adv.bn2.bias\", \"priv_adv.bn2.running_mean\", \"priv_adv.bn2.running_var\", \"priv_adv.bn2.num_batches_tracked\", \"priv_adv.bn3.weight\", \"priv_adv.bn3.bias\", \"priv_adv.bn3.running_mean\", \"priv_adv.bn3.running_var\", \"priv_adv.bn3.num_batches_tracked\", \"priv_adv.fc1.weight\", \"priv_adv.fc1.bias\", \"priv_adv.fc2.weight\", \"priv_adv.fc2.bias\", \"priv_adv.fc3.weight\", \"priv_adv.fc3.bias\", \"priv_coop.conv1.weight\", \"priv_coop.conv1.bias\", \"priv_coop.conv2.weight\", \"priv_coop.conv2.bias\", \"priv_coop.conv3.weight\", \"priv_coop.conv3.bias\", \"priv_coop.bn1.weight\", \"priv_coop.bn1.bias\", \"priv_coop.bn1.running_mean\", \"priv_coop.bn1.running_var\", \"priv_coop.bn1.num_batches_tracked\", \"priv_coop.bn2.weight\", \"priv_coop.bn2.bias\", \"priv_coop.bn2.running_mean\", \"priv_coop.bn2.running_var\", \"priv_coop.bn2.num_batches_tracked\", \"priv_coop.bn3.weight\", \"priv_coop.bn3.bias\", \"priv_coop.bn3.running_mean\", \"priv_coop.bn3.running_var\", \"priv_coop.bn3.num_batches_tracked\", \"priv_coop.fc1.weight\", \"priv_coop.fc1.bias\", \"priv_coop.fc2.weight\", \"priv_coop.fc2.bias\", \"priv_coop.fc3.weight\", \"priv_coop.fc3.bias\", \"util_adv.conv1.weight\", \"util_adv.conv1.bias\", \"util_adv.conv2.weight\", \"util_adv.conv2.bias\", \"util_adv.conv3.weight\", \"util_adv.conv3.bias\", \"util_adv.bn1.weight\", \"util_adv.bn1.bias\", \"util_adv.bn1.running_mean\", \"util_adv.bn1.running_var\", \"util_adv.bn1.num_batches_tracked\", \"util_adv.bn2.weight\", \"util_adv.bn2.bias\", \"util_adv.bn2.running_mean\", \"util_adv.bn2.running_var\", \"util_adv.bn2.num_batches_tracked\", \"util_adv.bn3.weight\", \"util_adv.bn3.bias\", \"util_adv.bn3.running_mean\", \"util_adv.bn3.running_var\", \"util_adv.bn3.num_batches_tracked\", \"util_adv.fc1.weight\", \"util_adv.fc1.bias\", \"util_adv.fc2.weight\", \"util_adv.fc2.bias\", \"util_adv.fc3.weight\", \"util_adv.fc3.bias\", \"util_coop.conv1.weight\", \"util_coop.conv1.bias\", \"util_coop.conv2.weight\", \"util_coop.conv2.bias\", \"util_coop.conv3.weight\", \"util_coop.conv3.bias\", \"util_coop.bn1.weight\", \"util_coop.bn1.bias\", \"util_coop.bn1.running_mean\", \"util_coop.bn1.running_var\", \"util_coop.bn1.num_batches_tracked\", \"util_coop.bn2.weight\", \"util_coop.bn2.bias\", \"util_coop.bn2.running_mean\", \"util_coop.bn2.running_var\", \"util_coop.bn2.num_batches_tracked\", \"util_coop.bn3.weight\", \"util_coop.bn3.bias\", \"util_coop.bn3.running_mean\", \"util_coop.bn3.running_var\", \"util_coop.bn3.num_batches_tracked\", \"util_coop.fc1.weight\", \"util_coop.fc1.bias\", \"util_coop.fc2.weight\", \"util_coop.fc2.bias\", \"util_coop.fc3.weight\", \"util_coop.fc3.bias\", \"discriminator.enc1.weight\", \"discriminator.enc1.bias\", \"discriminator.enc2.weight\", \"discriminator.enc2.bias\", \"discriminator.enc3.weight\", \"discriminator.enc3.bias\", \"discriminator.enc4.weight\", \"discriminator.enc4.bias\", \"discriminator.fc1.weight\", \"discriminator.fc1.bias\", \"discriminator.fc2.weight\", \"discriminator.fc2.bias\"]\n",
    "for key in keys_to_rem:\n",
    "    del weights[key]\n",
    "val_model.load_state_dict(weights)\n",
    "model = val_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retarget_random_action():\n",
    "    X_hat_random = {}\n",
    "    X_hat_constant = {}\n",
    "\n",
    "    # const = random.sample(list(X.keys()), 1)[0]\n",
    "    const = 'S007C001P025R001A045'.encode('utf-8')\n",
    "    x2_const = X[const].float().cuda().unsqueeze(0)\n",
    "    print(const)\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for file in tqdm(X):\n",
    "            x1 = X[file].unsqueeze(0)\n",
    "            while True:\n",
    "                sample = random.sample(list(X.keys()), 1)[0]\n",
    "                # ensure different actor\n",
    "                if sample.decode('utf-8')[9:12] != file.decode('utf-8')[9:12]:\n",
    "                    break\n",
    "            x2_random = X[sample].unsqueeze(0)\n",
    "            start = time.time()\n",
    "            X_hat_random[file] = val_model.eval(x1.float().cuda(), x2_random.float().cuda()).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            start = time.time()\n",
    "            X_hat_constant[file] = val_model.eval(x1.float().cuda(), x2_const).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            # render_video(X_hat_random[file])\n",
    "            # render_video(X_hat_constant[file])\n",
    "\n",
    "    print(f'Average time: {np.mean(times)}')\n",
    "    \n",
    "    # Save results\n",
    "    with open('results/DMR_X_hat_random_RA.pkl', 'wb') as f:\n",
    "        pickle.dump(X_hat_random, f)\n",
    "    with open(f'results/DMR_X_hat_constant_RA.pkl', 'wb') as f:\n",
    "        pickle.dump(X_hat_constant, f)\n",
    "\n",
    "def retarget_constant_action():\n",
    "    X_hat_random = {}\n",
    "    X_hat_constant = {}\n",
    "\n",
    "    const = 8\n",
    "    # x2_const = X[const].float().cuda().unsqueeze(0)\n",
    "    times = []\n",
    "\n",
    "    const_dict = {}\n",
    "    # Get a sample of each action from the constant actor\n",
    "    for file in X:\n",
    "        info = parse_file_name(file)\n",
    "        if info['P'] == const and info['A'] not in const_dict:\n",
    "            const_dict[info['A']] = X[file].float().cuda().unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for file in tqdm(X):\n",
    "            x1 = X[file].unsqueeze(0)\n",
    "            info = parse_file_name(file)\n",
    "            while True:\n",
    "                sample = random.sample(list(X.keys()), 1)[0]\n",
    "                # ensure different actor and same action\n",
    "                info_ = parse_file_name(sample)\n",
    "                if info_['P'] != info['P'] and info_['A'] == info['A']:\n",
    "                    break\n",
    "            x2_random = X[sample].unsqueeze(0)\n",
    "            start = time.time()\n",
    "            X_hat_random[file] = val_model.eval(x1.float().cuda(), x2_random.float().cuda()).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            \n",
    "            start = time.time()\n",
    "            X_hat_constant[file] = val_model.eval(x1.float().cuda(), const_dict[info['A']]).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            \n",
    "            # render_video(X_hat_random[file])\n",
    "            # render_video(X_hat_constant[file])\n",
    "\n",
    "    print(f'Average time: {np.mean(times)}')\n",
    "    \n",
    "    # Save results\n",
    "    with open('results/DMR_X_hat_random_CA.pkl', 'wb') as f:\n",
    "        pickle.dump(X_hat_random, f)\n",
    "    with open(f'results/DMR_X_hat_constant_CA.pkl', 'wb') as f:\n",
    "        pickle.dump(X_hat_constant, f)\n",
    "\n",
    "def retarget_specific(dummy, reference = None, just_render = False, use_dmr = False):\n",
    "    X_hat = {}\n",
    "    X_hat_dmr = {}\n",
    "\n",
    "    x2_const = X[dummy].float().cuda().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        if reference is None:\n",
    "            for file in X:\n",
    "                x1 = X[file].unsqueeze(0)\n",
    "                X_hat[file] = val_model.eval(x1.float().cuda(), x2_const).cpu().numpy().squeeze()\n",
    "                if use_dmr: X_hat_dmr[file] = dmr.eval(x1.float().cuda(), x2_const).cpu().numpy().squeeze()\n",
    "                if just_render: \n",
    "                    render_video(X_hat[file])\n",
    "                    if use_dmr: render_video(X_hat_dmr[file])\n",
    "                break\n",
    "        else:\n",
    "            X_hat = model.eval(X[reference].unsqueeze(0).float().cuda(), X[dummy].unsqueeze(0).float().cuda()).cpu().numpy().squeeze()\n",
    "            if use_dmr: X_hat_dmr = dmr.eval(X[reference].unsqueeze(0).float().cuda(), X[dummy].unsqueeze(0).float().cuda()).cpu().numpy().squeeze()\n",
    "            if just_render: \n",
    "                render_video(X_hat)\n",
    "                if use_dmr: render_video(X_hat_dmr)\n",
    "            else:\n",
    "                render_video(X_hat, gif=f'pmr_{dummy}')\n",
    "                if use_dmr: render_video(X_hat_dmr, gif=f'dmr_{dummy}')\n",
    "\n",
    "    # Save results\n",
    "    # with open(f'results/X_hat_{dummy}.pkl', 'wb') as f:\n",
    "    #     pickle.dump(X_hat, f)\n",
    "\n",
    "    # if use_dmr:\n",
    "    #     with open(f'results/X_hat_dmr_{dummy}.pkl', 'wb') as f:\n",
    "    #         pickle.dump(X_hat_dmr, f)\n",
    "\n",
    "    return X_hat\n",
    "\n",
    "# retarget_random_action()\n",
    "# retarget_constant_action()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Retargeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds samples and genders of specific actions\n",
    "genders = pd.read_csv('NTU\\SGN\\statistics\\Genders.csv')\n",
    "use_dmr = True\n",
    "\n",
    "action_to_use = 1\n",
    "for i in range(500):\n",
    "    dummy = random.sample(list(X.keys()), 1)[0]\n",
    "    actor = int(dummy[9:12])\n",
    "    action = int(dummy[17:20])\n",
    "    if action != action_to_use: continue\n",
    "    gender = genders.loc[actor-1, 'Gender']\n",
    "    print(dummy, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_visuals(action1, action2):\n",
    "    files = []\n",
    "    split_genders = True\n",
    "    first_gender = None\n",
    "    attempts = 0\n",
    "    while True:\n",
    "        attempts += 1\n",
    "        dummy = random.sample(list(X.keys()), 1)[0]\n",
    "        actor = int(dummy[9:12])\n",
    "        action = int(dummy[17:20])\n",
    "        gender = genders.loc[actor-1, 'Gender']\n",
    "        if action == action1: \n",
    "            if split_genders:\n",
    "                if gender == first_gender: continue\n",
    "                first_gender = gender\n",
    "            files.append(dummy)\n",
    "            action1 = -1\n",
    "        if action == action2:\n",
    "            if split_genders:\n",
    "                if gender == first_gender: continue\n",
    "                first_gender = gender\n",
    "            files.append(dummy)\n",
    "            action2 = -1\n",
    "        if action1 == -1 and action2 == -1: break\n",
    "    print(files)\n",
    "    retarget_specific(files[0], reference=files[1], just_render=False, use_dmr=use_dmr)\n",
    "    retarget_specific(files[1], reference=files[0], just_render=False, use_dmr=use_dmr)\n",
    "    render_video(X[files[0]], gif=files[0].decode('utf-8'))\n",
    "    render_video(X[files[1]], gif=files[1].decode('utf-8'))\n",
    "\n",
    "for i in range(1, 60, 2):\n",
    "    print(f'Action {i}')\n",
    "    make_visuals(i, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading\n",
    "# sitting S: 007, 015, 011\n",
    "# standing S: 016, 003, 006, 008\n",
    "ref = 'S006C003P017R001A011'.encode('utf-8') # female\n",
    "dummy = 'S015C003P019R001A011'.encode('utf-8') # male\n",
    "retarget_specific(dummy, reference=ref, just_render=False, use_dmr=use_dmr)\n",
    "render_video(X[ref])#, gif=ref)\n",
    "render_video(X[dummy])#, gif=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drink water\n",
    "ref = 'S001C001P001R001A001'.encode('utf-8') # female\n",
    "dummy = 'S010C002P021R001A001'.encode('utf-8') # male \n",
    "retarget_specific(dummy, reference=ref, just_render=False, use_dmr=use_dmr)\n",
    "render_video(X[ref], gif=ref)\n",
    "render_video(X[dummy], gif=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross hands\n",
    "ref = 'S003C001P002R001A040'.encode('utf-8') # female\n",
    "dummy = 'S009C003P008R002A040'.encode('utf-8') # male \n",
    "retarget_specific(dummy, reference=ref, just_render=True, use_dmr=use_dmr)\n",
    "render_video(X[ref])#, gif=ref)\n",
    "render_video(X[dummy])#, gif=dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate utility of other baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_24632\\3495056763.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sgn_gender.load_state_dict(torch.load('SGN/pretrained/gender.pt')['state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load SGN gender classification\n",
    "sgn_gender = SGN(2, None, seg, batch_size, 0).to(device)\n",
    "sgn_gender.load_state_dict(torch.load('SGN/pretrained/gender.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgn_train_x, sgn_train_y, sgn_val_x, sgn_val_y = np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1)), np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1))\n",
    "\n",
    "eval_renders_str_skeleton = ['S006C003P017R001A011.skeleton',\n",
    "                             'S015C003P019R001A011.skeleton']\n",
    "eval_renders_str = [x[:-9] for x in eval_renders_str_skeleton]\n",
    "eval_render_byte = [x.encode('utf-8') for x in eval_renders_str]\n",
    "genders = pd.read_csv('NTU\\SGN\\statistics\\Genders.csv').replace('M', 1).replace('F', 0)\n",
    "\n",
    "def anonymizer_to_sgn(t, max_frames=300):\n",
    "    xyz, frames, joints, actors = t.shape\n",
    "    \n",
    "    # Pre-allocate memory for the output array\n",
    "    X = np.zeros((max_frames, xyz * joints * actors), dtype=np.float32)\n",
    "    \n",
    "    # Reshape the input array for easier manipulation\n",
    "    t_reshaped = t.reshape((frames, -1))\n",
    "    \n",
    "    # Copy over the reshaped data to the pre-allocated output\n",
    "    X[:frames, :t_reshaped.shape[1]] = t_reshaped\n",
    "    \n",
    "    return X\n",
    "\n",
    "def eval(X_dict, gif_name=None, cameras=None, just_render=False):\n",
    "    # Remove NTU120 if needed\n",
    "    if not ntu_120:\n",
    "        X_dict = {k: v for k, v in X_dict.items() if int(k[17:20]) <= 60}\n",
    "\n",
    "    if only_ntu_120:\n",
    "        X_dict = {k: v for k, v in X_dict.items() if int(k[17:20]) > 60}\n",
    "\n",
    "    # Remove cameras if needed\n",
    "    if cameras is not None:\n",
    "        X_dict = {k: v for k, v in X_dict.items() if int(k[7]) in cameras}\n",
    "\n",
    "    print(f'Number of files: {len(X_dict)}')\n",
    "\n",
    "    if not just_render:\n",
    "        # Calculate MSE\n",
    "        anon = torch.zeros((len(X_dict), 300, 75))\n",
    "        raw = torch.zeros((len(X_dict), 75, 25, 3))\n",
    "        rem=0\n",
    "        for i, file in enumerate(X_dict):\n",
    "            if only_use_pos:\n",
    "                if type(file) != np.bytes_: file_byte = file.split('.')[0].encode('utf-8')\n",
    "                else: file_byte = file\n",
    "            else: file_byte = file\n",
    "            \n",
    "            # Ensure file exists in both dicts\n",
    "            if file_byte not in X or file not in X_dict:\n",
    "                rem+=1\n",
    "                continue\n",
    "            anon[i] = torch.tensor(X_dict[file])\n",
    "            if only_use_pos:\n",
    "                raw[i] = X[file_byte]\n",
    "            else:\n",
    "                raw[i] = X[file_byte][:, :, :3]\n",
    "\n",
    "        # Remove non existent files\n",
    "        anon = anon[:len(X_dict)-rem]\n",
    "        raw = raw[:len(X_dict)-rem]\n",
    "\n",
    "        # Remove zeros from the end of the sequence\n",
    "        for i in range(anon.shape[1]):\n",
    "            if not torch.all(anon[:, i] == 0):\n",
    "                anon = anon[:, :i+1]\n",
    "                raw = raw[:, :i+1]\n",
    "                break\n",
    "\n",
    "        # Reshape anon to be 75, 25, 3\n",
    "        if anon.shape[1] > 75: anon = anon[:, :75, :]\n",
    "        anon = anon[:, :, :75]\n",
    "        anon = anon.reshape((anon.shape[0], anon.shape[1], 25, 3))\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = torch.mean((anon - raw)**2, dim=3)\n",
    "        l2 = torch.mean(torch.sqrt(torch.sum((anon-raw)**2, dim=3)))\n",
    "        print(f'MSE:\\t\\t\\t\\t{torch.mean(mse)}\\nL2:\\t\\t\\t\\t{l2}\\n')\n",
    "\n",
    "        # Pre-allocate memory for the output array\n",
    "        x = np.zeros((len(X_dict), 300, 150), dtype=np.float32)\n",
    "        y_util = np.zeros(len(X_dict))\n",
    "        y_priv = np.zeros(len(X_dict))\n",
    "        y_gender = np.zeros(len(X_dict))\n",
    "\n",
    "        for i, file in enumerate(X_dict):\n",
    "            if X_dict[file].shape[1] == 75:\n",
    "                X_dict[file] = np.pad(X_dict[file], ((0, 0), (0, 75)), 'constant')\n",
    "\n",
    "            x[i] = np.array(X_dict[file], dtype=np.float32)\n",
    "            y_util[i] = int(file[17:20])\n",
    "            y_priv[i] = int(file[9:12])\n",
    "            y_gender[i] = genders.loc[y_priv[i]-1, 'Gender']\n",
    "\n",
    "        y_util = y_util - 1\n",
    "        y_priv = y_priv - 1\n",
    "        y_util = np.eye(utility_classes)[y_util.astype(int)]\n",
    "        y_priv = np.eye(privacy_classes)[y_priv.astype(int)]\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        acc, f1, prec, recall, topk = run_sgn_eval(sgn_train_x, sgn_train_y, x, y_util, sgn_val_x, sgn_val_y, 1, sgn_ar, k=k)\n",
    "        print(f'Utility Accuracy:\\t\\t{acc}\\nUtility F1:\\t\\t\\t{f1*100}\\nUtility Precision:\\t\\t{prec*100}\\nUtility Recall:\\t\\t\\t{recall*100}\\nTop-{k} Accuracy:\\t\\t\\t{topk}\\n')\n",
    "\n",
    "        acc, f1, prec, recall, topk = run_sgn_eval(sgn_train_x, sgn_train_y, x, y_priv, sgn_val_x, sgn_val_y, 1, sgn_priv, k=k)\n",
    "        print(f'Privacy Accuracy:\\t\\t{acc}\\nPrivacy F1:\\t\\t\\t{f1*100}\\nPrivacy Precision:\\t\\t{prec*100}\\nPrivacy Recall:\\t\\t\\t{recall*100}\\nTop-{k} Accuracy:\\t\\t\\t{topk}\\n')\n",
    "\n",
    "        # Gender classification\n",
    "        acc, f1 = run_sgn_gender_eval(sgn_train_x, sgn_train_y, x, y_priv, sgn_val_x, sgn_val_y, sgn_priv)\n",
    "        print(f'Gender Classificiation Accuracy:\\t{acc}\\nF1:\\t\\t\\t\\t{f1*100}\\n')\n",
    "    \n",
    "    if gif_name is None: return\n",
    "    print(gif_name, ' preparing to render')\n",
    "    for key in X_dict:\n",
    "        print(key)\n",
    "        break\n",
    "    print(eval_renders_str_skeleton)\n",
    "    print(eval_renders_str)\n",
    "    print(eval_render_byte)\n",
    "    \n",
    "    # Trim the data to only the first 75 frames, remove second actor, and remove zeroed out entries\n",
    "    for file in X_dict:\n",
    "        if X_dict[file].shape[0] != 75:\n",
    "            if X_dict[file].shape[1] != 75: X_dict[file] = X_dict[file][:75, :75]\n",
    "            else: X_dict[file] = X_dict[file][:75, :]\n",
    "        elif X_dict[file].shape[1] != 75: X_dict[file] = X_dict[file][:, :75]\n",
    "\n",
    "        for i in range(75):\n",
    "            if np.all(X_dict[file][i] == 0):\n",
    "                X_dict[file] = X_dict[file][:i]\n",
    "                break\n",
    "    \n",
    "    # Render the videos\n",
    "    for file in eval_renders_str_skeleton: \n",
    "        if file in X_dict:\n",
    "            print('rendering')\n",
    "            render_video(X_dict[file], gif=f'{gif_name}_{file[16:20]}, {file[8:12]}', show_render=False)\n",
    "            print(file)\n",
    "    for file in eval_renders_str: \n",
    "        if file in X_dict:\n",
    "            print('rendering')\n",
    "            render_video(X_dict[file], gif=f'{gif_name}_{file[16:20]}, {file[8:12]}', show_render=False)\n",
    "            print(file)\n",
    "    for file in eval_render_byte:\n",
    "        if file in X_dict:\n",
    "            print('rendering')\n",
    "            a = file[16:20].decode('utf-8')\n",
    "            p = file[8:12].decode('utf-8')\n",
    "            render_video(X_dict[file], gif=f'{gif_name}_{a}, {p}', show_render=False)\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x_pkl, from_moon=False, pad_data=True, gif_name='', cameras=None, just_render=False):\n",
    "    with open(x_pkl, 'rb') as f:\n",
    "        test_x = pickle.load(f)\n",
    "\n",
    "    if from_moon:\n",
    "        test_x = {k: v[0] for k, v in test_x.items()}\n",
    "        for file in test_x:\n",
    "            # Assuming anonymizer_to_sgn is a predefined function you have\n",
    "            test_x[file] = anonymizer_to_sgn(test_x[file])[:, :75]\n",
    "\n",
    "    if pad_data:\n",
    "        for file in test_x:\n",
    "            if test_x[file].shape[0] == 1:\n",
    "                test_x[file] = test_x[file][0]\n",
    "            test_x[file] = np.pad(test_x[file], ((0, 300-test_x[file].shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "    # If keys are bytes, convert to string\n",
    "    if type(list(test_x.keys())[0]) == np.bytes_: test_x = {k.decode('utf-8'): v for k, v in test_x.items()}\n",
    "\n",
    "    eval(test_x, gif_name=gif_name, cameras=cameras, just_render=just_render)\n",
    "\n",
    "datasets = {\n",
    "    'pmr_random_RA': ('results/X_hat_random_RA.pkl', False, True),\n",
    "    # 'pmr_constant_RA': ('results/X_hat_constant_RA.pkl', False, True),\n",
    "    # 'pmr_random_CA': ('results/X_hat_random_CA.pkl', False, True),\n",
    "    # 'pmr_constant_CA': ('results/X_hat_constant_CA.pkl', False, True),\n",
    "    # 'dmr_random_RA': ('results/DMR_X_hat_random_RA.pkl', False, True),\n",
    "    # 'dmr_constant_RA': ('results/DMR_X_hat_constant_RA.pkl', False, True),\n",
    "    # 'dmr_random_CA': ('results/DMR_X_hat_random_CA.pkl', False, True),\n",
    "    # 'dmr_constant_CA': ('results/DMR_X_hat_constant_CA.pkl', False, True),\n",
    "    # 'moon_unet': ('C:\\\\Users\\\\Carrt\\\\OneDrive\\\\Code\\\\Linkage Attack\\\\External Repositories\\\\Skeleton-anonymization\\\\X_unet_file.pkl', True, False),\n",
    "    # 'moon_resnet': ('C:\\\\Users\\\\Carrt\\\\OneDrive\\\\Code\\\\Linkage Attack\\\\External Repositories\\\\Skeleton-anonymization\\\\X_resnet_file.pkl', True, False),\n",
    "    # 'cmr': ('C:\\\\Users\\\\Carrt\\\\OneDrive\\\\Code\\\\Motion Privacy\\\\Defense Models\\\\Mean Skeleton\\\\X_FileNameKey_SingleActor_filtered.pkl', False, False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all datasets\n",
    "for gif_name, (x_pkl, from_moon, pad_data) in datasets.items():\n",
    "    print(f'Processing {gif_name}')\n",
    "    process_data(x_pkl, from_moon=from_moon, pad_data=pad_data, gif_name=gif_name, just_render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process specific dataset\n",
    "ds = 'pmr_random_CA'\n",
    "process_data(datasets[ds][0], from_moon=datasets[ds][1], pad_data=datasets[ds][2], gif_name=ds, cameras=test_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw\n",
    "if only_use_pos:\n",
    "    with open('ntu/SGN/X_full.pkl', 'rb') as f:\n",
    "        raw = pickle.load(f)\n",
    "else:\n",
    "    with open('ntu/X.pkl', 'rb') as f:\n",
    "        raw = pickle.load(f)\n",
    "\n",
    "for file in raw:\n",
    "    if only_use_pos:\n",
    "        # chop from (300, 150) to (300, 75)\n",
    "        raw[file] = raw[file][:, :75]\n",
    "    else:\n",
    "        # reshape from (frame, 25, 3) to (frame, 75)\n",
    "        raw[file] = raw[file][:, :, :3].reshape((raw[file].shape[0], 75))\n",
    "        # pad data to 300 frames\n",
    "        if raw[file].shape[0] == 300: continue\n",
    "        raw[file] = np.pad(raw[file], ((0, 300-raw[file].shape[0]), (0, 0)), 'constant')\n",
    "        if raw[file].shape != (300, 75): \n",
    "            print(file, raw[file].shape)\n",
    "            del raw[file]\n",
    "\n",
    "eval(raw, gif_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in eval_render_byte:\n",
    "#     a = file[16:20].decode('utf-8')\n",
    "#     p = file[8:12].decode('utf-8')\n",
    "#     render_video(X[file], gif=f'raw_{a}, {p}', show_render=False)\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "model = val_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action clustering\n",
    "d = []\n",
    "y = []\n",
    "for key in tqdm(X.keys()):\n",
    "        embedding = model.dynamic_encoder(X[key].unsqueeze(0).float().cuda()).cpu().detach().numpy().flatten()\n",
    "        d.append(embedding)\n",
    "        y.append(int(key[17:20]))\n",
    "\n",
    "d = np.array(d)\n",
    "y = np.array(y)\n",
    "\n",
    "# Perform TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(d)\n",
    "\n",
    "# Create a DataFrame for Plotly\n",
    "df = pd.DataFrame(tsne_results, columns=['TSNE-1', 'TSNE-2'])\n",
    "df['label'] = y.astype(str)  # Convert labels to string for better categorical handling\n",
    "\n",
    "# Create an interactive scatter plot\n",
    "fig = px.scatter(df, x='TSNE-1', y='TSNE-2', color='label',\n",
    "                 color_discrete_sequence=px.colors.qualitative.G10,\n",
    "                 labels={'label': 'Label'},\n",
    "                 title='t-SNE results with Interactive Labels')\n",
    "\n",
    "# Add interactive functionality for toggling visibility\n",
    "label_buttons = [dict(label='All',\n",
    "                      method='update',\n",
    "                      args=[{'visible': [True] * len(df['label'].unique())},\n",
    "                            {'title': 't-SNE results with Interactive Labels'}])]\n",
    "\n",
    "for label in df['label'].unique():\n",
    "    label_buttons.append(dict(label=f'Label {label}',\n",
    "                              method='update',\n",
    "                              args=[{'visible': [lbl == label for lbl in df['label']]},\n",
    "                                    {'title': f't-SNE: Label {label}'}]))\n",
    "\n",
    "fig.update_layout(showlegend=False,\n",
    "                  updatemenus=[dict(active=0,\n",
    "                                    buttons=label_buttons,\n",
    "                                    x=0.0,\n",
    "                                    xanchor='left',\n",
    "                                    y=1.2,\n",
    "                                    yanchor='top')])\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = set([6, 9, 22, 25, 29, 38, 39])\n",
    "\n",
    "# Actor clustering\n",
    "d = []\n",
    "y = []\n",
    "for key in tqdm(X.keys()):\n",
    "    if int(key[17:20]) in actions:\n",
    "        embedding = model.dynamic_encoder(X[key].unsqueeze(0).float().cuda()).cpu().detach().numpy().flatten()\n",
    "        d.append(embedding)\n",
    "        y.append(int(key[17:20]))\n",
    "\n",
    "d = np.array(d)\n",
    "y = np.array(y)\n",
    "\n",
    "# Map labels\n",
    "label_mapping = {\n",
    "    '6': 'pick up',\n",
    "    '7': 'throw',\n",
    "    '8': 'sit down',\n",
    "    '9': 'stand up',\n",
    "    '16': 'put on a shoe',\n",
    "    '17': 'take off shoe',\n",
    "    '22': 'put on glasses',\n",
    "    '23': 'hand waving',\n",
    "    '25': 'reach into pocket',\n",
    "    '29': 'play with phone/tablet',\n",
    "    '38': 'salute',\n",
    "    '39': 'put palms together',\n",
    "    '40': 'cross hands in front',\n",
    "    '42': 'staggering',\n",
    "    '43': 'falling down'\n",
    "}\n",
    "y_mapped = [label_mapping[str(label)] for label in y]\n",
    "\n",
    "# Perform TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=100, n_iter=2000)\n",
    "tsne_results = tsne.fit_transform(d)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "    hue=y_mapped,\n",
    "    palette=sns.color_palette(\"hls\", len(actions)),\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Clustering\n",
    "d_act = []\n",
    "y_act = []\n",
    "for key in tqdm(X.keys()):\n",
    "#     if int(key[9:12]) in actors:\n",
    "        embedding = model.static_encoder(X[key].unsqueeze(0).float().cuda()).cpu().detach().numpy().flatten()\n",
    "        d_act.append(embedding)\n",
    "        y_act.append(int(key[9:12]))\n",
    "\n",
    "d_act = np.array(d_act)\n",
    "y_act = np.array(y_act)\n",
    "\n",
    "# Perform TSNE\n",
    "tsne_act = TSNE(n_components=2, verbose=1, perplexity=100, n_iter=2000, learning_rate='auto')\n",
    "tsne_results_act = tsne_act.fit_transform(d_act)\n",
    "\n",
    "df = pd.DataFrame(tsne_results_act, columns=['TSNE-1', 'TSNE-2'])\n",
    "df['label'] = y_act.astype(str)  # Convert labels to string for better categorical handling\n",
    "\n",
    "# Create an interactive scatter plot\n",
    "fig = px.scatter(df, x='TSNE-1', y='TSNE-2', color='label',\n",
    "                 color_discrete_sequence=px.colors.qualitative.G10,\n",
    "                 labels={'label': 'Label'},\n",
    "                 title='t-SNE results with Interactive Labels')\n",
    "\n",
    "def create_visibility_list(selected_label, all_labels):\n",
    "    return [label == selected_label for label in all_labels]\n",
    "\n",
    "# Initialize visibility: Initially show all\n",
    "initial_visibility = [True] * len(df['label'].unique())\n",
    "\n",
    "# Create buttons\n",
    "label_buttons = [dict(label='All',\n",
    "                      method='update',\n",
    "                      args=[{'visible': initial_visibility},\n",
    "                            {'title': 't-SNE results with Interactive Labels'}])]\n",
    "\n",
    "for label in df['label'].unique():\n",
    "    visibility_list = create_visibility_list(label, df['label'].unique())\n",
    "    label_buttons.append(dict(label=f'Label {label}',\n",
    "                              method='update',\n",
    "                              args=[{'visible': visibility_list},\n",
    "                                    {'title': f't-SNE: Label {label}'}]))\n",
    "\n",
    "fig.update_layout(showlegend=False,\n",
    "                  updatemenus=[dict(active=0,\n",
    "                                    buttons=label_buttons,\n",
    "                                    x=0.0,\n",
    "                                    xanchor='left',\n",
    "                                    y=1.2,\n",
    "                                    yanchor='top')])\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = set([27, 28, 35, 37])\n",
    "\n",
    "# Actor clustering\n",
    "d_act = []\n",
    "y_act = []\n",
    "for key in tqdm(X.keys()):\n",
    "    if int(key[9:12]) in actors:\n",
    "        embedding = model.static_encoder(X[key].unsqueeze(0).float().cuda()).cpu().detach().numpy().flatten()\n",
    "        d_act.append(embedding)\n",
    "        y_act.append(int(key[9:12]))\n",
    "\n",
    "d_act = np.array(d_act)\n",
    "y_act = np.array(y_act)\n",
    "\n",
    "# # Perform TSNE\n",
    "tsne_act = TSNE(n_components=2, verbose=1)#, perplexity=100, n_iter=2000, learning_rate='auto')\n",
    "tsne_results_act = tsne_act.fit_transform(d_act)\n",
    "\n",
    "y_act_str = [f'Actor: {x}' for x in y_act]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=tsne_results_act[:,0], y=tsne_results_act[:,1],\n",
    "    hue=y_act_str,\n",
    "    palette=sns.color_palette(\"hls\", len(np.unique(y_act))),\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render action 1 from each actor above\n",
    "action = 1\n",
    "for actor in actors:\n",
    "    for key in X:\n",
    "        if int(key[9:12]) == actor and int(key[17:20]) == action:\n",
    "            render_video(X[key])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_skeleton(skeleton):\n",
    "    \"\"\"\n",
    "    Reshape the skeleton data from shape [1, 75, 25, 3] to [300, 150].\n",
    "    \"\"\"\n",
    "    skeleton = skeleton.squeeze(0)  # Remove the batch dimension, shape becomes [75, 25, 3]\n",
    "    skeleton = skeleton.reshape(75, -1)  # Reshape to [75, 75]\n",
    "    \n",
    "    # Pre-allocate the output with zeros to match the expected [300, 150] shape\n",
    "    padded_skeleton = torch.zeros((20, 75), dtype=torch.float32).cpu()\n",
    "    \n",
    "    # Fill in the existing data\n",
    "    padded_skeleton = skeleton[:20]\n",
    "    \n",
    "    return padded_skeleton\n",
    "\n",
    "sgn_train_x, sgn_train_y, sgn_val_x, sgn_val_y = np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1)), np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1))\n",
    "\n",
    "\n",
    "def predict_sgn(model, skeleton):\n",
    "    skeleton = torch.tensor(skeleton).cuda()\n",
    "    model.cuda()\n",
    "    out = model.eval_single(skeleton)\n",
    "    out = out.view((-1, skeleton.size(0)//skeleton.size(0), out.size(1)))\n",
    "    out = out.mean(1)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    out = np.argmax(out, axis=1)\n",
    "    return out[0]\n",
    "\n",
    "\n",
    "def sample_motion_retargeting(dataset, motion_retargeting_model, action_recognition_model, re_identification_model, target_sample_count=50, verbose=False):\n",
    "    successful_samples = {}\n",
    "    successful_sample_map = {}\n",
    "\n",
    "    # Continue sampling until we have enough successful samples\n",
    "    while len(successful_samples) < target_sample_count:\n",
    "        # Randomly select a reference skeleton\n",
    "        reference_file = random.sample(list(dataset.keys()), 1)[0]\n",
    "        reference_skeleton = dataset[reference_file].unsqueeze(0)\n",
    "        reference_action, reference_actor = parse_file_name(reference_file)['A'], parse_file_name(reference_file)['P']\n",
    "\n",
    "        # Randomly select a dummy skeleton that is not the same as the reference skeleton\n",
    "        while True:\n",
    "            dummy_file = random.sample(list(dataset.keys()), 1)[0]\n",
    "            dummy_identity = parse_file_name(dummy_file)['P']\n",
    "            if dummy_identity != reference_actor:\n",
    "                dummy_skeleton = dataset[dummy_file].unsqueeze(0)\n",
    "                break\n",
    "\n",
    "        if verbose: print(f\"Attemtping to anonymize {reference_file} using {dummy_file}\")\n",
    "\n",
    "        # Perform motion retargeting\n",
    "        anonymized_skeleton = motion_retargeting_model(reference_skeleton.cuda(), dummy_skeleton.cuda()).cpu().squeeze()\n",
    "\n",
    "        # Reshape the anonymized skeleton\n",
    "        anonymized_skeleton = reshape_skeleton(anonymized_skeleton)\n",
    "        \n",
    "        # Evaluate action recognition (utility)\n",
    "        anonymized_action = predict_sgn(action_recognition_model, anonymized_skeleton.unsqueeze(0).detach().numpy()) + 1 # +1 to match the original indexing\n",
    "\n",
    "        # Evaluate re-identification (privacy)\n",
    "        anonymized_identity = predict_sgn(re_identification_model, anonymized_skeleton.unsqueeze(0).detach().numpy()) + 1 # +1 to match the original indexing\n",
    "\n",
    "        # Keep the sample if it meets both conditions (action correct, identity anonymized)\n",
    "        if anonymized_action == reference_action and anonymized_identity == dummy_identity:\n",
    "            successful_samples[reference_file] = anonymized_skeleton\n",
    "            successful_sample_map[reference_file] = dummy_file\n",
    "            print(f\"Sample {len(successful_samples)} added: {reference_file}\")\n",
    "        else:\n",
    "            if verbose: print(f\"Sample rejected: {reference_file} using {dummy_file}. Predicted action: {anonymized_action.item()}, Actual action: {reference_action}. Predicted identity: {anonymized_identity.item()}, Actual identity: {dummy_identity}\")\n",
    "\n",
    "    return successful_samples, successful_sample_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 added: b'S008C001P001R002A019'\n",
      "Sample 2 added: b'S012C003P016R002A025'\n",
      "Sample 3 added: b'S007C001P025R001A022'\n",
      "Sample 4 added: b'S007C001P027R002A013'\n",
      "Sample 5 added: b'S015C002P025R002A025'\n",
      "Sample 6 added: b'S008C003P030R001A001'\n",
      "Sample 7 added: b'S003C002P018R002A011'\n",
      "Sample 8 added: b'S015C002P016R002A043'\n",
      "Sample 9 added: b'S008C003P032R001A018'\n",
      "Sample 10 added: b'S010C002P013R002A020'\n",
      "Sample 11 added: b'S008C001P008R002A027'\n",
      "Sample 12 added: b'S013C002P018R002A046'\n",
      "Sample 13 added: b'S004C002P007R002A022'\n",
      "Sample 14 added: b'S003C002P018R002A009'\n",
      "Sample 15 added: b'S004C002P003R002A007'\n",
      "Sample 16 added: b'S013C001P028R001A002'\n",
      "Sample 17 added: b'S007C003P028R001A025'\n",
      "Sample 18 added: b'S013C001P015R002A013'\n",
      "Sample 19 added: b'S014C001P007R002A028'\n",
      "Sample 20 added: b'S004C002P003R001A001'\n",
      "Sample 21 added: b'S006C001P019R002A014'\n",
      "Sample 22 added: b'S014C003P017R001A020'\n",
      "Sample 23 added: b'S004C002P003R001A025'\n",
      "Sample 24 added: b'S013C002P037R002A013'\n",
      "Sample 25 added: b'S005C002P015R002A004'\n",
      "Sample 26 added: b'S011C003P028R002A032'\n",
      "Sample 27 added: b'S013C002P027R002A032'\n",
      "Sample 28 added: b'S007C002P025R002A017'\n",
      "Sample 29 added: b'S008C003P001R002A018'\n",
      "Sample 30 added: b'S007C002P001R001A032'\n",
      "Sample 31 added: b'S011C002P017R002A021'\n",
      "Sample 32 added: b'S013C002P037R002A025'\n",
      "Sample 33 added: b'S004C003P008R002A011'\n",
      "Sample 34 added: b'S007C001P019R001A025'\n",
      "Sample 35 added: b'S010C002P013R002A013'\n",
      "Sample 36 added: b'S014C001P017R002A025'\n",
      "Sample 37 added: b'S005C001P018R001A032'\n",
      "Sample 38 added: b'S011C003P008R001A013'\n",
      "Sample 39 added: b'S008C002P019R002A028'\n",
      "Sample 40 added: b'S014C002P019R002A007'\n",
      "Sample 41 added: b'S007C001P025R002A020'\n",
      "Sample 42 added: b'S015C001P017R002A027'\n",
      "Sample 43 added: b'S004C002P007R002A009'\n",
      "Sample 44 added: b'S015C002P016R002A028'\n",
      "Sample 45 added: b'S002C001P008R002A033'\n",
      "Sample 46 added: b'S006C001P017R002A031'\n",
      "Sample 47 added: b'S007C002P008R002A011'\n",
      "Sample 48 added: b'S015C002P008R001A028'\n",
      "Sample 49 added: b'S008C003P025R001A021'\n",
      "Sample 50 added: b'S011C003P015R001A025'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{b'S008C001P001R002A019': tensor([[-0.0180, -0.2797, -0.0049,  ...,  0.0624, -0.2265, -0.1845],\n",
       "         [-0.0164, -0.2837, -0.0014,  ...,  0.0557, -0.2318, -0.1813],\n",
       "         [-0.0199, -0.2752, -0.0074,  ...,  0.0618, -0.2329, -0.1846],\n",
       "         ...,\n",
       "         [-0.0191, -0.3284,  0.0691,  ...,  0.1319, -0.2929, -0.1674],\n",
       "         [-0.0223, -0.3384,  0.0986,  ...,  0.1436, -0.2836, -0.1744],\n",
       "         [-0.0264, -0.3379,  0.0781,  ...,  0.1578, -0.2785, -0.1628]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S012C003P016R002A025': tensor([[-0.0029, -0.2672,  0.0542,  ...,  0.0866, -0.2007, -0.1146],\n",
       "         [-0.0029, -0.2658,  0.0543,  ...,  0.0841, -0.1975, -0.1173],\n",
       "         [-0.0007, -0.2658,  0.0564,  ...,  0.0829, -0.1946, -0.1178],\n",
       "         ...,\n",
       "         [ 0.0317, -0.2672,  0.0766,  ...,  0.0832, -0.0530, -0.1649],\n",
       "         [ 0.0317, -0.2659,  0.0753,  ...,  0.0863, -0.0439, -0.1683],\n",
       "         [ 0.0381, -0.2659,  0.0823,  ...,  0.0920, -0.0303, -0.1688]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S007C001P025R001A022': tensor([[-0.0018, -0.2052,  0.0163,  ...,  0.1007, -0.1999, -0.1970],\n",
       "         [-0.0017, -0.2046,  0.0088,  ...,  0.1010, -0.1929, -0.2034],\n",
       "         [-0.0024, -0.2046,  0.0115,  ...,  0.1051, -0.1853, -0.2080],\n",
       "         ...,\n",
       "         [-0.0124, -0.2003, -0.0069,  ...,  0.0745,  0.1601, -0.3083],\n",
       "         [-0.0136, -0.1974, -0.0088,  ...,  0.0647,  0.2170, -0.3094],\n",
       "         [-0.0150, -0.1959, -0.0090,  ...,  0.0636,  0.2498, -0.3080]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S007C001P027R002A013': tensor([[-7.4768e-04, -3.2877e-01, -1.4978e-02,  ...,  2.6679e-01,\n",
       "          -3.2506e-02, -6.2194e-02],\n",
       "         [-1.5524e-03, -3.2678e-01, -1.6796e-02,  ...,  2.7792e-01,\n",
       "          -3.0096e-02, -6.1467e-02],\n",
       "         [-8.5765e-05, -3.2702e-01, -1.5906e-02,  ...,  2.8635e-01,\n",
       "          -2.4534e-02, -6.2946e-02],\n",
       "         ...,\n",
       "         [ 3.1152e-03, -3.2775e-01, -2.3683e-02,  ...,  2.9549e-01,\n",
       "           2.1164e-01, -8.8477e-02],\n",
       "         [ 3.5642e-03, -3.2877e-01, -2.4256e-02,  ...,  2.8242e-01,\n",
       "           2.1930e-01, -9.3857e-02],\n",
       "         [ 7.4830e-03, -3.2892e-01, -2.4293e-02,  ...,  2.7027e-01,\n",
       "           2.0189e-01, -9.3783e-02]], grad_fn=<SliceBackward0>),\n",
       " b'S015C002P025R002A025': tensor([[-0.0017, -0.2669,  0.0128,  ...,  0.1728, -0.0768, -0.2052],\n",
       "         [-0.0027, -0.2652,  0.0075,  ...,  0.1790, -0.0736, -0.2083],\n",
       "         [-0.0014, -0.2644,  0.0057,  ...,  0.1791, -0.0720, -0.2059],\n",
       "         ...,\n",
       "         [-0.0036, -0.2688, -0.0008,  ...,  0.1389, -0.0340, -0.1979],\n",
       "         [-0.0037, -0.2691, -0.0007,  ...,  0.1360, -0.0331, -0.1987],\n",
       "         [-0.0039, -0.2696, -0.0005,  ...,  0.1270, -0.0304, -0.1987]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S008C003P030R001A001': tensor([[-0.0075, -0.3047, -0.0083,  ...,  0.2464, -0.3753, -0.1886],\n",
       "         [-0.0085, -0.3022, -0.0092,  ...,  0.2337, -0.3642, -0.1929],\n",
       "         [-0.0067, -0.3017, -0.0097,  ...,  0.2458, -0.3533, -0.2019],\n",
       "         ...,\n",
       "         [-0.0102, -0.2994, -0.0191,  ...,  0.2597,  0.3508, -0.3808],\n",
       "         [-0.0114, -0.2991, -0.0202,  ...,  0.2483,  0.4716, -0.3912],\n",
       "         [-0.0117, -0.2985, -0.0197,  ...,  0.2583,  0.5075, -0.3904]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S003C002P018R002A011': tensor([[-0.0063, -0.2846, -0.0012,  ...,  0.1962, -0.1029, -0.3213],\n",
       "         [-0.0072, -0.2834, -0.0023,  ...,  0.1986, -0.0956, -0.3305],\n",
       "         [-0.0058, -0.2833, -0.0022,  ...,  0.1995, -0.0877, -0.3338],\n",
       "         ...,\n",
       "         [-0.0094, -0.2908, -0.0095,  ...,  0.1768, -0.0217, -0.4215],\n",
       "         [-0.0095, -0.2917, -0.0097,  ...,  0.1793, -0.0235, -0.4303],\n",
       "         [-0.0096, -0.2920, -0.0094,  ...,  0.1827, -0.0178, -0.4373]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S015C002P016R002A043': tensor([[-1.1933e-02, -3.0766e-01,  1.7890e-01,  ...,  1.7523e-01,\n",
       "          -3.4395e-01,  3.0669e-01],\n",
       "         [-1.0803e-02, -3.0367e-01,  1.7391e-01,  ...,  1.8048e-01,\n",
       "          -3.3833e-01,  2.8515e-01],\n",
       "         [-1.8157e-02, -3.0137e-01,  1.8187e-01,  ...,  1.7192e-01,\n",
       "          -3.3238e-01,  2.9523e-01],\n",
       "         ...,\n",
       "         [-6.3746e-02, -3.1474e-01,  6.6634e-02,  ...,  1.9174e-01,\n",
       "          -1.3111e-01,  4.9330e-02],\n",
       "         [-6.2722e-02, -3.2555e-01,  7.0527e-02,  ...,  1.9135e-01,\n",
       "          -1.2330e-01,  7.0239e-03],\n",
       "         [-6.2648e-02, -3.2682e-01,  8.4519e-02,  ...,  1.9689e-01,\n",
       "          -1.0739e-01, -3.3229e-04]], grad_fn=<SliceBackward0>),\n",
       " b'S008C003P032R001A018': tensor([[-0.0178, -0.2751, -0.0083,  ...,  0.1252, -0.1261, -0.3289],\n",
       "         [-0.0171, -0.2812, -0.0027,  ...,  0.1239, -0.1313, -0.3272],\n",
       "         [-0.0211, -0.2764, -0.0080,  ...,  0.1267, -0.1358, -0.3279],\n",
       "         ...,\n",
       "         [-0.0227, -0.4120,  0.1661,  ...,  0.0999, -0.2359, -0.3278],\n",
       "         [-0.0263, -0.4284,  0.1905,  ...,  0.0904, -0.2492, -0.3314],\n",
       "         [-0.0290, -0.4332,  0.1792,  ...,  0.0801, -0.2680, -0.3213]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S010C002P013R002A020': tensor([[-0.0090, -0.2790,  0.1059,  ..., -0.0196, -0.1419, -0.3838],\n",
       "         [-0.0103, -0.2782,  0.1047,  ..., -0.0207, -0.1371, -0.3926],\n",
       "         [-0.0085, -0.2798,  0.1038,  ..., -0.0212, -0.1359, -0.3934],\n",
       "         ...,\n",
       "         [-0.0187, -0.3038,  0.0930,  ..., -0.0488, -0.1663, -0.4145],\n",
       "         [-0.0203, -0.3040,  0.0931,  ..., -0.0506, -0.1757, -0.4138],\n",
       "         [-0.0206, -0.3048,  0.0923,  ..., -0.0534, -0.1762, -0.4158]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S008C001P008R002A027': tensor([[-2.6538e-03, -2.9028e-01,  2.4515e-02,  ..., -4.0927e-02,\n",
       "          -2.0093e-01, -5.9350e-02],\n",
       "         [-3.1650e-03, -2.8901e-01,  1.6419e-02,  ..., -4.3619e-02,\n",
       "          -1.9558e-01, -5.9733e-02],\n",
       "         [-2.2432e-03, -2.8748e-01,  1.5732e-02,  ..., -4.2612e-02,\n",
       "          -1.8986e-01, -5.9684e-02],\n",
       "         ...,\n",
       "         [-4.5203e-03, -2.7828e-01, -2.0811e-04,  ..., -6.8465e-02,\n",
       "          -1.6979e-01, -4.8298e-02],\n",
       "         [-4.4641e-03, -2.7822e-01, -5.4841e-05,  ..., -6.6366e-02,\n",
       "          -1.7746e-01, -4.5739e-02],\n",
       "         [-4.8329e-03, -2.7727e-01,  3.8465e-03,  ..., -6.6387e-02,\n",
       "          -1.7710e-01, -4.3691e-02]], grad_fn=<SliceBackward0>),\n",
       " b'S013C002P018R002A046': tensor([[-0.0076, -0.2947,  0.0578,  ...,  0.2200, -0.3263,  0.1187],\n",
       "         [-0.0081, -0.2930,  0.0558,  ...,  0.2146, -0.3172,  0.1138],\n",
       "         [-0.0060, -0.2926,  0.0561,  ...,  0.2186, -0.3165,  0.1172],\n",
       "         ...,\n",
       "         [-0.0097, -0.2892,  0.0400,  ...,  0.2883,  0.0772, -0.0092],\n",
       "         [-0.0108, -0.2885,  0.0378,  ...,  0.2856,  0.2115, -0.0161],\n",
       "         [-0.0110, -0.2878,  0.0402,  ...,  0.2911,  0.2952, -0.0170]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S004C002P007R002A022': tensor([[-0.0021, -0.2617,  0.0284,  ...,  0.3920,  0.0310, -0.0994],\n",
       "         [-0.0031, -0.2595,  0.0233,  ...,  0.4003,  0.0684, -0.0988],\n",
       "         [-0.0017, -0.2591,  0.0248,  ...,  0.4110,  0.0878, -0.0982],\n",
       "         ...,\n",
       "         [-0.0035, -0.2458,  0.0307,  ...,  0.3917,  0.4156, -0.1027],\n",
       "         [-0.0042, -0.2436,  0.0308,  ...,  0.3857,  0.4154, -0.0978],\n",
       "         [-0.0041, -0.2418,  0.0317,  ...,  0.3846,  0.4242, -0.1013]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S003C002P018R002A009': tensor([[-1.6834e-04, -2.6374e-01, -2.0505e-02,  ...,  1.7023e-01,\n",
       "          -2.3122e-01, -1.3253e-01],\n",
       "         [-7.3943e-04, -2.6106e-01, -2.2397e-02,  ...,  1.7683e-01,\n",
       "          -2.3138e-01, -1.3508e-01],\n",
       "         [-1.0371e-03, -2.5938e-01, -2.3190e-02,  ...,  1.7676e-01,\n",
       "          -2.2903e-01, -1.3687e-01],\n",
       "         ...,\n",
       "         [-1.1532e-02, -2.7160e-01, -4.3990e-02,  ...,  1.5112e-01,\n",
       "          -1.5174e-01, -1.7185e-01],\n",
       "         [-1.3091e-02, -2.7265e-01, -4.4749e-02,  ...,  1.4861e-01,\n",
       "          -1.4652e-01, -1.7971e-01],\n",
       "         [-1.4306e-02, -2.7375e-01, -4.7091e-02,  ...,  1.3967e-01,\n",
       "          -1.3951e-01, -1.8125e-01]], grad_fn=<SliceBackward0>),\n",
       " b'S004C002P003R002A007': tensor([[ 0.1359, -0.3272, -0.0397,  ...,  0.2073, -0.2783, -0.1812],\n",
       "         [ 0.1351, -0.3319, -0.0403,  ...,  0.2232, -0.2851, -0.1806],\n",
       "         [ 0.1138, -0.3300, -0.0436,  ...,  0.2282, -0.2855, -0.1865],\n",
       "         ...,\n",
       "         [ 0.1115, -0.4317, -0.0571,  ...,  0.3173, -0.3977, -0.1975],\n",
       "         [ 0.0960, -0.4460, -0.0641,  ...,  0.3149, -0.4036, -0.2014],\n",
       "         [ 0.0924, -0.4446, -0.0719,  ...,  0.3052, -0.4023, -0.1916]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S013C001P028R001A002': tensor([[ 0.0120, -0.2513,  0.0832,  ..., -0.0049, -0.0097, -0.2671],\n",
       "         [ 0.0094, -0.2501,  0.0810,  ..., -0.0049, -0.0041, -0.2755],\n",
       "         [ 0.0189, -0.2508,  0.0841,  ..., -0.0051,  0.0013, -0.2757],\n",
       "         ...,\n",
       "         [-0.0009, -0.2499,  0.0693,  ..., -0.0192,  0.0661, -0.2956],\n",
       "         [-0.0007, -0.2497,  0.0676,  ..., -0.0203,  0.0665, -0.2940],\n",
       "         [-0.0009, -0.2497,  0.0700,  ..., -0.0213,  0.0660, -0.2924]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S007C003P028R001A025': tensor([[-0.0035, -0.2686,  0.0148,  ...,  0.1458, -0.2564, -0.1115],\n",
       "         [-0.0041, -0.2671,  0.0104,  ...,  0.1448, -0.2510, -0.1177],\n",
       "         [-0.0024, -0.2670,  0.0120,  ...,  0.1467, -0.2490, -0.1169],\n",
       "         ...,\n",
       "         [-0.0035, -0.2730, -0.0047,  ...,  0.1379, -0.1770, -0.1609],\n",
       "         [-0.0036, -0.2727, -0.0053,  ...,  0.1367, -0.1738, -0.1683],\n",
       "         [-0.0034, -0.2727, -0.0052,  ...,  0.1378, -0.1647, -0.1710]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S013C001P015R002A013': tensor([[-0.0146, -0.2569, -0.0180,  ...,  0.1568, -0.1102,  0.1507],\n",
       "         [-0.0157, -0.2558, -0.0197,  ...,  0.1601, -0.1095,  0.1522],\n",
       "         [-0.0150, -0.2566, -0.0179,  ...,  0.1600, -0.1058,  0.1505],\n",
       "         ...,\n",
       "         [-0.0332, -0.2677, -0.0330,  ...,  0.1029, -0.1058,  0.1081],\n",
       "         [-0.0350, -0.2677, -0.0346,  ...,  0.0987, -0.1087,  0.1046],\n",
       "         [-0.0364, -0.2679, -0.0361,  ...,  0.0899, -0.1089,  0.0978]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S014C001P007R002A028': tensor([[-0.0056, -0.2930,  0.1336,  ...,  0.2794, -0.2088, -0.0943],\n",
       "         [-0.0060, -0.2915,  0.1346,  ...,  0.2719, -0.1925, -0.1011],\n",
       "         [-0.0038, -0.2913,  0.1340,  ...,  0.2723, -0.1782, -0.1084],\n",
       "         ...,\n",
       "         [-0.0108, -0.2842,  0.1301,  ...,  0.1960,  0.4003, -0.3383],\n",
       "         [-0.0116, -0.2833,  0.1299,  ...,  0.1879,  0.4422, -0.3605],\n",
       "         [-0.0114, -0.2826,  0.1336,  ...,  0.1927,  0.4490, -0.3756]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S004C002P003R001A001': tensor([[-0.0049, -0.2964,  0.0711,  ..., -0.0971, -0.0130,  0.2763],\n",
       "         [-0.0059, -0.2944,  0.0683,  ..., -0.1007, -0.0062,  0.2525],\n",
       "         [-0.0042, -0.2954,  0.0665,  ..., -0.1004, -0.0034,  0.2845],\n",
       "         ...,\n",
       "         [-0.0185, -0.3003,  0.0772,  ..., -0.0997,  0.0970,  0.3789],\n",
       "         [-0.0210, -0.3012,  0.0746,  ..., -0.0942,  0.0688,  0.3747],\n",
       "         [-0.0229, -0.3022,  0.0834,  ..., -0.0893,  0.0822,  0.3568]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S006C001P019R002A014': tensor([[ 0.0122, -0.3008,  0.0574,  ...,  0.2119, -0.0014, -0.2788],\n",
       "         [ 0.0080, -0.2992,  0.0534,  ...,  0.2113,  0.0111, -0.2832],\n",
       "         [ 0.0112, -0.2987,  0.0549,  ...,  0.2174,  0.0217, -0.2821],\n",
       "         ...,\n",
       "         [-0.0026, -0.3007,  0.0137,  ...,  0.1399,  0.0325, -0.3255],\n",
       "         [-0.0019, -0.2999,  0.0125,  ...,  0.1298,  0.0134, -0.3259],\n",
       "         [-0.0016, -0.2995,  0.0173,  ...,  0.1189,  0.0112, -0.3301]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S014C003P017R001A020': tensor([[ 0.0176, -0.1433,  0.0781,  ..., -0.0015, -0.0523, -0.1919],\n",
       "         [ 0.0161, -0.1432,  0.0765,  ..., -0.0021, -0.0522, -0.1946],\n",
       "         [ 0.0152, -0.1420,  0.0765,  ..., -0.0022, -0.0505, -0.1962],\n",
       "         ...,\n",
       "         [-0.0042, -0.1406,  0.0588,  ..., -0.0115, -0.0515, -0.2175],\n",
       "         [-0.0046, -0.1386,  0.0642,  ..., -0.0126, -0.0534, -0.2177],\n",
       "         [-0.0066, -0.1373,  0.0590,  ..., -0.0145, -0.0534, -0.2186]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S004C002P003R001A025': tensor([[-0.0069, -0.3040,  0.0345,  ...,  0.0168, -0.2825, -0.1548],\n",
       "         [-0.0065, -0.3046,  0.0408,  ...,  0.0196, -0.2849, -0.1558],\n",
       "         [-0.0069, -0.3004,  0.0278,  ...,  0.0254, -0.2818, -0.1601],\n",
       "         ...,\n",
       "         [-0.0087, -0.3536,  0.0730,  ...,  0.0161, -0.2517, -0.1521],\n",
       "         [-0.0107, -0.3658,  0.0852,  ...,  0.0128, -0.2457, -0.1565],\n",
       "         [-0.0120, -0.3719,  0.0836,  ...,  0.0062, -0.2354, -0.1503]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S013C002P037R002A013': tensor([[ 0.0024, -0.2841,  0.0776,  ...,  0.2051, -0.1504,  0.0862],\n",
       "         [ 0.0003, -0.2826,  0.0755,  ...,  0.2181, -0.1459,  0.0783],\n",
       "         [ 0.0115, -0.2833,  0.0782,  ...,  0.2158, -0.1405,  0.0679],\n",
       "         ...,\n",
       "         [ 0.0032, -0.2843,  0.0739,  ...,  0.2232, -0.0111, -0.0477],\n",
       "         [ 0.0024, -0.2839,  0.0706,  ...,  0.2198, -0.0024, -0.0566],\n",
       "         [ 0.0033, -0.2837,  0.0743,  ...,  0.2083,  0.0142, -0.0583]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S005C002P015R002A004': tensor([[ 0.0440, -0.2668,  0.0596,  ...,  0.3434, -0.2360,  0.0284],\n",
       "         [ 0.0378, -0.2663,  0.0548,  ...,  0.3377, -0.2341,  0.0227],\n",
       "         [ 0.0423, -0.2640,  0.0553,  ...,  0.3611, -0.2249,  0.0075],\n",
       "         ...,\n",
       "         [ 0.0424, -0.2810, -0.0043,  ...,  0.5531, -0.0738, -0.1514],\n",
       "         [ 0.0406, -0.2796, -0.0065,  ...,  0.5736, -0.0479, -0.1751],\n",
       "         [ 0.0407, -0.2780, -0.0084,  ...,  0.5975, -0.0258, -0.1917]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S011C003P028R002A032': tensor([[-0.0007, -0.2101,  0.0825,  ...,  0.1484, -0.1734, -0.2232],\n",
       "         [-0.0011, -0.2090,  0.0817,  ...,  0.1462, -0.1724, -0.2283],\n",
       "         [ 0.0040, -0.2090,  0.0856,  ...,  0.1482, -0.1672, -0.2327],\n",
       "         ...,\n",
       "         [-0.0020, -0.2012,  0.0818,  ...,  0.1234, -0.1309, -0.3482],\n",
       "         [-0.0019, -0.1994,  0.0795,  ...,  0.1179, -0.1216, -0.3626],\n",
       "         [-0.0022, -0.1986,  0.0810,  ...,  0.1174, -0.1075, -0.3712]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S013C002P027R002A032': tensor([[-0.0070, -0.3085,  0.0766,  ...,  0.2227, -0.2954, -0.0789],\n",
       "         [-0.0077, -0.3073,  0.0730,  ...,  0.2275, -0.2924, -0.0823],\n",
       "         [-0.0059, -0.3073,  0.0741,  ...,  0.2280, -0.2890, -0.0829],\n",
       "         ...,\n",
       "         [-0.0086, -0.3161,  0.0711,  ...,  0.2373, -0.2609, -0.1256],\n",
       "         [-0.0092, -0.3163,  0.0726,  ...,  0.2360, -0.2679, -0.1288],\n",
       "         [-0.0091, -0.3166,  0.0756,  ...,  0.2351, -0.2647, -0.1323]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S007C002P025R002A017': tensor([[ 0.0259, -0.3320,  0.1547,  ...,  0.1905, -0.6522,  0.0503],\n",
       "         [ 0.0279, -0.3284,  0.1503,  ...,  0.2006, -0.6572,  0.0178],\n",
       "         [ 0.0288, -0.3294,  0.1584,  ...,  0.1933, -0.6638,  0.0387],\n",
       "         ...,\n",
       "         [ 0.0382, -0.3431,  0.1096,  ...,  0.2121, -0.6245, -0.0148],\n",
       "         [ 0.0356, -0.3452,  0.1044,  ...,  0.2018, -0.6040, -0.0224],\n",
       "         [ 0.0444, -0.3476,  0.0916,  ...,  0.1964, -0.5606, -0.0305]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S008C003P001R002A018': tensor([[ 0.0213, -0.2556,  0.1053,  ..., -0.0310, -0.0871, -0.1420],\n",
       "         [ 0.0184, -0.2541,  0.1029,  ..., -0.0313, -0.0833, -0.1439],\n",
       "         [ 0.0280, -0.2540,  0.1044,  ..., -0.0312, -0.0761, -0.1486],\n",
       "         ...,\n",
       "         [ 0.0129, -0.2423,  0.1042,  ..., -0.0293, -0.0330, -0.1298],\n",
       "         [ 0.0099, -0.2410,  0.1036,  ..., -0.0287, -0.0328, -0.1268],\n",
       "         [ 0.0097, -0.2403,  0.1059,  ..., -0.0285, -0.0254, -0.1225]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S007C002P001R001A032': tensor([[-4.6896e-05, -2.2609e-01,  5.1072e-02,  ...,  4.4912e-02,\n",
       "           3.0139e-01, -2.2737e-01],\n",
       "         [-8.3740e-04, -2.2470e-01,  4.7413e-02,  ...,  3.9683e-02,\n",
       "           3.2997e-01, -2.3519e-01],\n",
       "         [ 2.9562e-03, -2.2434e-01,  4.5899e-02,  ...,  4.6468e-02,\n",
       "           3.3206e-01, -2.3182e-01],\n",
       "         ...,\n",
       "         [-4.8183e-03, -2.2326e-01,  2.8415e-02,  ..., -7.4589e-03,\n",
       "           3.2595e-01, -2.1215e-01],\n",
       "         [-5.3504e-03, -2.2322e-01,  2.7090e-02,  ..., -9.0266e-03,\n",
       "           3.2872e-01, -2.0696e-01],\n",
       "         [-5.9758e-03, -2.2326e-01,  2.7285e-02,  ..., -1.0596e-02,\n",
       "           3.3632e-01, -2.0814e-01]], grad_fn=<SliceBackward0>),\n",
       " b'S011C002P017R002A021': tensor([[-0.0028, -0.2231, -0.0013,  ..., -0.0864, -0.0160, -0.1656],\n",
       "         [-0.0036, -0.2220, -0.0019,  ..., -0.0874, -0.0143, -0.1709],\n",
       "         [-0.0025, -0.2213, -0.0014,  ..., -0.0865, -0.0136, -0.1696],\n",
       "         ...,\n",
       "         [-0.0074, -0.2270, -0.0048,  ..., -0.0790, -0.0357, -0.1662],\n",
       "         [-0.0081, -0.2278, -0.0040,  ..., -0.0775, -0.0401, -0.1657],\n",
       "         [-0.0091, -0.2280, -0.0044,  ..., -0.0763, -0.0416, -0.1672]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S013C002P037R002A025': tensor([[-1.2548e-03, -3.2336e-01,  3.2071e-02,  ..., -1.1621e-01,\n",
       "          -8.5689e-02,  6.9366e-02],\n",
       "         [-1.7761e-03, -3.2193e-01,  3.2221e-02,  ..., -1.2102e-01,\n",
       "          -7.8376e-02,  5.2611e-02],\n",
       "         [ 1.4447e-04, -3.2265e-01,  3.4593e-02,  ..., -1.2251e-01,\n",
       "          -7.4469e-02,  5.0949e-02],\n",
       "         ...,\n",
       "         [-6.8884e-03, -3.3927e-01,  1.9626e-02,  ..., -1.9690e-01,\n",
       "           2.7782e-02,  5.3221e-02],\n",
       "         [-7.5609e-03, -3.4231e-01,  1.6524e-02,  ..., -1.9664e-01,\n",
       "           2.3401e-02,  3.8060e-02],\n",
       "         [-7.8974e-03, -3.4420e-01,  2.3849e-02,  ..., -1.9746e-01,\n",
       "           5.4256e-02,  3.4035e-02]], grad_fn=<SliceBackward0>),\n",
       " b'S004C003P008R002A011': tensor([[ 0.0140, -0.2891,  0.0598,  ...,  0.1375, -0.2642, -0.0556],\n",
       "         [ 0.0117, -0.2873,  0.0574,  ...,  0.1382, -0.2615, -0.0600],\n",
       "         [ 0.0221, -0.2874,  0.0580,  ...,  0.1367, -0.2593, -0.0623],\n",
       "         ...,\n",
       "         [ 0.0272, -0.2975,  0.0456,  ...,  0.1081, -0.0808, -0.1464],\n",
       "         [ 0.0279, -0.2981,  0.0419,  ...,  0.1059, -0.0616, -0.1592],\n",
       "         [ 0.0309, -0.2993,  0.0477,  ...,  0.1043, -0.0434, -0.1620]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S007C001P019R001A025': tensor([[-0.0032, -0.3546, -0.0143,  ..., -0.0110, -0.2270, -0.0934],\n",
       "         [-0.0040, -0.3524, -0.0155,  ..., -0.0117, -0.2218, -0.0965],\n",
       "         [-0.0026, -0.3520, -0.0155,  ..., -0.0119, -0.2158, -0.0992],\n",
       "         ...,\n",
       "         [-0.0045, -0.3653, -0.0243,  ..., -0.0208, -0.0130, -0.1121],\n",
       "         [-0.0049, -0.3683, -0.0248,  ..., -0.0223,  0.0040, -0.1177],\n",
       "         [-0.0046, -0.3694, -0.0244,  ..., -0.0227,  0.0520, -0.1140]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S010C002P013R002A013': tensor([[-0.0054, -0.3090,  0.0477,  ..., -0.0415, -0.2058, -0.0054],\n",
       "         [-0.0060, -0.3071,  0.0440,  ..., -0.0425, -0.2024, -0.0097],\n",
       "         [-0.0040, -0.3073,  0.0448,  ..., -0.0426, -0.2015, -0.0092],\n",
       "         ...,\n",
       "         [-0.0051, -0.3137,  0.0412,  ..., -0.0449, -0.1648,  0.0508],\n",
       "         [-0.0054, -0.3148,  0.0408,  ..., -0.0440, -0.1613,  0.0440],\n",
       "         [-0.0054, -0.3152,  0.0452,  ..., -0.0436, -0.1448,  0.0301]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S014C001P017R002A025': tensor([[-1.1797e-03, -1.9754e-01,  5.5659e-02,  ...,  8.4821e-02,\n",
       "          -2.5241e-01, -4.4752e-02],\n",
       "         [-1.6146e-03, -1.9675e-01,  5.4537e-02,  ...,  7.8558e-02,\n",
       "          -2.5684e-01, -4.2520e-02],\n",
       "         [-4.3751e-05, -1.9677e-01,  5.9424e-02,  ...,  7.8843e-02,\n",
       "          -2.5449e-01, -4.7048e-02],\n",
       "         ...,\n",
       "         [-4.0441e-03, -2.0044e-01,  3.0169e-02,  ...,  2.1449e-02,\n",
       "          -2.7893e-01, -6.3895e-02],\n",
       "         [-4.1434e-03, -1.9982e-01,  2.7286e-02,  ...,  1.7145e-02,\n",
       "          -2.8750e-01, -6.2572e-02],\n",
       "         [-4.4463e-03, -2.0032e-01,  2.5937e-02,  ...,  1.1523e-02,\n",
       "          -2.9415e-01, -5.7906e-02]], grad_fn=<SliceBackward0>),\n",
       " b'S005C001P018R001A032': tensor([[-0.0100, -0.2882,  0.0258,  ...,  0.0131, -0.3357, -0.1608],\n",
       "         [-0.0098, -0.2904,  0.0270,  ...,  0.0191, -0.3454, -0.1571],\n",
       "         [-0.0102, -0.2874,  0.0201,  ...,  0.0219, -0.3445, -0.1629],\n",
       "         ...,\n",
       "         [-0.0170, -0.3085,  0.0129,  ...,  0.0039, -0.3555, -0.2369],\n",
       "         [-0.0192, -0.3107,  0.0184,  ..., -0.0020, -0.3372, -0.2537],\n",
       "         [-0.0214, -0.3106,  0.0163,  ..., -0.0058, -0.3001, -0.2709]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S011C003P008R001A013': tensor([[-0.0126, -0.3165, -0.0036,  ...,  0.1369, -0.2411, -0.3139],\n",
       "         [-0.0127, -0.3199, -0.0024,  ...,  0.1407, -0.2485, -0.3129],\n",
       "         [-0.0129, -0.3200, -0.0014,  ...,  0.1416, -0.2449, -0.3229],\n",
       "         ...,\n",
       "         [-0.0263, -0.3723,  0.0115,  ...,  0.1736, -0.3457, -0.4216],\n",
       "         [-0.0298, -0.3740, -0.0019,  ...,  0.1679, -0.3501, -0.4446],\n",
       "         [-0.0324, -0.3735, -0.0032,  ...,  0.1743, -0.3404, -0.4569]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S008C002P019R002A028': tensor([[-2.9518e-04, -2.9846e-01,  1.0682e-01,  ...,  9.2456e-02,\n",
       "          -1.9292e-01, -1.3506e-01],\n",
       "         [-8.4187e-04, -2.9499e-01,  1.0584e-01,  ...,  8.8505e-02,\n",
       "          -1.8359e-01, -1.3906e-01],\n",
       "         [ 8.3611e-03, -2.9344e-01,  9.8877e-02,  ...,  8.4138e-02,\n",
       "          -1.6893e-01, -1.4823e-01],\n",
       "         ...,\n",
       "         [-5.8338e-03, -2.9865e-01,  5.5084e-02,  ...,  1.4449e-02,\n",
       "           3.1965e-01, -3.0311e-01],\n",
       "         [-6.6025e-03, -2.9886e-01,  5.6527e-02,  ...,  1.5183e-02,\n",
       "           3.5855e-01, -3.1010e-01],\n",
       "         [-6.7626e-03, -3.0008e-01,  6.0635e-02,  ...,  1.1924e-02,\n",
       "           3.4115e-01, -3.1123e-01]], grad_fn=<SliceBackward0>),\n",
       " b'S014C002P019R002A007': tensor([[-1.0828e-04, -3.0718e-01,  5.7372e-02,  ...,  5.6988e-02,\n",
       "          -3.3725e-01, -7.0463e-02],\n",
       "         [-9.5340e-04, -3.0658e-01,  5.3399e-02,  ...,  6.8951e-02,\n",
       "          -3.4674e-01, -6.8557e-02],\n",
       "         [-1.0809e-03, -3.0867e-01,  6.1939e-02,  ...,  6.3844e-02,\n",
       "          -3.5736e-01, -6.6527e-02],\n",
       "         ...,\n",
       "         [-1.2590e-02, -3.3681e-01, -2.3266e-03,  ...,  7.4318e-02,\n",
       "          -4.3957e-01,  1.1222e-01],\n",
       "         [-1.3297e-02, -3.3663e-01, -5.2172e-03,  ...,  6.5647e-02,\n",
       "          -4.2914e-01,  9.9090e-02],\n",
       "         [-1.2749e-02, -3.3906e-01, -6.0456e-03,  ...,  4.7930e-02,\n",
       "          -3.8087e-01,  6.1883e-02]], grad_fn=<SliceBackward0>),\n",
       " b'S007C001P025R002A020': tensor([[ 0.0061, -0.2419,  0.0429,  ...,  0.3411, -0.2672, -0.0618],\n",
       "         [ 0.0014, -0.2395,  0.0375,  ...,  0.3420, -0.2561, -0.0605],\n",
       "         [ 0.0131, -0.2387,  0.0345,  ...,  0.3518, -0.2492, -0.0660],\n",
       "         ...,\n",
       "         [ 0.0126, -0.2357,  0.0108,  ...,  0.3626,  0.3846, -0.0787],\n",
       "         [ 0.0120, -0.2343,  0.0064,  ...,  0.3502,  0.4770, -0.0791],\n",
       "         [ 0.0118, -0.2342,  0.0107,  ...,  0.3477,  0.5336, -0.0789]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S015C001P017R002A027': tensor([[ 3.3712e-02, -2.7078e-01,  1.1076e-01,  ...,  2.4149e-01,\n",
       "          -2.9118e-01,  3.3082e-03],\n",
       "         [ 3.2635e-02, -2.7045e-01,  1.0747e-01,  ...,  2.4801e-01,\n",
       "          -2.9199e-01,  1.1609e-02],\n",
       "         [ 4.0908e-02, -2.6931e-01,  1.1054e-01,  ...,  2.5370e-01,\n",
       "          -2.8994e-01,  1.5165e-02],\n",
       "         ...,\n",
       "         [ 8.2644e-02, -2.8178e-01,  1.1778e-01,  ...,  3.2614e-01,\n",
       "          -2.9977e-01, -5.4689e-03],\n",
       "         [ 8.7559e-02, -2.8204e-01,  1.1823e-01,  ...,  3.3200e-01,\n",
       "          -3.0721e-01, -3.8867e-03],\n",
       "         [ 9.2609e-02, -2.8017e-01,  1.1872e-01,  ...,  3.3245e-01,\n",
       "          -3.1234e-01, -1.7494e-04]], grad_fn=<SliceBackward0>),\n",
       " b'S004C002P007R002A009': tensor([[ 0.0233, -0.2576, -0.0145,  ..., -0.1679,  0.3910, -0.2304],\n",
       "         [ 0.0148, -0.2515, -0.0187,  ..., -0.1719,  0.4233, -0.2316],\n",
       "         [ 0.0359, -0.2507, -0.0196,  ..., -0.1726,  0.4127, -0.2263],\n",
       "         ...,\n",
       "         [ 0.0254, -0.2448, -0.0338,  ..., -0.1888,  0.1891, -0.1775],\n",
       "         [ 0.0218, -0.2420, -0.0333,  ..., -0.1851,  0.1523, -0.1696],\n",
       "         [ 0.0194, -0.2421, -0.0307,  ..., -0.1854,  0.1372, -0.1754]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S015C002P016R002A028': tensor([[-1.7396e-04, -2.6079e-01,  9.3260e-02,  ...,  2.0690e-01,\n",
       "          -2.9134e-01, -1.6987e-02],\n",
       "         [-8.7839e-04, -2.5976e-01,  9.0195e-02,  ...,  2.1227e-01,\n",
       "          -2.8830e-01, -2.0577e-02],\n",
       "         [ 4.6357e-03, -2.5984e-01,  9.2419e-02,  ...,  2.1190e-01,\n",
       "          -2.8605e-01, -1.8326e-02],\n",
       "         ...,\n",
       "         [-3.6955e-03, -2.7157e-01,  7.3967e-02,  ...,  2.2501e-01,\n",
       "          -3.0918e-01, -5.3144e-02],\n",
       "         [-4.3277e-03, -2.7118e-01,  7.0769e-02,  ...,  2.2767e-01,\n",
       "          -3.1862e-01, -5.5636e-02],\n",
       "         [-4.1934e-03, -2.7202e-01,  7.2651e-02,  ...,  2.2830e-01,\n",
       "          -3.1592e-01, -6.1089e-02]], grad_fn=<SliceBackward0>),\n",
       " b'S002C001P008R002A033': tensor([[ 0.0028, -0.2748,  0.1154,  ...,  0.1827, -0.2446, -0.0769],\n",
       "         [ 0.0007, -0.2737,  0.1122,  ...,  0.1869, -0.2424, -0.0814],\n",
       "         [ 0.0109, -0.2744,  0.1151,  ...,  0.1820, -0.2398, -0.0846],\n",
       "         ...,\n",
       "         [ 0.0061, -0.2834,  0.1133,  ...,  0.1269, -0.1660, -0.1620],\n",
       "         [ 0.0076, -0.2847,  0.1149,  ...,  0.1240, -0.1598, -0.1747],\n",
       "         [ 0.0087, -0.2856,  0.1180,  ...,  0.1184, -0.1439, -0.1827]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S006C001P017R002A031': tensor([[-1.9324e-03, -2.6155e-01,  5.0734e-02,  ..., -4.0183e-02,\n",
       "          -2.1068e-01, -1.3300e-01],\n",
       "         [-3.0149e-03, -2.6035e-01,  5.0034e-02,  ..., -4.5087e-02,\n",
       "          -2.1089e-01, -1.3494e-01],\n",
       "         [-1.5772e-03, -2.5872e-01,  4.7089e-02,  ..., -4.7484e-02,\n",
       "          -2.0375e-01, -1.4118e-01],\n",
       "         ...,\n",
       "         [-1.1001e-02, -2.6440e-01,  2.2804e-03,  ..., -1.2236e-01,\n",
       "          -6.6726e-02, -2.4131e-01],\n",
       "         [-1.2374e-02, -2.6256e-01, -2.3926e-04,  ..., -1.2357e-01,\n",
       "          -5.4088e-02, -2.5188e-01],\n",
       "         [-1.3192e-02, -2.6187e-01,  9.0939e-04,  ..., -1.2306e-01,\n",
       "          -3.5898e-02, -2.5896e-01]], grad_fn=<SliceBackward0>),\n",
       " b'S007C002P008R002A011': tensor([[-0.0060, -0.2891,  0.0169,  ...,  0.2003, -0.1194, -0.2125],\n",
       "         [-0.0069, -0.2880,  0.0138,  ...,  0.2029, -0.1114, -0.2181],\n",
       "         [-0.0055, -0.2876,  0.0142,  ...,  0.2061, -0.1046, -0.2198],\n",
       "         ...,\n",
       "         [-0.0119, -0.2928, -0.0007,  ...,  0.1718, -0.0071, -0.2939],\n",
       "         [-0.0124, -0.2933, -0.0007,  ...,  0.1710, -0.0068, -0.3064],\n",
       "         [-0.0128, -0.2930, -0.0005,  ...,  0.1697, -0.0048, -0.3139]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S015C002P008R001A028': tensor([[-0.0034, -0.2830,  0.0659,  ...,  0.0175, -0.1391, -0.2599],\n",
       "         [-0.0039, -0.2815,  0.0631,  ...,  0.0184, -0.1374, -0.2645],\n",
       "         [-0.0022, -0.2821,  0.0663,  ...,  0.0203, -0.1331, -0.2674],\n",
       "         ...,\n",
       "         [-0.0070, -0.2828,  0.0378,  ..., -0.0158, -0.0948, -0.3223],\n",
       "         [-0.0073, -0.2830,  0.0365,  ..., -0.0191, -0.0972, -0.3270],\n",
       "         [-0.0075, -0.2832,  0.0379,  ..., -0.0225, -0.0948, -0.3303]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S008C003P025R001A021': tensor([[-0.0261, -0.3070,  0.0113,  ..., -0.0809, -0.3991, -0.2799],\n",
       "         [-0.0272, -0.3097,  0.0377,  ..., -0.0861, -0.4010, -0.2791],\n",
       "         [-0.0262, -0.3049,  0.0207,  ..., -0.0880, -0.3958, -0.2804],\n",
       "         ...,\n",
       "         [-0.0636, -0.3063,  0.1292,  ..., -0.1321, -0.4220, -0.2645],\n",
       "         [-0.0694, -0.3045,  0.1361,  ..., -0.1354, -0.4255, -0.2616],\n",
       "         [-0.0738, -0.2970,  0.1347,  ..., -0.1366, -0.3921, -0.2665]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " b'S011C003P015R001A025': tensor([[-0.0082, -0.2861,  0.0473,  ...,  0.1580, -0.1990, -0.0461],\n",
       "         [-0.0088, -0.2848,  0.0460,  ...,  0.1554, -0.1882, -0.0487],\n",
       "         [-0.0070, -0.2851,  0.0461,  ...,  0.1555, -0.1815, -0.0489],\n",
       "         ...,\n",
       "         [-0.0134, -0.2912,  0.0295,  ...,  0.1916,  0.1488, -0.0967],\n",
       "         [-0.0141, -0.2919,  0.0285,  ...,  0.1931,  0.2073, -0.1051],\n",
       "         [-0.0145, -0.2919,  0.0310,  ...,  0.1965,  0.2327, -0.1085]],\n",
       "        grad_fn=<SliceBackward0>)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples, key_to_dummy = sample_motion_retargeting(X, model, sgn_ar, sgn_priv)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the samples\n",
    "with open('results/samples.pkl', 'wb') as f:\n",
    "    pickle.dump(samples, f)\n",
    "with open('results/samples_map.pkl', 'wb') as f:\n",
    "    pickle.dump(key_to_dummy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'S008C001P001R002A019': b'S004C002P007R002A006',\n",
       " b'S012C003P016R002A025': b'S013C001P027R002A040',\n",
       " b'S007C001P025R001A022': b'S012C001P019R002A001',\n",
       " b'S007C001P027R002A013': b'S008C003P019R002A049',\n",
       " b'S015C002P025R002A025': b'S008C002P015R002A001',\n",
       " b'S008C003P030R001A001': b'S009C002P019R002A023',\n",
       " b'S003C002P018R002A011': b'S003C003P019R002A011',\n",
       " b'S015C002P016R002A043': b'S014C003P027R001A021',\n",
       " b'S008C003P032R001A018': b'S001C001P008R001A006',\n",
       " b'S010C002P013R002A020': b'S013C001P019R001A017',\n",
       " b'S008C001P008R002A027': b'S006C003P007R002A012',\n",
       " b'S013C002P018R002A046': b'S008C003P008R001A038',\n",
       " b'S004C002P007R002A022': b'S014C003P019R002A039',\n",
       " b'S003C002P018R002A009': b'S006C001P007R002A045',\n",
       " b'S004C002P003R002A007': b'S011C003P007R002A006',\n",
       " b'S013C001P028R001A002': b'S011C002P027R002A034',\n",
       " b'S007C003P028R001A025': b'S010C001P019R001A038',\n",
       " b'S013C001P015R002A013': b'S006C003P019R002A036',\n",
       " b'S014C001P007R002A028': b'S014C002P027R002A021',\n",
       " b'S004C002P003R001A001': b'S008C002P007R001A031',\n",
       " b'S006C001P019R002A014': b'S013C001P017R001A027',\n",
       " b'S014C003P017R001A020': b'S014C002P027R001A024',\n",
       " b'S004C002P003R001A025': b'S015C001P016R002A016',\n",
       " b'S013C002P037R002A013': b'S011C003P027R002A034',\n",
       " b'S005C002P015R002A004': b'S015C002P019R002A007',\n",
       " b'S011C003P028R002A032': b'S014C003P019R001A032',\n",
       " b'S013C002P027R002A032': b'S015C002P007R002A020',\n",
       " b'S007C002P025R002A017': b'S015C003P015R001A010',\n",
       " b'S008C003P001R002A018': b'S014C001P019R001A004',\n",
       " b'S007C002P001R001A032': b'S012C003P019R001A047',\n",
       " b'S011C002P017R002A021': b'S007C002P025R001A040',\n",
       " b'S013C002P037R002A025': b'S009C002P015R001A032',\n",
       " b'S004C003P008R002A011': b'S013C002P027R002A010',\n",
       " b'S007C001P019R001A025': b'S008C001P007R001A003',\n",
       " b'S010C002P013R002A013': b'S007C003P016R002A040',\n",
       " b'S014C001P017R002A025': b'S013C002P025R001A025',\n",
       " b'S005C001P018R001A032': b'S007C001P027R002A035',\n",
       " b'S011C003P008R001A013': b'S002C002P007R002A009',\n",
       " b'S008C002P019R002A028': b'S001C001P007R001A019',\n",
       " b'S014C002P019R002A007': b'S001C001P007R002A026',\n",
       " b'S007C001P025R002A020': b'S002C003P007R002A021',\n",
       " b'S015C001P017R002A027': b'S014C002P007R002A005',\n",
       " b'S004C002P007R002A009': b'S007C002P019R001A025',\n",
       " b'S015C002P016R002A028': b'S016C002P007R002A044',\n",
       " b'S002C001P008R002A033': b'S017C002P015R002A002',\n",
       " b'S006C001P017R002A031': b'S009C002P019R001A039',\n",
       " b'S007C002P008R002A011': b'S011C003P007R002A045',\n",
       " b'S015C002P008R001A028': b'S004C003P007R001A036',\n",
       " b'S008C003P025R001A021': b'S001C002P007R001A009',\n",
       " b'S011C003P015R001A025': b'S003C003P019R001A038'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_to_dummy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
