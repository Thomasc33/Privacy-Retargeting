{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/thomas/Downloads/nturgb+d_skeletons'\n",
    "path = 'D:\\\\Datasets\\\\Motion Privacy\\\\NTU RGB+D 120\\\\Skeleton Data'\n",
    "ntu_120 = False\n",
    "is_privacy = False\n",
    "if is_privacy: num_classes = 106 if ntu_120 else 40\n",
    "else: num_classes = 120 if ntu_120 else 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../NTU/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ntu_120:\n",
    "    to_del = [] \n",
    "    for key in X:\n",
    "        if int(key[17:20]) > 60:\n",
    "            to_del.append(key)\n",
    "    for key in to_del:\n",
    "        del X[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_optimized():\n",
    "    # Preallocate memory for x and y, assuming we know the length of X\n",
    "    x = np.zeros((len(X), 300, 150), dtype=np.float32)  # 300 timesteps, 150 features (75 * 2 skeletons)\n",
    "    y = np.zeros((len(X), num_classes), dtype=np.float32)\n",
    "\n",
    "    for i, (file, data) in enumerate(X.items()):\n",
    "        # Get the y index\n",
    "        if is_privacy: y_idx = int(file[9:12]) - 1\n",
    "        else: y_idx = int(file[17:20]) - 1\n",
    "        \n",
    "        # Set the correct class index\n",
    "        y[i, y_idx] = 1\n",
    "        \n",
    "        # Process the data\n",
    "        data = data[:, :, :3]  # Assuming data is a 3D array\n",
    "        num_timesteps = min(data.shape[0], 300)\n",
    "        \n",
    "        # Assign the data to the preallocated array (flattening last dimensions)\n",
    "        x[i, :num_timesteps, :75] = data[:num_timesteps].reshape(num_timesteps, -1)\n",
    "        # The second skeleton remains zero due to preallocation\n",
    "\n",
    "    return x, y\n",
    "\n",
    "X_, Y_ = process_data_optimized()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_, Y_, test_size=0.3, random_state=42)\n",
    "# Split into validation and test\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=0.5, random_state=42)\n",
    "\n",
    "del X_, Y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGN\n",
    "\n",
    "All code in this section is adapted from Microsoft's SGN. [Github](https://github.com/microsoft/SGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from model import SGN\n",
    "from data import NTUDataLoaders, AverageMeter\n",
    "from util import make_dir, get_num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters/Tuning Parameters\n",
    "network='SGN'\n",
    "dataset='NTU'\n",
    "start_epoch=0\n",
    "case= 0 if is_privacy else 1 # 0 = privacy, 1 = Action\n",
    "batch_size=64\n",
    "max_epochs=120\n",
    "monitor='val_acc'\n",
    "lr=0.001\n",
    "weight_decay=0.0001\n",
    "lr_factor=0.1\n",
    "workers=16\n",
    "print_freq = 20\n",
    "do_train=1\n",
    "seg=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "\n",
    "        output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # clear gradients out before each mini-batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_freq == 0:\n",
    "            print('Epoch-{:<3d} {:3d} batches\\t'\n",
    "                  'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'accu {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                      epoch + 1, i + 1, loss=losses, acc=acces))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def test(test_loader, model, checkpoint, lable_path, pred_path):\n",
    "    acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.load_state_dict(torch.load(checkpoint)['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    np.savetxt(lable_path, label_output, fmt='%d')\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "    np.savetxt(pred_path, pred_output, fmt='%f')\n",
    "\n",
    "    print('Test: accuracy {:.3f}, time: {:.2f}s'\n",
    "          .format(acces.avg, time.time() - t_start))\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters:  691048\n",
      "The modes is: SGN\n",
      "It is using GPU!\n",
      "Train on 31742 samples, validate on 6802 samples\n",
      "0 0.001\n",
      "Epoch-1    20 batches\tloss 3.8490 (4.1781)\taccu 7.812 (4.062)\n",
      "Epoch-1    40 batches\tloss 3.6485 (3.9858)\taccu 10.938 (6.211)\n",
      "Epoch-1    60 batches\tloss 3.4706 (3.8575)\taccu 15.625 (8.125)\n",
      "Epoch-1    80 batches\tloss 3.3913 (3.7393)\taccu 14.062 (10.117)\n",
      "Epoch-1   100 batches\tloss 3.1283 (3.6242)\taccu 26.562 (12.531)\n",
      "Epoch-1   120 batches\tloss 2.8137 (3.5167)\taccu 23.438 (14.492)\n",
      "Epoch-1   140 batches\tloss 2.8730 (3.4351)\taccu 34.375 (16.317)\n",
      "Epoch-1   160 batches\tloss 2.8446 (3.3557)\taccu 29.688 (18.467)\n",
      "Epoch-1   180 batches\tloss 2.6352 (3.2836)\taccu 31.250 (20.399)\n",
      "Epoch-1   200 batches\tloss 2.7443 (3.2234)\taccu 28.125 (21.898)\n",
      "Epoch-1   220 batches\tloss 2.4158 (3.1614)\taccu 37.500 (23.473)\n",
      "Epoch-1   240 batches\tloss 2.2927 (3.1091)\taccu 48.438 (24.844)\n",
      "Epoch-1   260 batches\tloss 2.6028 (3.0638)\taccu 42.188 (26.010)\n",
      "Epoch-1   280 batches\tloss 2.5930 (3.0187)\taccu 40.625 (27.232)\n",
      "Epoch-1   300 batches\tloss 2.2391 (2.9824)\taccu 45.312 (28.094)\n",
      "Epoch-1   320 batches\tloss 2.3059 (2.9467)\taccu 51.562 (29.043)\n",
      "Epoch-1   340 batches\tloss 2.3523 (2.9108)\taccu 51.562 (30.060)\n",
      "Epoch-1   360 batches\tloss 2.3141 (2.8762)\taccu 42.188 (31.003)\n",
      "Epoch-1   380 batches\tloss 2.0788 (2.8446)\taccu 48.438 (31.928)\n",
      "Epoch-1   400 batches\tloss 2.4242 (2.8133)\taccu 46.875 (32.902)\n",
      "Epoch-1   420 batches\tloss 2.1097 (2.7830)\taccu 53.125 (33.761)\n",
      "Epoch-1   440 batches\tloss 2.2936 (2.7542)\taccu 45.312 (34.581)\n",
      "Epoch-1   460 batches\tloss 1.9393 (2.7291)\taccu 57.812 (35.316)\n",
      "Epoch-1   480 batches\tloss 2.1291 (2.7032)\taccu 56.250 (36.120)\n",
      "Epoch-1   112.6s\tTrain: loss 2.6862\taccu 36.6193\tValid: loss 2.0745\taccu 56.0879\n",
      "Epoch 1: val_acc improved from -inf to 56.0879, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "1 0.001\n",
      "Epoch-2    20 batches\tloss 2.3678 (2.0686)\taccu 43.750 (57.266)\n",
      "Epoch-2    40 batches\tloss 2.0793 (2.0379)\taccu 56.250 (57.539)\n",
      "Epoch-2    60 batches\tloss 1.9937 (2.0401)\taccu 51.562 (56.667)\n",
      "Epoch-2    80 batches\tloss 1.7962 (2.0311)\taccu 62.500 (56.914)\n",
      "Epoch-2   100 batches\tloss 2.0323 (2.0255)\taccu 64.062 (57.344)\n",
      "Epoch-2   120 batches\tloss 1.9762 (2.0305)\taccu 59.375 (57.305)\n",
      "Epoch-2   140 batches\tloss 2.1205 (2.0306)\taccu 60.938 (57.366)\n",
      "Epoch-2   160 batches\tloss 1.8291 (2.0186)\taccu 62.500 (57.588)\n",
      "Epoch-2   180 batches\tloss 2.2924 (2.0178)\taccu 54.688 (57.656)\n",
      "Epoch-2   200 batches\tloss 1.9449 (2.0138)\taccu 53.125 (57.828)\n",
      "Epoch-2   220 batches\tloss 1.7880 (2.0056)\taccu 64.062 (58.011)\n",
      "Epoch-2   240 batches\tloss 2.1393 (1.9969)\taccu 54.688 (58.125)\n",
      "Epoch-2   260 batches\tloss 1.8183 (1.9903)\taccu 59.375 (58.263)\n",
      "Epoch-2   280 batches\tloss 1.8016 (1.9852)\taccu 59.375 (58.521)\n",
      "Epoch-2   300 batches\tloss 1.9343 (1.9815)\taccu 60.938 (58.609)\n",
      "Epoch-2   320 batches\tloss 1.7172 (1.9725)\taccu 75.000 (59.014)\n",
      "Epoch-2   340 batches\tloss 1.8210 (1.9649)\taccu 56.250 (59.329)\n",
      "Epoch-2   360 batches\tloss 2.0938 (1.9602)\taccu 54.688 (59.540)\n",
      "Epoch-2   380 batches\tloss 1.7016 (1.9574)\taccu 62.500 (59.646)\n",
      "Epoch-2   400 batches\tloss 1.7128 (1.9517)\taccu 70.312 (59.871)\n",
      "Epoch-2   420 batches\tloss 1.9093 (1.9474)\taccu 70.312 (60.063)\n",
      "Epoch-2   440 batches\tloss 1.7368 (1.9427)\taccu 70.312 (60.206)\n",
      "Epoch-2   460 batches\tloss 1.8171 (1.9369)\taccu 76.562 (60.394)\n",
      "Epoch-2   480 batches\tloss 1.7900 (1.9314)\taccu 62.500 (60.635)\n",
      "Epoch-2   110.3s\tTrain: loss 1.9293\taccu 60.7197\tValid: loss 1.7848\taccu 66.4062\n",
      "Epoch 2: val_acc improved from 56.0879 to 66.4062, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "2 0.001\n",
      "Epoch-3    20 batches\tloss 1.8712 (1.7591)\taccu 62.500 (67.266)\n",
      "Epoch-3    40 batches\tloss 1.7043 (1.7599)\taccu 64.062 (66.602)\n",
      "Epoch-3    60 batches\tloss 1.7886 (1.7778)\taccu 59.375 (65.859)\n",
      "Epoch-3    80 batches\tloss 1.6273 (1.7669)\taccu 75.000 (65.898)\n",
      "Epoch-3   100 batches\tloss 2.0476 (1.7651)\taccu 59.375 (65.922)\n",
      "Epoch-3   120 batches\tloss 2.1119 (1.7734)\taccu 57.812 (65.924)\n",
      "Epoch-3   140 batches\tloss 1.7681 (1.7704)\taccu 67.188 (65.859)\n",
      "Epoch-3   160 batches\tloss 2.1178 (1.7705)\taccu 57.812 (66.104)\n",
      "Epoch-3   180 batches\tloss 1.8404 (1.7683)\taccu 67.188 (66.267)\n",
      "Epoch-3   200 batches\tloss 1.7822 (1.7634)\taccu 62.500 (66.438)\n",
      "Epoch-3   220 batches\tloss 1.6889 (1.7575)\taccu 67.188 (66.747)\n",
      "Epoch-3   240 batches\tloss 1.6602 (1.7573)\taccu 67.188 (66.745)\n",
      "Epoch-3   260 batches\tloss 1.8018 (1.7581)\taccu 60.938 (66.725)\n",
      "Epoch-3   280 batches\tloss 1.6180 (1.7589)\taccu 78.125 (66.741)\n",
      "Epoch-3   300 batches\tloss 1.8079 (1.7547)\taccu 70.312 (66.885)\n",
      "Epoch-3   320 batches\tloss 1.6134 (1.7497)\taccu 68.750 (67.080)\n",
      "Epoch-3   340 batches\tloss 1.3787 (1.7443)\taccu 79.688 (67.160)\n",
      "Epoch-3   360 batches\tloss 1.5368 (1.7407)\taccu 75.000 (67.296)\n",
      "Epoch-3   380 batches\tloss 1.5843 (1.7385)\taccu 70.312 (67.356)\n",
      "Epoch-3   400 batches\tloss 1.6230 (1.7392)\taccu 70.312 (67.375)\n",
      "Epoch-3   420 batches\tloss 1.6986 (1.7364)\taccu 70.312 (67.481)\n",
      "Epoch-3   440 batches\tloss 1.6591 (1.7367)\taccu 71.875 (67.585)\n",
      "Epoch-3   460 batches\tloss 1.9297 (1.7360)\taccu 62.500 (67.605)\n",
      "Epoch-3   480 batches\tloss 1.5000 (1.7334)\taccu 79.688 (67.705)\n",
      "Epoch-3   149.2s\tTrain: loss 1.7309\taccu 67.7652\tValid: loss 1.6791\taccu 69.8850\n",
      "Epoch 3: val_acc improved from 66.4062 to 69.8850, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "3 0.001\n",
      "Epoch-4    20 batches\tloss 1.6232 (1.6451)\taccu 81.250 (71.172)\n",
      "Epoch-4    40 batches\tloss 1.5566 (1.6201)\taccu 76.562 (71.602)\n",
      "Epoch-4    60 batches\tloss 1.5544 (1.6196)\taccu 75.000 (71.849)\n",
      "Epoch-4    80 batches\tloss 1.5137 (1.6328)\taccu 71.875 (70.977)\n",
      "Epoch-4   100 batches\tloss 1.5842 (1.6238)\taccu 75.000 (71.516)\n",
      "Epoch-4   120 batches\tloss 1.8023 (1.6260)\taccu 70.312 (71.276)\n",
      "Epoch-4   140 batches\tloss 1.5363 (1.6304)\taccu 70.312 (71.205)\n",
      "Epoch-4   160 batches\tloss 1.6047 (1.6255)\taccu 73.438 (71.416)\n",
      "Epoch-4   180 batches\tloss 1.5951 (1.6267)\taccu 81.250 (71.493)\n",
      "Epoch-4   200 batches\tloss 1.7854 (1.6278)\taccu 62.500 (71.609)\n",
      "Epoch-4   220 batches\tloss 1.6081 (1.6299)\taccu 65.625 (71.491)\n",
      "Epoch-4   240 batches\tloss 1.6230 (1.6321)\taccu 75.000 (71.367)\n",
      "Epoch-4   260 batches\tloss 1.5398 (1.6318)\taccu 78.125 (71.442)\n",
      "Epoch-4   280 batches\tloss 1.9172 (1.6310)\taccu 56.250 (71.501)\n",
      "Epoch-4   300 batches\tloss 1.9037 (1.6336)\taccu 60.938 (71.385)\n",
      "Epoch-4   320 batches\tloss 1.6100 (1.6312)\taccu 67.188 (71.396)\n",
      "Epoch-4   340 batches\tloss 1.4760 (1.6317)\taccu 76.562 (71.452)\n",
      "Epoch-4   360 batches\tloss 1.7715 (1.6336)\taccu 70.312 (71.415)\n",
      "Epoch-4   380 batches\tloss 1.4445 (1.6334)\taccu 79.688 (71.419)\n",
      "Epoch-4   400 batches\tloss 1.6263 (1.6314)\taccu 75.000 (71.512)\n",
      "Epoch-4   420 batches\tloss 1.5536 (1.6314)\taccu 78.125 (71.496)\n",
      "Epoch-4   440 batches\tloss 1.5245 (1.6325)\taccu 76.562 (71.463)\n",
      "Epoch-4   460 batches\tloss 1.6207 (1.6324)\taccu 68.750 (71.454)\n",
      "Epoch-4   480 batches\tloss 1.5428 (1.6308)\taccu 73.438 (71.517)\n",
      "Epoch-4   127.1s\tTrain: loss 1.6312\taccu 71.5278\tValid: loss 1.5832\taccu 73.0248\n",
      "Epoch 4: val_acc improved from 69.8850 to 73.0248, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "4 0.001\n",
      "Epoch-5    20 batches\tloss 1.3998 (1.5736)\taccu 73.438 (72.656)\n",
      "Epoch-5    40 batches\tloss 1.6041 (1.5567)\taccu 75.000 (74.141)\n",
      "Epoch-5    60 batches\tloss 1.5619 (1.5655)\taccu 82.812 (73.411)\n",
      "Epoch-5    80 batches\tloss 1.5019 (1.5545)\taccu 73.438 (73.906)\n",
      "Epoch-5   100 batches\tloss 1.5266 (1.5494)\taccu 76.562 (74.172)\n",
      "Epoch-5   120 batches\tloss 1.7100 (1.5624)\taccu 73.438 (73.867)\n",
      "Epoch-5   140 batches\tloss 1.4604 (1.5682)\taccu 75.000 (73.516)\n",
      "Epoch-5   160 batches\tloss 1.5211 (1.5669)\taccu 78.125 (73.623)\n",
      "Epoch-5   180 batches\tloss 1.4478 (1.5623)\taccu 73.438 (73.767)\n",
      "Epoch-5   200 batches\tloss 1.6150 (1.5677)\taccu 73.438 (73.594)\n",
      "Epoch-5   220 batches\tloss 1.7914 (1.5699)\taccu 65.625 (73.530)\n",
      "Epoch-5   240 batches\tloss 1.3212 (1.5686)\taccu 76.562 (73.535)\n",
      "Epoch-5   260 batches\tloss 1.4410 (1.5705)\taccu 79.688 (73.582)\n",
      "Epoch-5   280 batches\tloss 1.5949 (1.5712)\taccu 65.625 (73.521)\n",
      "Epoch-5   300 batches\tloss 1.4975 (1.5676)\taccu 71.875 (73.604)\n",
      "Epoch-5   320 batches\tloss 1.6352 (1.5684)\taccu 70.312 (73.643)\n",
      "Epoch-5   340 batches\tloss 1.4756 (1.5688)\taccu 76.562 (73.617)\n",
      "Epoch-5   360 batches\tloss 1.4782 (1.5696)\taccu 79.688 (73.594)\n",
      "Epoch-5   380 batches\tloss 1.6179 (1.5696)\taccu 73.438 (73.577)\n",
      "Epoch-5   400 batches\tloss 1.5817 (1.5690)\taccu 73.438 (73.617)\n",
      "Epoch-5   420 batches\tloss 1.4888 (1.5665)\taccu 75.000 (73.690)\n",
      "Epoch-5   440 batches\tloss 1.4451 (1.5660)\taccu 79.688 (73.683)\n",
      "Epoch-5   460 batches\tloss 1.5640 (1.5655)\taccu 75.000 (73.740)\n",
      "Epoch-5   480 batches\tloss 1.5637 (1.5647)\taccu 71.875 (73.763)\n",
      "Epoch-5   120.4s\tTrain: loss 1.5645\taccu 73.7847\tValid: loss 1.5448\taccu 74.5136\n",
      "Epoch 5: val_acc improved from 73.0248 to 74.5136, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "5 0.001\n",
      "Epoch-6    20 batches\tloss 1.4374 (1.4861)\taccu 79.688 (77.188)\n",
      "Epoch-6    40 batches\tloss 1.7206 (1.4971)\taccu 67.188 (76.484)\n",
      "Epoch-6    60 batches\tloss 1.6228 (1.5037)\taccu 68.750 (75.885)\n",
      "Epoch-6    80 batches\tloss 1.5407 (1.5074)\taccu 73.438 (75.762)\n",
      "Epoch-6   100 batches\tloss 1.4007 (1.5099)\taccu 76.562 (75.828)\n",
      "Epoch-6   120 batches\tloss 1.3898 (1.5065)\taccu 82.812 (76.094)\n",
      "Epoch-6   140 batches\tloss 1.3688 (1.5073)\taccu 81.250 (75.926)\n",
      "Epoch-6   160 batches\tloss 1.4442 (1.5057)\taccu 70.312 (76.016)\n",
      "Epoch-6   180 batches\tloss 1.5971 (1.5084)\taccu 75.000 (75.903)\n",
      "Epoch-6   200 batches\tloss 1.6419 (1.5086)\taccu 71.875 (76.023)\n",
      "Epoch-6   220 batches\tloss 1.2941 (1.5073)\taccu 82.812 (76.051)\n",
      "Epoch-6   240 batches\tloss 1.4793 (1.5102)\taccu 76.562 (75.970)\n",
      "Epoch-6   260 batches\tloss 1.5995 (1.5126)\taccu 78.125 (75.889)\n",
      "Epoch-6   280 batches\tloss 1.3985 (1.5100)\taccu 76.562 (75.971)\n",
      "Epoch-6   300 batches\tloss 1.2552 (1.5081)\taccu 84.375 (76.026)\n",
      "Epoch-6   320 batches\tloss 1.4800 (1.5075)\taccu 76.562 (76.035)\n",
      "Epoch-6   340 batches\tloss 1.4769 (1.5062)\taccu 76.562 (76.135)\n",
      "Epoch-6   360 batches\tloss 1.5447 (1.5054)\taccu 71.875 (76.124)\n",
      "Epoch-6   380 batches\tloss 1.5648 (1.5039)\taccu 76.562 (76.151)\n",
      "Epoch-6   400 batches\tloss 1.4957 (1.5072)\taccu 79.688 (76.020)\n",
      "Epoch-6   420 batches\tloss 1.5259 (1.5065)\taccu 73.438 (76.034)\n",
      "Epoch-6   440 batches\tloss 1.4950 (1.5064)\taccu 78.125 (76.030)\n",
      "Epoch-6   460 batches\tloss 1.6166 (1.5073)\taccu 75.000 (75.965)\n",
      "Epoch-6   480 batches\tloss 1.4312 (1.5075)\taccu 81.250 (75.931)\n",
      "Epoch-6   127.1s\tTrain: loss 1.5061\taccu 75.9880\tValid: loss 1.5055\taccu 75.3685\n",
      "Epoch 6: val_acc improved from 74.5136 to 75.3685, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "6 0.001\n",
      "Epoch-7    20 batches\tloss 1.4107 (1.4653)\taccu 81.250 (76.875)\n",
      "Epoch-7    40 batches\tloss 1.3928 (1.4474)\taccu 82.812 (78.164)\n",
      "Epoch-7    60 batches\tloss 1.4423 (1.4518)\taccu 76.562 (78.099)\n",
      "Epoch-7    80 batches\tloss 1.5152 (1.4566)\taccu 73.438 (78.242)\n",
      "Epoch-7   100 batches\tloss 1.5706 (1.4625)\taccu 70.312 (77.656)\n",
      "Epoch-7   120 batches\tloss 1.6404 (1.4640)\taccu 71.875 (77.786)\n",
      "Epoch-7   140 batches\tloss 1.4765 (1.4701)\taccu 75.000 (77.634)\n",
      "Epoch-7   160 batches\tloss 1.5556 (1.4636)\taccu 78.125 (78.086)\n",
      "Epoch-7   180 batches\tloss 1.4114 (1.4669)\taccu 81.250 (77.969)\n",
      "Epoch-7   200 batches\tloss 1.3905 (1.4650)\taccu 85.938 (77.961)\n",
      "Epoch-7   220 batches\tloss 1.5144 (1.4655)\taccu 76.562 (77.947)\n",
      "Epoch-7   240 batches\tloss 1.3732 (1.4637)\taccu 82.812 (77.871)\n",
      "Epoch-7   260 batches\tloss 1.3649 (1.4648)\taccu 82.812 (77.861)\n",
      "Epoch-7   280 batches\tloss 1.4463 (1.4643)\taccu 73.438 (77.812)\n",
      "Epoch-7   300 batches\tloss 1.3857 (1.4628)\taccu 79.688 (77.906)\n",
      "Epoch-7   320 batches\tloss 1.4612 (1.4645)\taccu 82.812 (77.778)\n",
      "Epoch-7   340 batches\tloss 1.3528 (1.4665)\taccu 78.125 (77.707)\n",
      "Epoch-7   360 batches\tloss 1.4193 (1.4686)\taccu 82.812 (77.648)\n",
      "Epoch-7   380 batches\tloss 1.3511 (1.4695)\taccu 79.688 (77.553)\n",
      "Epoch-7   400 batches\tloss 1.3614 (1.4667)\taccu 82.812 (77.652)\n",
      "Epoch-7   420 batches\tloss 1.5952 (1.4685)\taccu 71.875 (77.634)\n",
      "Epoch-7   440 batches\tloss 1.3978 (1.4667)\taccu 79.688 (77.724)\n",
      "Epoch-7   460 batches\tloss 1.3827 (1.4651)\taccu 82.812 (77.775)\n",
      "Epoch-7   480 batches\tloss 1.4337 (1.4649)\taccu 78.125 (77.757)\n",
      "Epoch-7   134.6s\tTrain: loss 1.4657\taccu 77.7431\tValid: loss 1.4998\taccu 76.0908\n",
      "Epoch 7: val_acc improved from 75.3685 to 76.0908, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "7 0.001\n",
      "Epoch-8    20 batches\tloss 1.2409 (1.3966)\taccu 82.812 (79.688)\n",
      "Epoch-8    40 batches\tloss 1.1846 (1.3881)\taccu 90.625 (80.664)\n",
      "Epoch-8    60 batches\tloss 1.4223 (1.3800)\taccu 79.688 (80.651)\n",
      "Epoch-8    80 batches\tloss 1.4940 (1.3932)\taccu 73.438 (80.215)\n",
      "Epoch-8   100 batches\tloss 1.3299 (1.3954)\taccu 85.938 (80.141)\n",
      "Epoch-8   120 batches\tloss 1.2606 (1.4023)\taccu 87.500 (79.909)\n",
      "Epoch-8   140 batches\tloss 1.1885 (1.4022)\taccu 85.938 (79.844)\n",
      "Epoch-8   160 batches\tloss 1.3051 (1.4025)\taccu 84.375 (80.000)\n",
      "Epoch-8   180 batches\tloss 1.3399 (1.4059)\taccu 82.812 (79.922)\n",
      "Epoch-8   200 batches\tloss 1.5188 (1.4094)\taccu 71.875 (79.750)\n",
      "Epoch-8   220 batches\tloss 1.4286 (1.4121)\taccu 76.562 (79.666)\n",
      "Epoch-8   240 batches\tloss 1.4709 (1.4150)\taccu 81.250 (79.655)\n",
      "Epoch-8   260 batches\tloss 1.4752 (1.4156)\taccu 81.250 (79.712)\n",
      "Epoch-8   280 batches\tloss 1.4067 (1.4197)\taccu 75.000 (79.542)\n",
      "Epoch-8   300 batches\tloss 1.3636 (1.4207)\taccu 81.250 (79.443)\n",
      "Epoch-8   320 batches\tloss 1.3108 (1.4199)\taccu 85.938 (79.521)\n",
      "Epoch-8   340 batches\tloss 1.3410 (1.4173)\taccu 82.812 (79.605)\n",
      "Epoch-8   360 batches\tloss 1.3846 (1.4169)\taccu 76.562 (79.605)\n",
      "Epoch-8   380 batches\tloss 1.2624 (1.4165)\taccu 85.938 (79.630)\n",
      "Epoch-8   400 batches\tloss 1.4370 (1.4177)\taccu 68.750 (79.512)\n",
      "Epoch-8   420 batches\tloss 1.3407 (1.4185)\taccu 85.938 (79.516)\n",
      "Epoch-8   440 batches\tloss 1.2811 (1.4193)\taccu 82.812 (79.457)\n",
      "Epoch-8   460 batches\tloss 1.5995 (1.4205)\taccu 73.438 (79.365)\n",
      "Epoch-8   480 batches\tloss 1.4937 (1.4209)\taccu 78.125 (79.365)\n",
      "Epoch-8   135.4s\tTrain: loss 1.4220\taccu 79.3182\tValid: loss 1.4408\taccu 79.0537\n",
      "Epoch 8: val_acc improved from 76.0908 to 79.0537, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "8 0.001\n",
      "Epoch-9    20 batches\tloss 1.5652 (1.3621)\taccu 73.438 (82.734)\n",
      "Epoch-9    40 batches\tloss 1.5326 (1.3648)\taccu 73.438 (82.344)\n",
      "Epoch-9    60 batches\tloss 1.3934 (1.3700)\taccu 81.250 (82.292)\n",
      "Epoch-9    80 batches\tloss 1.4658 (1.3637)\taccu 75.000 (82.012)\n",
      "Epoch-9   100 batches\tloss 1.5659 (1.3665)\taccu 75.000 (81.875)\n",
      "Epoch-9   120 batches\tloss 1.3751 (1.3736)\taccu 79.688 (81.510)\n",
      "Epoch-9   140 batches\tloss 1.3576 (1.3798)\taccu 76.562 (81.049)\n",
      "Epoch-9   160 batches\tloss 1.4622 (1.3830)\taccu 71.875 (80.938)\n",
      "Epoch-9   180 batches\tloss 1.6431 (1.3836)\taccu 70.312 (80.885)\n",
      "Epoch-9   200 batches\tloss 1.3928 (1.3844)\taccu 78.125 (80.875)\n",
      "Epoch-9   220 batches\tloss 1.3577 (1.3839)\taccu 82.812 (80.810)\n",
      "Epoch-9   240 batches\tloss 1.3668 (1.3853)\taccu 78.125 (80.710)\n",
      "Epoch-9   260 batches\tloss 1.5481 (1.3880)\taccu 78.125 (80.673)\n",
      "Epoch-9   280 batches\tloss 1.2917 (1.3880)\taccu 82.812 (80.670)\n",
      "Epoch-9   300 batches\tloss 1.3481 (1.3882)\taccu 79.688 (80.698)\n",
      "Epoch-9   320 batches\tloss 1.3184 (1.3884)\taccu 81.250 (80.708)\n",
      "Epoch-9   340 batches\tloss 1.3833 (1.3889)\taccu 82.812 (80.708)\n",
      "Epoch-9   360 batches\tloss 1.3534 (1.3865)\taccu 82.812 (80.794)\n",
      "Epoch-9   380 batches\tloss 1.4361 (1.3877)\taccu 82.812 (80.728)\n",
      "Epoch-9   400 batches\tloss 1.3019 (1.3887)\taccu 87.500 (80.660)\n",
      "Epoch-9   420 batches\tloss 1.3788 (1.3906)\taccu 82.812 (80.618)\n",
      "Epoch-9   440 batches\tloss 1.3330 (1.3912)\taccu 81.250 (80.589)\n",
      "Epoch-9   460 batches\tloss 1.4092 (1.3900)\taccu 81.250 (80.642)\n",
      "Epoch-9   480 batches\tloss 1.3083 (1.3934)\taccu 81.250 (80.573)\n",
      "Epoch-9   131.4s\tTrain: loss 1.3942\taccu 80.5429\tValid: loss 1.4557\taccu 78.2577\n",
      "Epoch 9: val_acc did not improve\n",
      "9 0.001\n",
      "Epoch-10   20 batches\tloss 1.4628 (1.3769)\taccu 78.125 (81.562)\n",
      "Epoch-10   40 batches\tloss 1.3700 (1.3633)\taccu 76.562 (81.953)\n",
      "Epoch-10   60 batches\tloss 1.5455 (1.3589)\taccu 68.750 (81.901)\n",
      "Epoch-10   80 batches\tloss 1.2172 (1.3566)\taccu 85.938 (81.562)\n",
      "Epoch-10  100 batches\tloss 1.4233 (1.3545)\taccu 78.125 (81.562)\n",
      "Epoch-10  120 batches\tloss 1.5498 (1.3599)\taccu 73.438 (81.549)\n",
      "Epoch-10  140 batches\tloss 1.3476 (1.3582)\taccu 81.250 (81.607)\n",
      "Epoch-10  160 batches\tloss 1.2961 (1.3605)\taccu 84.375 (81.777)\n",
      "Epoch-10  180 batches\tloss 1.4399 (1.3593)\taccu 76.562 (81.832)\n",
      "Epoch-10  200 batches\tloss 1.2262 (1.3591)\taccu 89.062 (81.828)\n",
      "Epoch-10  220 batches\tloss 1.2738 (1.3609)\taccu 87.500 (81.676)\n",
      "Epoch-10  240 batches\tloss 1.3025 (1.3647)\taccu 82.812 (81.576)\n",
      "Epoch-10  260 batches\tloss 1.4282 (1.3651)\taccu 78.125 (81.520)\n",
      "Epoch-10  280 batches\tloss 1.1733 (1.3637)\taccu 90.625 (81.551)\n",
      "Epoch-10  300 batches\tloss 1.4323 (1.3652)\taccu 81.250 (81.531)\n",
      "Epoch-10  320 batches\tloss 1.3785 (1.3678)\taccu 84.375 (81.465)\n",
      "Epoch-10  340 batches\tloss 1.4846 (1.3674)\taccu 68.750 (81.503)\n",
      "Epoch-10  360 batches\tloss 1.2897 (1.3661)\taccu 82.812 (81.549)\n",
      "Epoch-10  380 batches\tloss 1.3861 (1.3664)\taccu 71.875 (81.493)\n",
      "Epoch-10  400 batches\tloss 1.5778 (1.3662)\taccu 76.562 (81.504)\n",
      "Epoch-10  420 batches\tloss 1.4343 (1.3662)\taccu 79.688 (81.473)\n",
      "Epoch-10  440 batches\tloss 1.5203 (1.3675)\taccu 76.562 (81.431)\n",
      "Epoch-10  460 batches\tloss 1.3021 (1.3678)\taccu 79.688 (81.386)\n",
      "Epoch-10  480 batches\tloss 1.2894 (1.3663)\taccu 87.500 (81.436)\n",
      "Epoch-10  131.6s\tTrain: loss 1.3672\taccu 81.4205\tValid: loss 1.4359\taccu 78.6114\n",
      "Epoch 10: val_acc did not improve\n",
      "10 0.001\n",
      "Epoch-11   20 batches\tloss 1.1181 (1.3093)\taccu 90.625 (83.672)\n",
      "Epoch-11   40 batches\tloss 1.2732 (1.3280)\taccu 87.500 (83.125)\n",
      "Epoch-11   60 batches\tloss 1.3557 (1.3251)\taccu 76.562 (83.203)\n",
      "Epoch-11   80 batches\tloss 1.4632 (1.3354)\taccu 76.562 (82.676)\n",
      "Epoch-11  100 batches\tloss 1.5022 (1.3394)\taccu 70.312 (82.281)\n",
      "Epoch-11  120 batches\tloss 1.3190 (1.3368)\taccu 82.812 (82.318)\n",
      "Epoch-11  140 batches\tloss 1.2574 (1.3340)\taccu 81.250 (82.344)\n",
      "Epoch-11  160 batches\tloss 1.3829 (1.3340)\taccu 76.562 (82.363)\n",
      "Epoch-11  180 batches\tloss 1.4019 (1.3392)\taccu 81.250 (82.170)\n",
      "Epoch-11  200 batches\tloss 1.5694 (1.3390)\taccu 75.000 (82.242)\n",
      "Epoch-11  220 batches\tloss 1.5103 (1.3387)\taccu 76.562 (82.251)\n",
      "Epoch-11  240 batches\tloss 1.2364 (1.3389)\taccu 84.375 (82.233)\n",
      "Epoch-11  260 batches\tloss 1.3571 (1.3379)\taccu 78.125 (82.254)\n",
      "Epoch-11  280 batches\tloss 1.3232 (1.3406)\taccu 82.812 (82.126)\n",
      "Epoch-11  300 batches\tloss 1.3764 (1.3413)\taccu 84.375 (82.125)\n",
      "Epoch-11  320 batches\tloss 1.4100 (1.3428)\taccu 78.125 (82.114)\n",
      "Epoch-11  340 batches\tloss 1.2972 (1.3424)\taccu 84.375 (82.155)\n",
      "Epoch-11  360 batches\tloss 1.3558 (1.3450)\taccu 82.812 (82.092)\n",
      "Epoch-11  380 batches\tloss 1.3163 (1.3452)\taccu 84.375 (82.146)\n",
      "Epoch-11  400 batches\tloss 1.3516 (1.3456)\taccu 84.375 (82.113)\n",
      "Epoch-11  420 batches\tloss 1.2566 (1.3448)\taccu 82.812 (82.161)\n",
      "Epoch-11  440 batches\tloss 1.2333 (1.3458)\taccu 85.938 (82.159)\n",
      "Epoch-11  460 batches\tloss 1.2946 (1.3465)\taccu 79.688 (82.075)\n",
      "Epoch-11  480 batches\tloss 1.2977 (1.3461)\taccu 85.938 (82.096)\n",
      "Epoch-11  130.7s\tTrain: loss 1.3468\taccu 82.0297\tValid: loss 1.4119\taccu 80.0413\n",
      "Epoch 11: val_acc improved from 79.0537 to 80.0413, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "11 0.001\n",
      "Epoch-12   20 batches\tloss 1.2502 (1.2873)\taccu 87.500 (85.547)\n",
      "Epoch-12   40 batches\tloss 1.2203 (1.3038)\taccu 89.062 (84.453)\n",
      "Epoch-12   60 batches\tloss 1.3408 (1.3195)\taccu 82.812 (83.776)\n",
      "Epoch-12   80 batches\tloss 1.3438 (1.3131)\taccu 81.250 (84.238)\n",
      "Epoch-12  100 batches\tloss 1.4031 (1.3123)\taccu 84.375 (84.094)\n",
      "Epoch-12  120 batches\tloss 1.0951 (1.3119)\taccu 90.625 (84.128)\n",
      "Epoch-12  140 batches\tloss 1.3644 (1.3156)\taccu 78.125 (83.929)\n",
      "Epoch-12  160 batches\tloss 1.3451 (1.3174)\taccu 79.688 (83.623)\n",
      "Epoch-12  180 batches\tloss 1.2862 (1.3134)\taccu 79.688 (83.646)\n",
      "Epoch-12  200 batches\tloss 1.3453 (1.3122)\taccu 84.375 (83.602)\n",
      "Epoch-12  220 batches\tloss 1.2955 (1.3131)\taccu 81.250 (83.544)\n",
      "Epoch-12  240 batches\tloss 1.2241 (1.3158)\taccu 89.062 (83.457)\n",
      "Epoch-12  260 batches\tloss 1.2418 (1.3157)\taccu 81.250 (83.389)\n",
      "Epoch-12  280 batches\tloss 1.3819 (1.3175)\taccu 78.125 (83.326)\n",
      "Epoch-12  300 batches\tloss 1.3216 (1.3190)\taccu 85.938 (83.203)\n",
      "Epoch-12  320 batches\tloss 1.2685 (1.3220)\taccu 84.375 (83.027)\n",
      "Epoch-12  340 batches\tloss 1.1152 (1.3238)\taccu 92.188 (82.946)\n",
      "Epoch-12  360 batches\tloss 1.3943 (1.3275)\taccu 79.688 (82.769)\n",
      "Epoch-12  380 batches\tloss 1.3069 (1.3265)\taccu 81.250 (82.812)\n",
      "Epoch-12  400 batches\tloss 1.2766 (1.3258)\taccu 81.250 (82.789)\n",
      "Epoch-12  420 batches\tloss 1.1829 (1.3275)\taccu 92.188 (82.764)\n",
      "Epoch-12  440 batches\tloss 1.2298 (1.3275)\taccu 89.062 (82.759)\n",
      "Epoch-12  460 batches\tloss 1.4157 (1.3272)\taccu 78.125 (82.772)\n",
      "Epoch-12  480 batches\tloss 1.4213 (1.3289)\taccu 81.250 (82.692)\n",
      "Epoch-12  131.2s\tTrain: loss 1.3276\taccu 82.7336\tValid: loss 1.3826\taccu 80.3951\n",
      "Epoch 12: val_acc improved from 80.0413 to 80.3951, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "12 0.001\n",
      "Epoch-13   20 batches\tloss 1.2570 (1.2856)\taccu 79.688 (84.531)\n",
      "Epoch-13   40 batches\tloss 1.2513 (1.2984)\taccu 84.375 (83.750)\n",
      "Epoch-13   60 batches\tloss 1.4611 (1.2915)\taccu 78.125 (83.984)\n",
      "Epoch-13   80 batches\tloss 1.2667 (1.2867)\taccu 84.375 (84.512)\n",
      "Epoch-13  100 batches\tloss 1.1459 (1.2846)\taccu 93.750 (84.500)\n",
      "Epoch-13  120 batches\tloss 1.2405 (1.2897)\taccu 89.062 (84.154)\n",
      "Epoch-13  140 batches\tloss 1.5194 (1.2918)\taccu 75.000 (84.074)\n",
      "Epoch-13  160 batches\tloss 1.4404 (1.2967)\taccu 78.125 (83.887)\n",
      "Epoch-13  180 batches\tloss 1.4882 (1.3008)\taccu 76.562 (83.802)\n",
      "Epoch-13  200 batches\tloss 1.5674 (1.3010)\taccu 75.000 (83.766)\n",
      "Epoch-13  220 batches\tloss 1.1514 (1.3034)\taccu 89.062 (83.722)\n",
      "Epoch-13  240 batches\tloss 1.3139 (1.3036)\taccu 81.250 (83.698)\n",
      "Epoch-13  260 batches\tloss 1.3922 (1.3042)\taccu 76.562 (83.654)\n",
      "Epoch-13  280 batches\tloss 1.1937 (1.3032)\taccu 85.938 (83.689)\n",
      "Epoch-13  300 batches\tloss 1.3600 (1.3055)\taccu 81.250 (83.542)\n",
      "Epoch-13  320 batches\tloss 1.3205 (1.3047)\taccu 82.812 (83.579)\n",
      "Epoch-13  340 batches\tloss 1.3730 (1.3037)\taccu 78.125 (83.580)\n",
      "Epoch-13  360 batches\tloss 1.3075 (1.3037)\taccu 87.500 (83.581)\n",
      "Epoch-13  380 batches\tloss 1.5695 (1.3047)\taccu 70.312 (83.507)\n",
      "Epoch-13  400 batches\tloss 1.1344 (1.3041)\taccu 89.062 (83.539)\n",
      "Epoch-13  420 batches\tloss 1.2550 (1.3037)\taccu 89.062 (83.553)\n",
      "Epoch-13  440 batches\tloss 1.2969 (1.3038)\taccu 87.500 (83.565)\n",
      "Epoch-13  460 batches\tloss 1.3995 (1.3045)\taccu 81.250 (83.546)\n",
      "Epoch-13  480 batches\tloss 1.2592 (1.3042)\taccu 87.500 (83.509)\n",
      "Epoch-13  132.0s\tTrain: loss 1.3034\taccu 83.4943\tValid: loss 1.3800\taccu 80.5425\n",
      "Epoch 13: val_acc improved from 80.3951 to 80.5425, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "13 0.001\n",
      "Epoch-14   20 batches\tloss 1.1260 (1.2348)\taccu 92.188 (87.109)\n",
      "Epoch-14   40 batches\tloss 1.2851 (1.2583)\taccu 82.812 (85.352)\n",
      "Epoch-14   60 batches\tloss 1.2161 (1.2655)\taccu 90.625 (85.260)\n",
      "Epoch-14   80 batches\tloss 1.2521 (1.2620)\taccu 81.250 (85.234)\n",
      "Epoch-14  100 batches\tloss 1.3812 (1.2645)\taccu 81.250 (85.250)\n",
      "Epoch-14  120 batches\tloss 1.2844 (1.2643)\taccu 84.375 (85.169)\n",
      "Epoch-14  140 batches\tloss 1.3704 (1.2723)\taccu 81.250 (84.710)\n",
      "Epoch-14  160 batches\tloss 1.2507 (1.2744)\taccu 87.500 (84.600)\n",
      "Epoch-14  180 batches\tloss 1.2140 (1.2783)\taccu 85.938 (84.366)\n",
      "Epoch-14  200 batches\tloss 1.2816 (1.2783)\taccu 84.375 (84.352)\n",
      "Epoch-14  220 batches\tloss 1.2563 (1.2786)\taccu 82.812 (84.283)\n",
      "Epoch-14  240 batches\tloss 1.4095 (1.2798)\taccu 81.250 (84.271)\n",
      "Epoch-14  260 batches\tloss 1.3149 (1.2789)\taccu 81.250 (84.333)\n",
      "Epoch-14  280 batches\tloss 1.2268 (1.2779)\taccu 89.062 (84.369)\n",
      "Epoch-14  300 batches\tloss 1.3178 (1.2789)\taccu 81.250 (84.292)\n",
      "Epoch-14  320 batches\tloss 1.3631 (1.2788)\taccu 81.250 (84.297)\n",
      "Epoch-14  340 batches\tloss 1.3023 (1.2803)\taccu 84.375 (84.200)\n",
      "Epoch-14  360 batches\tloss 1.2074 (1.2822)\taccu 84.375 (84.158)\n",
      "Epoch-14  380 batches\tloss 1.3039 (1.2847)\taccu 81.250 (84.067)\n",
      "Epoch-14  400 batches\tloss 1.3074 (1.2867)\taccu 81.250 (84.023)\n",
      "Epoch-14  420 batches\tloss 1.5019 (1.2873)\taccu 76.562 (84.007)\n",
      "Epoch-14  440 batches\tloss 1.3465 (1.2872)\taccu 76.562 (84.077)\n",
      "Epoch-14  460 batches\tloss 1.2784 (1.2878)\taccu 85.938 (84.059)\n",
      "Epoch-14  480 batches\tloss 1.2549 (1.2887)\taccu 84.375 (84.001)\n",
      "Epoch-14  130.9s\tTrain: loss 1.2883\taccu 84.0436\tValid: loss 1.3701\taccu 80.6014\n",
      "Epoch 14: val_acc improved from 80.5425 to 80.6014, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "14 0.001\n",
      "Epoch-15   20 batches\tloss 1.2282 (1.2434)\taccu 87.500 (86.484)\n",
      "Epoch-15   40 batches\tloss 1.1432 (1.2564)\taccu 89.062 (86.094)\n",
      "Epoch-15   60 batches\tloss 1.1451 (1.2463)\taccu 95.312 (86.354)\n",
      "Epoch-15   80 batches\tloss 1.1996 (1.2438)\taccu 79.688 (86.348)\n",
      "Epoch-15  100 batches\tloss 1.2952 (1.2410)\taccu 82.812 (86.359)\n",
      "Epoch-15  120 batches\tloss 1.2497 (1.2427)\taccu 87.500 (86.393)\n",
      "Epoch-15  140 batches\tloss 1.3165 (1.2443)\taccu 84.375 (86.295)\n",
      "Epoch-15  160 batches\tloss 1.4470 (1.2484)\taccu 78.125 (86.113)\n",
      "Epoch-15  180 batches\tloss 1.1498 (1.2528)\taccu 87.500 (85.842)\n",
      "Epoch-15  200 batches\tloss 1.1634 (1.2549)\taccu 85.938 (85.727)\n",
      "Epoch-15  220 batches\tloss 1.2847 (1.2548)\taccu 82.812 (85.661)\n",
      "Epoch-15  240 batches\tloss 1.3011 (1.2547)\taccu 82.812 (85.664)\n",
      "Epoch-15  260 batches\tloss 1.3702 (1.2555)\taccu 81.250 (85.601)\n",
      "Epoch-15  280 batches\tloss 1.3287 (1.2574)\taccu 82.812 (85.452)\n",
      "Epoch-15  300 batches\tloss 1.2800 (1.2596)\taccu 82.812 (85.349)\n",
      "Epoch-15  320 batches\tloss 1.4542 (1.2618)\taccu 79.688 (85.239)\n",
      "Epoch-15  340 batches\tloss 1.1795 (1.2629)\taccu 90.625 (85.234)\n",
      "Epoch-15  360 batches\tloss 1.5064 (1.2657)\taccu 79.688 (85.104)\n",
      "Epoch-15  380 batches\tloss 1.2166 (1.2674)\taccu 87.500 (84.992)\n",
      "Epoch-15  400 batches\tloss 1.3251 (1.2688)\taccu 84.375 (84.918)\n",
      "Epoch-15  420 batches\tloss 1.3402 (1.2700)\taccu 81.250 (84.859)\n",
      "Epoch-15  440 batches\tloss 1.3163 (1.2721)\taccu 85.938 (84.748)\n",
      "Epoch-15  460 batches\tloss 1.2863 (1.2720)\taccu 81.250 (84.759)\n",
      "Epoch-15  480 batches\tloss 1.2396 (1.2730)\taccu 84.375 (84.733)\n",
      "Epoch-15  131.2s\tTrain: loss 1.2746\taccu 84.6938\tValid: loss 1.3806\taccu 80.1592\n",
      "Epoch 15: val_acc did not improve\n",
      "15 0.001\n",
      "Epoch-16   20 batches\tloss 1.2179 (1.2270)\taccu 87.500 (85.859)\n",
      "Epoch-16   40 batches\tloss 1.2286 (1.2143)\taccu 84.375 (87.305)\n",
      "Epoch-16   60 batches\tloss 1.2444 (1.2163)\taccu 85.938 (87.240)\n",
      "Epoch-16   80 batches\tloss 1.3612 (1.2286)\taccu 84.375 (86.699)\n",
      "Epoch-16  100 batches\tloss 1.2971 (1.2391)\taccu 84.375 (86.203)\n",
      "Epoch-16  120 batches\tloss 1.1767 (1.2409)\taccu 89.062 (86.367)\n",
      "Epoch-16  140 batches\tloss 1.1312 (1.2368)\taccu 89.062 (86.395)\n",
      "Epoch-16  160 batches\tloss 1.2679 (1.2401)\taccu 79.688 (86.230)\n",
      "Epoch-16  180 batches\tloss 1.2155 (1.2437)\taccu 89.062 (86.076)\n",
      "Epoch-16  200 batches\tloss 1.3319 (1.2449)\taccu 78.125 (86.039)\n",
      "Epoch-16  220 batches\tloss 1.2405 (1.2476)\taccu 84.375 (85.916)\n",
      "Epoch-16  240 batches\tloss 1.2016 (1.2478)\taccu 85.938 (85.898)\n",
      "Epoch-16  260 batches\tloss 1.2449 (1.2508)\taccu 89.062 (85.823)\n",
      "Epoch-16  280 batches\tloss 1.1205 (1.2497)\taccu 92.188 (85.915)\n",
      "Epoch-16  300 batches\tloss 1.2700 (1.2498)\taccu 84.375 (85.870)\n",
      "Epoch-16  320 batches\tloss 1.2493 (1.2510)\taccu 87.500 (85.776)\n",
      "Epoch-16  340 batches\tloss 1.2476 (1.2510)\taccu 87.500 (85.786)\n",
      "Epoch-16  360 batches\tloss 1.2473 (1.2522)\taccu 85.938 (85.673)\n",
      "Epoch-16  380 batches\tloss 1.2559 (1.2536)\taccu 82.812 (85.654)\n",
      "Epoch-16  400 batches\tloss 1.2491 (1.2535)\taccu 82.812 (85.668)\n",
      "Epoch-16  420 batches\tloss 1.1891 (1.2547)\taccu 84.375 (85.614)\n",
      "Epoch-16  440 batches\tloss 1.1853 (1.2550)\taccu 90.625 (85.629)\n",
      "Epoch-16  460 batches\tloss 1.4025 (1.2553)\taccu 79.688 (85.618)\n",
      "Epoch-16  480 batches\tloss 1.2562 (1.2551)\taccu 84.375 (85.618)\n",
      "Epoch-16  100.3s\tTrain: loss 1.2557\taccu 85.6408\tValid: loss 1.3779\taccu 80.5719\n",
      "Epoch 16: val_acc did not improve\n",
      "16 0.001\n",
      "Epoch-17   20 batches\tloss 1.1909 (1.2001)\taccu 90.625 (88.125)\n",
      "Epoch-17   40 batches\tloss 1.2077 (1.2021)\taccu 87.500 (88.203)\n",
      "Epoch-17   60 batches\tloss 1.3141 (1.2115)\taccu 82.812 (87.500)\n",
      "Epoch-17   80 batches\tloss 1.2484 (1.2176)\taccu 87.500 (87.090)\n",
      "Epoch-17  100 batches\tloss 1.1903 (1.2204)\taccu 85.938 (87.141)\n",
      "Epoch-17  120 batches\tloss 1.3287 (1.2222)\taccu 85.938 (87.279)\n",
      "Epoch-17  140 batches\tloss 1.2447 (1.2233)\taccu 87.500 (87.321)\n",
      "Epoch-17  160 batches\tloss 1.2681 (1.2332)\taccu 89.062 (86.748)\n",
      "Epoch-17  180 batches\tloss 1.2204 (1.2359)\taccu 82.812 (86.597)\n",
      "Epoch-17  200 batches\tloss 1.2470 (1.2380)\taccu 84.375 (86.484)\n",
      "Epoch-17  220 batches\tloss 1.2850 (1.2424)\taccu 85.938 (86.293)\n",
      "Epoch-17  240 batches\tloss 1.3095 (1.2422)\taccu 89.062 (86.289)\n",
      "Epoch-17  260 batches\tloss 1.2596 (1.2427)\taccu 82.812 (86.232)\n",
      "Epoch-17  280 batches\tloss 1.3263 (1.2432)\taccu 82.812 (86.122)\n",
      "Epoch-17  300 batches\tloss 1.2508 (1.2440)\taccu 89.062 (86.125)\n",
      "Epoch-17  320 batches\tloss 1.4286 (1.2463)\taccu 79.688 (86.069)\n",
      "Epoch-17  340 batches\tloss 1.3147 (1.2462)\taccu 87.500 (86.025)\n",
      "Epoch-17  360 batches\tloss 1.2833 (1.2453)\taccu 81.250 (86.037)\n",
      "Epoch-17  380 batches\tloss 1.2923 (1.2456)\taccu 82.812 (86.053)\n",
      "Epoch-17  400 batches\tloss 1.2993 (1.2459)\taccu 85.938 (86.047)\n",
      "Epoch-17  420 batches\tloss 1.2185 (1.2484)\taccu 85.938 (85.934)\n",
      "Epoch-17  440 batches\tloss 1.2321 (1.2483)\taccu 89.062 (85.902)\n",
      "Epoch-17  460 batches\tloss 1.1719 (1.2477)\taccu 92.188 (85.934)\n",
      "Epoch-17  480 batches\tloss 1.2965 (1.2481)\taccu 79.688 (85.856)\n",
      "Epoch-17  99.2s\tTrain: loss 1.2481\taccu 85.8270\tValid: loss 1.3703\taccu 80.9552\n",
      "Epoch 17: val_acc improved from 80.6014 to 80.9552, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "17 0.001\n",
      "Epoch-18   20 batches\tloss 1.2534 (1.1999)\taccu 85.938 (87.969)\n",
      "Epoch-18   40 batches\tloss 1.3068 (1.1951)\taccu 90.625 (88.516)\n",
      "Epoch-18   60 batches\tloss 1.1553 (1.1995)\taccu 90.625 (88.099)\n",
      "Epoch-18   80 batches\tloss 1.2210 (1.1987)\taccu 84.375 (87.949)\n",
      "Epoch-18  100 batches\tloss 1.2478 (1.2041)\taccu 84.375 (87.453)\n",
      "Epoch-18  120 batches\tloss 1.2235 (1.2046)\taccu 85.938 (87.643)\n",
      "Epoch-18  140 batches\tloss 1.1654 (1.2050)\taccu 85.938 (87.634)\n",
      "Epoch-18  160 batches\tloss 1.3022 (1.2074)\taccu 85.938 (87.461)\n",
      "Epoch-18  180 batches\tloss 1.2086 (1.2081)\taccu 84.375 (87.474)\n",
      "Epoch-18  200 batches\tloss 1.2853 (1.2108)\taccu 85.938 (87.297)\n",
      "Epoch-18  220 batches\tloss 1.3353 (1.2162)\taccu 79.688 (87.053)\n",
      "Epoch-18  240 batches\tloss 1.3273 (1.2175)\taccu 82.812 (87.005)\n",
      "Epoch-18  260 batches\tloss 1.1694 (1.2178)\taccu 89.062 (86.977)\n",
      "Epoch-18  280 batches\tloss 1.2566 (1.2197)\taccu 85.938 (86.908)\n",
      "Epoch-18  300 batches\tloss 1.2021 (1.2203)\taccu 87.500 (86.875)\n",
      "Epoch-18  320 batches\tloss 1.1417 (1.2207)\taccu 90.625 (86.807)\n",
      "Epoch-18  340 batches\tloss 1.2005 (1.2201)\taccu 85.938 (86.834)\n",
      "Epoch-18  360 batches\tloss 1.2178 (1.2206)\taccu 85.938 (86.832)\n",
      "Epoch-18  380 batches\tloss 1.1952 (1.2222)\taccu 85.938 (86.780)\n",
      "Epoch-18  400 batches\tloss 1.2368 (1.2246)\taccu 87.500 (86.711)\n",
      "Epoch-18  420 batches\tloss 1.3324 (1.2278)\taccu 81.250 (86.548)\n",
      "Epoch-18  440 batches\tloss 1.2457 (1.2275)\taccu 85.938 (86.559)\n",
      "Epoch-18  460 batches\tloss 1.0606 (1.2278)\taccu 96.875 (86.525)\n",
      "Epoch-18  480 batches\tloss 1.4317 (1.2282)\taccu 78.125 (86.484)\n",
      "Epoch-18  119.9s\tTrain: loss 1.2286\taccu 86.4647\tValid: loss 1.3493\taccu 81.5596\n",
      "Epoch 18: val_acc improved from 80.9552 to 81.5596, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "18 0.001\n",
      "Epoch-19   20 batches\tloss 1.2870 (1.1724)\taccu 84.375 (88.984)\n",
      "Epoch-19   40 batches\tloss 1.1134 (1.1853)\taccu 89.062 (88.789)\n",
      "Epoch-19   60 batches\tloss 1.1602 (1.1914)\taccu 92.188 (88.516)\n",
      "Epoch-19   80 batches\tloss 1.3234 (1.1885)\taccu 85.938 (88.633)\n",
      "Epoch-19  100 batches\tloss 1.2868 (1.2000)\taccu 87.500 (88.156)\n",
      "Epoch-19  120 batches\tloss 1.1170 (1.2009)\taccu 93.750 (88.151)\n",
      "Epoch-19  140 batches\tloss 1.0824 (1.1962)\taccu 90.625 (88.214)\n",
      "Epoch-19  160 batches\tloss 1.2845 (1.1989)\taccu 87.500 (88.027)\n",
      "Epoch-19  180 batches\tloss 1.2366 (1.1977)\taccu 90.625 (88.090)\n",
      "Epoch-19  200 batches\tloss 1.1458 (1.1974)\taccu 92.188 (88.141)\n",
      "Epoch-19  220 batches\tloss 1.2618 (1.1994)\taccu 82.812 (87.990)\n",
      "Epoch-19  240 batches\tloss 1.2173 (1.2032)\taccu 84.375 (87.891)\n",
      "Epoch-19  260 batches\tloss 1.3347 (1.2073)\taccu 84.375 (87.674)\n",
      "Epoch-19  280 batches\tloss 1.1579 (1.2111)\taccu 89.062 (87.489)\n",
      "Epoch-19  300 batches\tloss 1.0450 (1.2096)\taccu 96.875 (87.578)\n",
      "Epoch-19  320 batches\tloss 1.1765 (1.2126)\taccu 87.500 (87.383)\n",
      "Epoch-19  340 batches\tloss 1.1714 (1.2148)\taccu 84.375 (87.270)\n",
      "Epoch-19  360 batches\tloss 1.1737 (1.2155)\taccu 87.500 (87.227)\n",
      "Epoch-19  380 batches\tloss 1.2737 (1.2171)\taccu 82.812 (87.081)\n",
      "Epoch-19  400 batches\tloss 1.3712 (1.2193)\taccu 81.250 (86.973)\n",
      "Epoch-19  420 batches\tloss 1.0860 (1.2210)\taccu 95.312 (86.942)\n",
      "Epoch-19  440 batches\tloss 1.1279 (1.2217)\taccu 93.750 (86.907)\n",
      "Epoch-19  460 batches\tloss 1.2507 (1.2218)\taccu 87.500 (86.868)\n",
      "Epoch-19  480 batches\tloss 1.1158 (1.2217)\taccu 93.750 (86.862)\n",
      "Epoch-19  131.9s\tTrain: loss 1.2217\taccu 86.8782\tValid: loss 1.3479\taccu 81.7070\n",
      "Epoch 19: val_acc improved from 81.5596 to 81.7070, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "19 0.001\n",
      "Epoch-20   20 batches\tloss 1.2677 (1.1757)\taccu 90.625 (88.516)\n",
      "Epoch-20   40 batches\tloss 1.0899 (1.1636)\taccu 92.188 (88.789)\n",
      "Epoch-20   60 batches\tloss 0.9933 (1.1625)\taccu 98.438 (89.271)\n",
      "Epoch-20   80 batches\tloss 1.1076 (1.1683)\taccu 92.188 (88.984)\n",
      "Epoch-20  100 batches\tloss 1.3167 (1.1731)\taccu 82.812 (88.562)\n",
      "Epoch-20  120 batches\tloss 1.2235 (1.1827)\taccu 82.812 (88.229)\n",
      "Epoch-20  140 batches\tloss 1.1331 (1.1858)\taccu 93.750 (88.181)\n",
      "Epoch-20  160 batches\tloss 1.0636 (1.1836)\taccu 96.875 (88.291)\n",
      "Epoch-20  180 batches\tloss 1.2321 (1.1866)\taccu 90.625 (88.082)\n",
      "Epoch-20  200 batches\tloss 1.1292 (1.1875)\taccu 89.062 (87.969)\n",
      "Epoch-20  220 batches\tloss 1.1975 (1.1897)\taccu 89.062 (87.926)\n",
      "Epoch-20  240 batches\tloss 1.1898 (1.1924)\taccu 89.062 (87.878)\n",
      "Epoch-20  260 batches\tloss 1.3176 (1.1930)\taccu 84.375 (87.897)\n",
      "Epoch-20  280 batches\tloss 1.2362 (1.1967)\taccu 89.062 (87.785)\n",
      "Epoch-20  300 batches\tloss 1.1488 (1.1992)\taccu 93.750 (87.688)\n",
      "Epoch-20  320 batches\tloss 1.2123 (1.1991)\taccu 90.625 (87.646)\n",
      "Epoch-20  340 batches\tloss 1.2290 (1.2000)\taccu 87.500 (87.583)\n",
      "Epoch-20  360 batches\tloss 1.1905 (1.2010)\taccu 87.500 (87.474)\n",
      "Epoch-20  380 batches\tloss 1.1492 (1.2017)\taccu 93.750 (87.459)\n",
      "Epoch-20  400 batches\tloss 1.2033 (1.1993)\taccu 87.500 (87.562)\n",
      "Epoch-20  420 batches\tloss 1.2678 (1.1999)\taccu 85.938 (87.586)\n",
      "Epoch-20  440 batches\tloss 1.1359 (1.2009)\taccu 93.750 (87.575)\n",
      "Epoch-20  460 batches\tloss 1.1814 (1.2008)\taccu 87.500 (87.578)\n",
      "Epoch-20  480 batches\tloss 1.2774 (1.2011)\taccu 81.250 (87.572)\n",
      "Epoch-20  132.0s\tTrain: loss 1.2024\taccu 87.5284\tValid: loss 1.3574\taccu 81.5596\n",
      "Epoch 20: val_acc did not improve\n",
      "20 0.001\n",
      "Epoch-21   20 batches\tloss 1.1069 (1.1499)\taccu 89.062 (89.141)\n",
      "Epoch-21   40 batches\tloss 1.2286 (1.1619)\taccu 89.062 (89.375)\n",
      "Epoch-21   60 batches\tloss 1.1904 (1.1712)\taccu 85.938 (88.802)\n",
      "Epoch-21   80 batches\tloss 1.2441 (1.1658)\taccu 87.500 (89.043)\n",
      "Epoch-21  100 batches\tloss 1.1489 (1.1694)\taccu 90.625 (88.859)\n",
      "Epoch-21  120 batches\tloss 1.2506 (1.1702)\taccu 87.500 (88.854)\n",
      "Epoch-21  140 batches\tloss 1.2306 (1.1745)\taccu 84.375 (88.683)\n",
      "Epoch-21  160 batches\tloss 1.2961 (1.1757)\taccu 85.938 (88.652)\n",
      "Epoch-21  180 batches\tloss 1.1335 (1.1790)\taccu 89.062 (88.455)\n",
      "Epoch-21  200 batches\tloss 1.0918 (1.1779)\taccu 89.062 (88.461)\n",
      "Epoch-21  220 batches\tloss 1.1715 (1.1829)\taccu 89.062 (88.210)\n",
      "Epoch-21  240 batches\tloss 1.1079 (1.1826)\taccu 85.938 (88.242)\n",
      "Epoch-21  260 batches\tloss 1.2113 (1.1840)\taccu 89.062 (88.215)\n",
      "Epoch-21  280 batches\tloss 0.9722 (1.1855)\taccu 100.000 (88.170)\n",
      "Epoch-21  300 batches\tloss 1.3056 (1.1869)\taccu 85.938 (88.141)\n",
      "Epoch-21  320 batches\tloss 1.2937 (1.1896)\taccu 81.250 (88.037)\n",
      "Epoch-21  340 batches\tloss 1.2472 (1.1909)\taccu 89.062 (88.019)\n",
      "Epoch-21  360 batches\tloss 1.3167 (1.1916)\taccu 85.938 (87.999)\n",
      "Epoch-21  380 batches\tloss 1.1886 (1.1925)\taccu 87.500 (87.932)\n",
      "Epoch-21  400 batches\tloss 1.0880 (1.1933)\taccu 93.750 (87.875)\n",
      "Epoch-21  420 batches\tloss 1.1161 (1.1953)\taccu 89.062 (87.786)\n",
      "Epoch-21  440 batches\tloss 1.3236 (1.1967)\taccu 85.938 (87.756)\n",
      "Epoch-21  460 batches\tloss 1.1950 (1.1990)\taccu 85.938 (87.666)\n",
      "Epoch-21  480 batches\tloss 1.2864 (1.1996)\taccu 81.250 (87.653)\n",
      "Epoch-21  131.5s\tTrain: loss 1.2001\taccu 87.6231\tValid: loss 1.3664\taccu 81.3090\n",
      "Epoch 21: val_acc did not improve\n",
      "21 0.001\n",
      "Epoch-22   20 batches\tloss 1.2271 (1.1698)\taccu 84.375 (89.219)\n",
      "Epoch-22   40 batches\tloss 1.3303 (1.1570)\taccu 82.812 (89.492)\n",
      "Epoch-22   60 batches\tloss 1.0930 (1.1522)\taccu 92.188 (89.818)\n",
      "Epoch-22   80 batches\tloss 1.0738 (1.1516)\taccu 90.625 (89.570)\n",
      "Epoch-22  100 batches\tloss 1.2490 (1.1537)\taccu 82.812 (89.578)\n",
      "Epoch-22  120 batches\tloss 1.1709 (1.1633)\taccu 89.062 (89.206)\n",
      "Epoch-22  140 batches\tloss 1.1657 (1.1663)\taccu 84.375 (89.096)\n",
      "Epoch-22  160 batches\tloss 1.1404 (1.1662)\taccu 90.625 (89.023)\n",
      "Epoch-22  180 batches\tloss 1.1771 (1.1667)\taccu 87.500 (89.019)\n",
      "Epoch-22  200 batches\tloss 1.1715 (1.1652)\taccu 85.938 (89.117)\n",
      "Epoch-22  220 batches\tloss 1.1789 (1.1667)\taccu 87.500 (88.977)\n",
      "Epoch-22  240 batches\tloss 1.0863 (1.1707)\taccu 95.312 (88.815)\n",
      "Epoch-22  260 batches\tloss 1.4075 (1.1735)\taccu 78.125 (88.756)\n",
      "Epoch-22  280 batches\tloss 1.2265 (1.1748)\taccu 87.500 (88.722)\n",
      "Epoch-22  300 batches\tloss 1.0407 (1.1767)\taccu 95.312 (88.615)\n",
      "Epoch-22  320 batches\tloss 1.1473 (1.1766)\taccu 89.062 (88.560)\n",
      "Epoch-22  340 batches\tloss 1.1922 (1.1783)\taccu 89.062 (88.497)\n",
      "Epoch-22  360 batches\tloss 1.1801 (1.1823)\taccu 87.500 (88.355)\n",
      "Epoch-22  380 batches\tloss 1.2155 (1.1835)\taccu 82.812 (88.298)\n",
      "Epoch-22  400 batches\tloss 1.0991 (1.1840)\taccu 93.750 (88.266)\n",
      "Epoch-22  420 batches\tloss 1.1971 (1.1845)\taccu 87.500 (88.207)\n",
      "Epoch-22  440 batches\tloss 1.3089 (1.1871)\taccu 81.250 (88.104)\n",
      "Epoch-22  460 batches\tloss 1.2689 (1.1870)\taccu 82.812 (88.077)\n",
      "Epoch-22  480 batches\tloss 1.3048 (1.1873)\taccu 79.688 (88.047)\n",
      "Epoch-22  131.3s\tTrain: loss 1.1876\taccu 88.0461\tValid: loss 1.3438\taccu 82.0312\n",
      "Epoch 22: val_acc improved from 81.7070 to 82.0312, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "22 0.001\n",
      "Epoch-23   20 batches\tloss 1.2441 (1.1815)\taccu 84.375 (88.359)\n",
      "Epoch-23   40 batches\tloss 1.0529 (1.1615)\taccu 95.312 (88.945)\n",
      "Epoch-23   60 batches\tloss 1.1720 (1.1623)\taccu 87.500 (89.115)\n",
      "Epoch-23   80 batches\tloss 1.1275 (1.1650)\taccu 93.750 (88.887)\n",
      "Epoch-23  100 batches\tloss 1.2504 (1.1634)\taccu 85.938 (88.906)\n",
      "Epoch-23  120 batches\tloss 1.1792 (1.1632)\taccu 87.500 (88.893)\n",
      "Epoch-23  140 batches\tloss 1.1735 (1.1620)\taccu 87.500 (88.951)\n",
      "Epoch-23  160 batches\tloss 1.0327 (1.1620)\taccu 93.750 (88.975)\n",
      "Epoch-23  180 batches\tloss 1.0954 (1.1657)\taccu 93.750 (88.819)\n",
      "Epoch-23  200 batches\tloss 1.1374 (1.1630)\taccu 92.188 (88.984)\n",
      "Epoch-23  220 batches\tloss 1.3864 (1.1644)\taccu 82.812 (88.857)\n",
      "Epoch-23  240 batches\tloss 1.1678 (1.1632)\taccu 90.625 (88.939)\n",
      "Epoch-23  260 batches\tloss 1.1364 (1.1652)\taccu 90.625 (88.876)\n",
      "Epoch-23  280 batches\tloss 1.1760 (1.1683)\taccu 90.625 (88.728)\n",
      "Epoch-23  300 batches\tloss 1.3318 (1.1694)\taccu 84.375 (88.708)\n",
      "Epoch-23  320 batches\tloss 1.4579 (1.1711)\taccu 79.688 (88.667)\n",
      "Epoch-23  340 batches\tloss 1.1174 (1.1711)\taccu 93.750 (88.644)\n",
      "Epoch-23  360 batches\tloss 1.1446 (1.1741)\taccu 92.188 (88.550)\n",
      "Epoch-23  380 batches\tloss 1.1797 (1.1751)\taccu 90.625 (88.520)\n",
      "Epoch-23  400 batches\tloss 1.2669 (1.1760)\taccu 82.812 (88.473)\n",
      "Epoch-23  420 batches\tloss 1.1035 (1.1771)\taccu 90.625 (88.445)\n",
      "Epoch-23  440 batches\tloss 1.1637 (1.1778)\taccu 87.500 (88.391)\n",
      "Epoch-23  460 batches\tloss 1.2100 (1.1793)\taccu 84.375 (88.359)\n",
      "Epoch-23  480 batches\tloss 1.1813 (1.1802)\taccu 87.500 (88.275)\n",
      "Epoch-23  131.5s\tTrain: loss 1.1804\taccu 88.2639\tValid: loss 1.3353\taccu 82.2671\n",
      "Epoch 23: val_acc improved from 82.0312 to 82.2671, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "23 0.001\n",
      "Epoch-24   20 batches\tloss 1.0477 (1.1451)\taccu 95.312 (89.766)\n",
      "Epoch-24   40 batches\tloss 1.0581 (1.1367)\taccu 96.875 (90.156)\n",
      "Epoch-24   60 batches\tloss 1.2029 (1.1450)\taccu 92.188 (89.948)\n",
      "Epoch-24   80 batches\tloss 1.0110 (1.1483)\taccu 98.438 (89.824)\n",
      "Epoch-24  100 batches\tloss 1.0820 (1.1522)\taccu 92.188 (89.641)\n",
      "Epoch-24  120 batches\tloss 1.0713 (1.1466)\taccu 92.188 (89.870)\n",
      "Epoch-24  140 batches\tloss 1.1538 (1.1473)\taccu 90.625 (89.799)\n",
      "Epoch-24  160 batches\tloss 1.1131 (1.1544)\taccu 92.188 (89.512)\n",
      "Epoch-24  180 batches\tloss 1.1585 (1.1598)\taccu 89.062 (89.288)\n",
      "Epoch-24  200 batches\tloss 1.0380 (1.1625)\taccu 95.312 (89.156)\n",
      "Epoch-24  220 batches\tloss 1.2546 (1.1637)\taccu 81.250 (89.034)\n",
      "Epoch-24  240 batches\tloss 1.0800 (1.1635)\taccu 90.625 (89.108)\n",
      "Epoch-24  260 batches\tloss 1.1512 (1.1638)\taccu 90.625 (89.044)\n",
      "Epoch-24  280 batches\tloss 1.0816 (1.1666)\taccu 93.750 (88.973)\n",
      "Epoch-24  300 batches\tloss 1.0572 (1.1661)\taccu 92.188 (88.953)\n",
      "Epoch-24  320 batches\tloss 1.2252 (1.1657)\taccu 84.375 (89.014)\n",
      "Epoch-24  340 batches\tloss 1.3579 (1.1659)\taccu 81.250 (89.021)\n",
      "Epoch-24  360 batches\tloss 1.2436 (1.1654)\taccu 82.812 (89.045)\n",
      "Epoch-24  380 batches\tloss 1.2760 (1.1665)\taccu 85.938 (89.034)\n",
      "Epoch-24  400 batches\tloss 1.1459 (1.1676)\taccu 87.500 (88.945)\n",
      "Epoch-24  420 batches\tloss 1.2026 (1.1694)\taccu 89.062 (88.906)\n",
      "Epoch-24  440 batches\tloss 1.2492 (1.1705)\taccu 85.938 (88.864)\n",
      "Epoch-24  460 batches\tloss 1.1510 (1.1705)\taccu 89.062 (88.825)\n",
      "Epoch-24  480 batches\tloss 1.2641 (1.1718)\taccu 82.812 (88.763)\n",
      "Epoch-24  131.2s\tTrain: loss 1.1726\taccu 88.6806\tValid: loss 1.3284\taccu 82.1197\n",
      "Epoch 24: val_acc did not improve\n",
      "24 0.001\n",
      "Epoch-25   20 batches\tloss 1.0980 (1.0944)\taccu 92.188 (91.797)\n",
      "Epoch-25   40 batches\tloss 1.1097 (1.1139)\taccu 90.625 (91.211)\n",
      "Epoch-25   60 batches\tloss 1.0346 (1.1174)\taccu 93.750 (90.755)\n",
      "Epoch-25   80 batches\tloss 1.1061 (1.1283)\taccu 93.750 (90.332)\n",
      "Epoch-25  100 batches\tloss 1.0428 (1.1329)\taccu 95.312 (90.156)\n",
      "Epoch-25  120 batches\tloss 1.1618 (1.1367)\taccu 93.750 (90.039)\n",
      "Epoch-25  140 batches\tloss 1.1300 (1.1373)\taccu 89.062 (90.033)\n",
      "Epoch-25  160 batches\tloss 1.0437 (1.1391)\taccu 95.312 (90.039)\n",
      "Epoch-25  180 batches\tloss 1.0548 (1.1410)\taccu 93.750 (89.957)\n",
      "Epoch-25  200 batches\tloss 1.2480 (1.1444)\taccu 85.938 (89.766)\n",
      "Epoch-25  220 batches\tloss 1.2196 (1.1457)\taccu 85.938 (89.723)\n",
      "Epoch-25  240 batches\tloss 1.0945 (1.1455)\taccu 89.062 (89.661)\n",
      "Epoch-25  260 batches\tloss 1.1685 (1.1493)\taccu 92.188 (89.447)\n",
      "Epoch-25  280 batches\tloss 1.1247 (1.1511)\taccu 93.750 (89.459)\n",
      "Epoch-25  300 batches\tloss 1.2382 (1.1521)\taccu 84.375 (89.417)\n",
      "Epoch-25  320 batches\tloss 1.1556 (1.1548)\taccu 87.500 (89.316)\n",
      "Epoch-25  340 batches\tloss 1.1895 (1.1544)\taccu 87.500 (89.324)\n",
      "Epoch-25  360 batches\tloss 1.3005 (1.1564)\taccu 79.688 (89.232)\n",
      "Epoch-25  380 batches\tloss 1.2018 (1.1585)\taccu 92.188 (89.206)\n",
      "Epoch-25  400 batches\tloss 1.2042 (1.1607)\taccu 85.938 (89.117)\n",
      "Epoch-25  420 batches\tloss 1.3409 (1.1632)\taccu 85.938 (89.051)\n",
      "Epoch-25  440 batches\tloss 1.2250 (1.1638)\taccu 85.938 (89.013)\n",
      "Epoch-25  460 batches\tloss 1.1128 (1.1627)\taccu 92.188 (89.059)\n",
      "Epoch-25  480 batches\tloss 1.1832 (1.1641)\taccu 90.625 (88.978)\n",
      "Epoch-25  131.3s\tTrain: loss 1.1645\taccu 88.9836\tValid: loss 1.3253\taccu 82.6798\n",
      "Epoch 25: val_acc improved from 82.2671 to 82.6798, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "25 0.001\n",
      "Epoch-26   20 batches\tloss 1.1347 (1.1288)\taccu 89.062 (90.469)\n",
      "Epoch-26   40 batches\tloss 1.1781 (1.1350)\taccu 85.938 (89.609)\n",
      "Epoch-26   60 batches\tloss 1.1624 (1.1340)\taccu 90.625 (89.896)\n",
      "Epoch-26   80 batches\tloss 1.1524 (1.1354)\taccu 89.062 (89.805)\n",
      "Epoch-26  100 batches\tloss 1.1590 (1.1371)\taccu 85.938 (89.719)\n",
      "Epoch-26  120 batches\tloss 1.0561 (1.1361)\taccu 92.188 (89.779)\n",
      "Epoch-26  140 batches\tloss 1.1024 (1.1370)\taccu 89.062 (89.777)\n",
      "Epoch-26  160 batches\tloss 1.1306 (1.1401)\taccu 90.625 (89.707)\n",
      "Epoch-26  180 batches\tloss 1.2762 (1.1441)\taccu 85.938 (89.592)\n",
      "Epoch-26  200 batches\tloss 1.3272 (1.1451)\taccu 79.688 (89.531)\n",
      "Epoch-26  220 batches\tloss 1.0777 (1.1452)\taccu 93.750 (89.531)\n",
      "Epoch-26  240 batches\tloss 1.0695 (1.1435)\taccu 93.750 (89.655)\n",
      "Epoch-26  260 batches\tloss 1.0693 (1.1461)\taccu 89.062 (89.597)\n",
      "Epoch-26  280 batches\tloss 1.2403 (1.1452)\taccu 82.812 (89.609)\n",
      "Epoch-26  300 batches\tloss 1.3109 (1.1470)\taccu 82.812 (89.552)\n",
      "Epoch-26  320 batches\tloss 1.1119 (1.1481)\taccu 89.062 (89.478)\n",
      "Epoch-26  340 batches\tloss 1.1399 (1.1496)\taccu 87.500 (89.430)\n",
      "Epoch-26  360 batches\tloss 1.0376 (1.1494)\taccu 93.750 (89.440)\n",
      "Epoch-26  380 batches\tloss 1.1245 (1.1493)\taccu 89.062 (89.433)\n",
      "Epoch-26  400 batches\tloss 1.1259 (1.1502)\taccu 89.062 (89.406)\n",
      "Epoch-26  420 batches\tloss 1.1821 (1.1499)\taccu 85.938 (89.423)\n",
      "Epoch-26  440 batches\tloss 1.1823 (1.1517)\taccu 92.188 (89.364)\n",
      "Epoch-26  460 batches\tloss 1.1497 (1.1524)\taccu 89.062 (89.365)\n",
      "Epoch-26  480 batches\tloss 1.2275 (1.1535)\taccu 89.062 (89.368)\n",
      "Epoch-26  131.6s\tTrain: loss 1.1536\taccu 89.3908\tValid: loss 1.3308\taccu 82.1344\n",
      "Epoch 26: val_acc did not improve\n",
      "26 0.001\n",
      "Epoch-27   20 batches\tloss 1.1895 (1.1433)\taccu 90.625 (90.391)\n",
      "Epoch-27   40 batches\tloss 1.3106 (1.1517)\taccu 81.250 (89.688)\n",
      "Epoch-27   60 batches\tloss 1.0642 (1.1404)\taccu 92.188 (90.052)\n",
      "Epoch-27   80 batches\tloss 1.0750 (1.1444)\taccu 92.188 (89.727)\n",
      "Epoch-27  100 batches\tloss 1.0781 (1.1373)\taccu 96.875 (90.047)\n",
      "Epoch-27  120 batches\tloss 1.2208 (1.1373)\taccu 89.062 (89.987)\n",
      "Epoch-27  140 batches\tloss 1.0602 (1.1370)\taccu 96.875 (90.022)\n",
      "Epoch-27  160 batches\tloss 1.0727 (1.1363)\taccu 96.875 (90.078)\n",
      "Epoch-27  180 batches\tloss 1.0346 (1.1354)\taccu 92.188 (90.148)\n",
      "Epoch-27  200 batches\tloss 1.0910 (1.1355)\taccu 93.750 (90.117)\n",
      "Epoch-27  220 batches\tloss 1.1497 (1.1375)\taccu 87.500 (90.050)\n",
      "Epoch-27  240 batches\tloss 1.1620 (1.1369)\taccu 89.062 (90.026)\n",
      "Epoch-27  260 batches\tloss 1.0527 (1.1407)\taccu 95.312 (89.940)\n",
      "Epoch-27  280 batches\tloss 1.0791 (1.1393)\taccu 93.750 (89.944)\n",
      "Epoch-27  300 batches\tloss 1.1530 (1.1400)\taccu 87.500 (89.979)\n",
      "Epoch-27  320 batches\tloss 1.2198 (1.1395)\taccu 90.625 (90.039)\n",
      "Epoch-27  340 batches\tloss 1.1474 (1.1409)\taccu 89.062 (89.972)\n",
      "Epoch-27  360 batches\tloss 1.2730 (1.1443)\taccu 84.375 (89.822)\n",
      "Epoch-27  380 batches\tloss 1.0926 (1.1446)\taccu 93.750 (89.799)\n",
      "Epoch-27  400 batches\tloss 1.1481 (1.1458)\taccu 89.062 (89.738)\n",
      "Epoch-27  420 batches\tloss 1.2245 (1.1476)\taccu 87.500 (89.628)\n",
      "Epoch-27  440 batches\tloss 1.2363 (1.1492)\taccu 87.500 (89.567)\n",
      "Epoch-27  460 batches\tloss 1.1413 (1.1491)\taccu 90.625 (89.596)\n",
      "Epoch-27  480 batches\tloss 1.0965 (1.1508)\taccu 90.625 (89.495)\n",
      "Epoch-27  128.5s\tTrain: loss 1.1506\taccu 89.5265\tValid: loss 1.3364\taccu 82.2966\n",
      "Epoch 27: val_acc did not improve\n",
      "27 0.001\n",
      "Epoch-28   20 batches\tloss 1.0159 (1.1295)\taccu 95.312 (90.625)\n",
      "Epoch-28   40 batches\tloss 1.0310 (1.1179)\taccu 96.875 (91.406)\n",
      "Epoch-28   60 batches\tloss 1.0915 (1.1075)\taccu 90.625 (91.589)\n",
      "Epoch-28   80 batches\tloss 1.1422 (1.1097)\taccu 93.750 (91.504)\n",
      "Epoch-28  100 batches\tloss 1.2021 (1.1114)\taccu 85.938 (91.234)\n",
      "Epoch-28  120 batches\tloss 1.1120 (1.1168)\taccu 95.312 (91.042)\n",
      "Epoch-28  140 batches\tloss 1.1153 (1.1164)\taccu 90.625 (91.049)\n",
      "Epoch-28  160 batches\tloss 1.1566 (1.1192)\taccu 84.375 (90.850)\n",
      "Epoch-28  180 batches\tloss 1.1285 (1.1209)\taccu 90.625 (90.764)\n",
      "Epoch-28  200 batches\tloss 1.2246 (1.1210)\taccu 90.625 (90.758)\n",
      "Epoch-28  220 batches\tloss 1.2056 (1.1250)\taccu 82.812 (90.554)\n",
      "Epoch-28  240 batches\tloss 1.1134 (1.1265)\taccu 90.625 (90.514)\n",
      "Epoch-28  260 batches\tloss 1.2509 (1.1303)\taccu 89.062 (90.391)\n",
      "Epoch-28  280 batches\tloss 1.3662 (1.1330)\taccu 76.562 (90.240)\n",
      "Epoch-28  300 batches\tloss 1.2254 (1.1343)\taccu 87.500 (90.167)\n",
      "Epoch-28  320 batches\tloss 1.0506 (1.1334)\taccu 93.750 (90.215)\n",
      "Epoch-28  340 batches\tloss 1.0873 (1.1345)\taccu 95.312 (90.147)\n",
      "Epoch-28  360 batches\tloss 1.0534 (1.1376)\taccu 92.188 (89.965)\n",
      "Epoch-28  380 batches\tloss 1.1662 (1.1385)\taccu 92.188 (89.959)\n",
      "Epoch-28  400 batches\tloss 1.1478 (1.1388)\taccu 93.750 (90.008)\n",
      "Epoch-28  420 batches\tloss 1.3264 (1.1413)\taccu 79.688 (89.911)\n",
      "Epoch-28  440 batches\tloss 1.2937 (1.1414)\taccu 84.375 (89.890)\n",
      "Epoch-28  460 batches\tloss 1.0836 (1.1430)\taccu 92.188 (89.769)\n",
      "Epoch-28  480 batches\tloss 1.2540 (1.1440)\taccu 87.500 (89.714)\n",
      "Epoch-28  102.6s\tTrain: loss 1.1445\taccu 89.6907\tValid: loss 1.3529\taccu 81.8838\n",
      "Epoch 28: val_acc did not improve\n",
      "28 0.001\n",
      "Epoch-29   20 batches\tloss 1.0603 (1.0945)\taccu 95.312 (91.797)\n",
      "Epoch-29   40 batches\tloss 1.0761 (1.1096)\taccu 92.188 (90.703)\n",
      "Epoch-29   60 batches\tloss 1.0756 (1.1074)\taccu 89.062 (90.833)\n",
      "Epoch-29   80 batches\tloss 1.0308 (1.1040)\taccu 92.188 (91.250)\n",
      "Epoch-29  100 batches\tloss 1.0994 (1.1066)\taccu 90.625 (91.047)\n",
      "Epoch-29  120 batches\tloss 1.0671 (1.1118)\taccu 93.750 (90.768)\n",
      "Epoch-29  140 batches\tloss 1.0751 (1.1124)\taccu 93.750 (90.837)\n",
      "Epoch-29  160 batches\tloss 1.1843 (1.1149)\taccu 87.500 (90.830)\n",
      "Epoch-29  180 batches\tloss 1.1026 (1.1173)\taccu 95.312 (90.764)\n",
      "Epoch-29  200 batches\tloss 1.1341 (1.1170)\taccu 92.188 (90.828)\n",
      "Epoch-29  220 batches\tloss 1.0859 (1.1199)\taccu 89.062 (90.668)\n",
      "Epoch-29  240 batches\tloss 1.0947 (1.1218)\taccu 90.625 (90.540)\n",
      "Epoch-29  260 batches\tloss 1.2970 (1.1243)\taccu 85.938 (90.517)\n",
      "Epoch-29  280 batches\tloss 1.0475 (1.1274)\taccu 95.312 (90.368)\n",
      "Epoch-29  300 batches\tloss 1.0985 (1.1279)\taccu 90.625 (90.354)\n",
      "Epoch-29  320 batches\tloss 1.2478 (1.1299)\taccu 82.812 (90.200)\n",
      "Epoch-29  340 batches\tloss 1.1576 (1.1318)\taccu 93.750 (90.124)\n",
      "Epoch-29  360 batches\tloss 1.1197 (1.1330)\taccu 90.625 (90.022)\n",
      "Epoch-29  380 batches\tloss 1.1845 (1.1345)\taccu 84.375 (90.012)\n",
      "Epoch-29  400 batches\tloss 1.1932 (1.1342)\taccu 85.938 (90.066)\n",
      "Epoch-29  420 batches\tloss 1.1844 (1.1352)\taccu 90.625 (90.019)\n",
      "Epoch-29  440 batches\tloss 1.1891 (1.1350)\taccu 87.500 (90.036)\n",
      "Epoch-29  460 batches\tloss 1.1955 (1.1365)\taccu 90.625 (89.935)\n",
      "Epoch-29  480 batches\tloss 1.0360 (1.1364)\taccu 93.750 (89.954)\n",
      "Epoch-29  103.3s\tTrain: loss 1.1374\taccu 89.9400\tValid: loss 1.3350\taccu 82.3555\n",
      "Epoch 29: val_acc did not improve\n",
      "29 0.001\n",
      "Epoch-30   20 batches\tloss 1.2388 (1.0951)\taccu 87.500 (92.109)\n",
      "Epoch-30   40 batches\tloss 1.0441 (1.1002)\taccu 95.312 (92.305)\n",
      "Epoch-30   60 batches\tloss 1.0713 (1.1040)\taccu 93.750 (91.589)\n",
      "Epoch-30   80 batches\tloss 1.1784 (1.1049)\taccu 89.062 (91.445)\n",
      "Epoch-30  100 batches\tloss 1.1163 (1.1103)\taccu 92.188 (91.219)\n",
      "Epoch-30  120 batches\tloss 1.0242 (1.1075)\taccu 96.875 (91.354)\n",
      "Epoch-30  140 batches\tloss 1.0621 (1.1084)\taccu 92.188 (91.295)\n",
      "Epoch-30  160 batches\tloss 1.1295 (1.1125)\taccu 92.188 (91.064)\n",
      "Epoch-30  180 batches\tloss 1.2490 (1.1118)\taccu 84.375 (91.128)\n",
      "Epoch-30  200 batches\tloss 1.0386 (1.1100)\taccu 93.750 (91.258)\n",
      "Epoch-30  220 batches\tloss 1.1911 (1.1105)\taccu 89.062 (91.229)\n",
      "Epoch-30  240 batches\tloss 1.0552 (1.1129)\taccu 92.188 (91.107)\n",
      "Epoch-30  260 batches\tloss 1.1688 (1.1144)\taccu 89.062 (91.100)\n",
      "Epoch-30  280 batches\tloss 1.2205 (1.1166)\taccu 87.500 (90.982)\n",
      "Epoch-30  300 batches\tloss 1.1206 (1.1185)\taccu 87.500 (90.885)\n",
      "Epoch-30  320 batches\tloss 1.2208 (1.1217)\taccu 87.500 (90.752)\n",
      "Epoch-30  340 batches\tloss 1.1727 (1.1228)\taccu 89.062 (90.694)\n",
      "Epoch-30  360 batches\tloss 1.2703 (1.1241)\taccu 89.062 (90.651)\n",
      "Epoch-30  380 batches\tloss 1.1495 (1.1247)\taccu 90.625 (90.641)\n",
      "Epoch-30  400 batches\tloss 1.1188 (1.1248)\taccu 93.750 (90.645)\n",
      "Epoch-30  420 batches\tloss 1.1030 (1.1256)\taccu 90.625 (90.629)\n",
      "Epoch-30  440 batches\tloss 1.0227 (1.1264)\taccu 95.312 (90.611)\n",
      "Epoch-30  460 batches\tloss 1.1097 (1.1267)\taccu 89.062 (90.591)\n",
      "Epoch-30  480 batches\tloss 1.2210 (1.1291)\taccu 89.062 (90.479)\n",
      "Epoch-30  125.9s\tTrain: loss 1.1303\taccu 90.4198\tValid: loss 1.3386\taccu 82.1050\n",
      "Epoch 30: val_acc did not improve\n",
      "30 0.001\n",
      "Epoch-31   20 batches\tloss 1.0655 (1.1071)\taccu 92.188 (90.391)\n",
      "Epoch-31   40 batches\tloss 1.0537 (1.1048)\taccu 90.625 (91.172)\n",
      "Epoch-31   60 batches\tloss 1.1656 (1.1062)\taccu 93.750 (91.641)\n",
      "Epoch-31   80 batches\tloss 1.0576 (1.1043)\taccu 95.312 (91.582)\n",
      "Epoch-31  100 batches\tloss 1.1092 (1.1020)\taccu 90.625 (91.703)\n",
      "Epoch-31  120 batches\tloss 1.2169 (1.1054)\taccu 84.375 (91.445)\n",
      "Epoch-31  140 batches\tloss 1.2014 (1.1089)\taccu 85.938 (91.217)\n",
      "Epoch-31  160 batches\tloss 1.0424 (1.1081)\taccu 95.312 (91.230)\n",
      "Epoch-31  180 batches\tloss 1.1651 (1.1124)\taccu 84.375 (91.024)\n",
      "Epoch-31  200 batches\tloss 1.1599 (1.1129)\taccu 90.625 (91.047)\n",
      "Epoch-31  220 batches\tloss 1.0945 (1.1150)\taccu 89.062 (90.952)\n",
      "Epoch-31  240 batches\tloss 1.0804 (1.1151)\taccu 93.750 (90.944)\n",
      "Epoch-31  260 batches\tloss 1.0479 (1.1133)\taccu 93.750 (90.986)\n",
      "Epoch-31  280 batches\tloss 1.3037 (1.1156)\taccu 79.688 (90.831)\n",
      "Epoch-31  300 batches\tloss 1.0340 (1.1172)\taccu 95.312 (90.729)\n",
      "Epoch-31  320 batches\tloss 1.1159 (1.1174)\taccu 90.625 (90.688)\n",
      "Epoch-31  340 batches\tloss 1.1637 (1.1198)\taccu 90.625 (90.630)\n",
      "Epoch-31  360 batches\tloss 1.2245 (1.1218)\taccu 84.375 (90.564)\n",
      "Epoch-31  380 batches\tloss 1.1293 (1.1215)\taccu 87.500 (90.576)\n",
      "Epoch-31  400 batches\tloss 1.1744 (1.1225)\taccu 89.062 (90.555)\n",
      "Epoch-31  420 batches\tloss 1.2235 (1.1245)\taccu 90.625 (90.469)\n",
      "Epoch-31  440 batches\tloss 1.1907 (1.1250)\taccu 90.625 (90.444)\n",
      "Epoch-31  460 batches\tloss 1.0956 (1.1256)\taccu 93.750 (90.380)\n",
      "Epoch-31  480 batches\tloss 1.1241 (1.1259)\taccu 87.500 (90.339)\n",
      "Epoch-31  128.7s\tTrain: loss 1.1265\taccu 90.2967\tValid: loss 1.3134\taccu 83.0336\n",
      "Epoch 31: val_acc improved from 82.6798 to 83.0336, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "31 0.001\n",
      "Epoch-32   20 batches\tloss 1.0373 (1.0880)\taccu 93.750 (91.797)\n",
      "Epoch-32   40 batches\tloss 1.0352 (1.0781)\taccu 95.312 (92.617)\n",
      "Epoch-32   60 batches\tloss 1.1578 (1.0887)\taccu 90.625 (92.161)\n",
      "Epoch-32   80 batches\tloss 1.0801 (1.0873)\taccu 92.188 (92.207)\n",
      "Epoch-32  100 batches\tloss 1.0965 (1.0909)\taccu 92.188 (91.906)\n",
      "Epoch-32  120 batches\tloss 1.3066 (1.0962)\taccu 81.250 (91.732)\n",
      "Epoch-32  140 batches\tloss 1.1076 (1.0987)\taccu 89.062 (91.641)\n",
      "Epoch-32  160 batches\tloss 1.0938 (1.1004)\taccu 90.625 (91.602)\n",
      "Epoch-32  180 batches\tloss 1.1623 (1.1021)\taccu 89.062 (91.571)\n",
      "Epoch-32  200 batches\tloss 1.1372 (1.1044)\taccu 90.625 (91.500)\n",
      "Epoch-32  220 batches\tloss 1.0901 (1.1032)\taccu 92.188 (91.584)\n",
      "Epoch-32  240 batches\tloss 1.0680 (1.1029)\taccu 92.188 (91.582)\n",
      "Epoch-32  260 batches\tloss 1.1582 (1.1052)\taccu 93.750 (91.478)\n",
      "Epoch-32  280 batches\tloss 1.0699 (1.1068)\taccu 92.188 (91.417)\n",
      "Epoch-32  300 batches\tloss 1.2148 (1.1098)\taccu 87.500 (91.281)\n",
      "Epoch-32  320 batches\tloss 1.1850 (1.1104)\taccu 90.625 (91.274)\n",
      "Epoch-32  340 batches\tloss 1.1816 (1.1105)\taccu 85.938 (91.259)\n",
      "Epoch-32  360 batches\tloss 1.2077 (1.1106)\taccu 82.812 (91.246)\n",
      "Epoch-32  380 batches\tloss 1.0553 (1.1121)\taccu 93.750 (91.127)\n",
      "Epoch-32  400 batches\tloss 1.1321 (1.1124)\taccu 89.062 (91.129)\n",
      "Epoch-32  420 batches\tloss 1.2420 (1.1151)\taccu 81.250 (91.049)\n",
      "Epoch-32  440 batches\tloss 1.2354 (1.1159)\taccu 90.625 (91.019)\n",
      "Epoch-32  460 batches\tloss 1.1472 (1.1172)\taccu 87.500 (90.944)\n",
      "Epoch-32  480 batches\tloss 1.0922 (1.1184)\taccu 89.062 (90.905)\n",
      "Epoch-32  130.1s\tTrain: loss 1.1199\taccu 90.8460\tValid: loss 1.3124\taccu 83.1221\n",
      "Epoch 32: val_acc improved from 83.0336 to 83.1221, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "32 0.001\n",
      "Epoch-33   20 batches\tloss 1.0372 (1.1121)\taccu 96.875 (91.172)\n",
      "Epoch-33   40 batches\tloss 1.0889 (1.1080)\taccu 92.188 (91.484)\n",
      "Epoch-33   60 batches\tloss 1.1491 (1.1046)\taccu 90.625 (91.719)\n",
      "Epoch-33   80 batches\tloss 1.1550 (1.1090)\taccu 84.375 (91.367)\n",
      "Epoch-33  100 batches\tloss 1.1014 (1.1058)\taccu 92.188 (91.391)\n",
      "Epoch-33  120 batches\tloss 1.1282 (1.1042)\taccu 92.188 (91.523)\n",
      "Epoch-33  140 batches\tloss 1.2034 (1.1076)\taccu 87.500 (91.250)\n",
      "Epoch-33  160 batches\tloss 1.0936 (1.1082)\taccu 93.750 (91.211)\n",
      "Epoch-33  180 batches\tloss 1.0523 (1.1074)\taccu 93.750 (91.293)\n",
      "Epoch-33  200 batches\tloss 1.0883 (1.1068)\taccu 93.750 (91.258)\n",
      "Epoch-33  220 batches\tloss 1.0680 (1.1093)\taccu 92.188 (91.108)\n",
      "Epoch-33  240 batches\tloss 1.1595 (1.1099)\taccu 90.625 (91.068)\n",
      "Epoch-33  260 batches\tloss 1.0583 (1.1121)\taccu 95.312 (91.010)\n",
      "Epoch-33  280 batches\tloss 1.1169 (1.1137)\taccu 90.625 (90.938)\n",
      "Epoch-33  300 batches\tloss 0.9944 (1.1150)\taccu 95.312 (90.891)\n",
      "Epoch-33  320 batches\tloss 1.1549 (1.1148)\taccu 89.062 (90.942)\n",
      "Epoch-33  340 batches\tloss 1.0325 (1.1157)\taccu 93.750 (90.919)\n",
      "Epoch-33  360 batches\tloss 1.1185 (1.1152)\taccu 93.750 (90.920)\n",
      "Epoch-33  380 batches\tloss 1.1827 (1.1166)\taccu 89.062 (90.826)\n",
      "Epoch-33  400 batches\tloss 1.2115 (1.1170)\taccu 85.938 (90.828)\n",
      "Epoch-33  420 batches\tloss 1.0751 (1.1173)\taccu 95.312 (90.811)\n",
      "Epoch-33  440 batches\tloss 1.1068 (1.1169)\taccu 92.188 (90.827)\n",
      "Epoch-33  460 batches\tloss 1.2187 (1.1177)\taccu 87.500 (90.802)\n",
      "Epoch-33  480 batches\tloss 1.1007 (1.1178)\taccu 90.625 (90.817)\n",
      "Epoch-33  128.5s\tTrain: loss 1.1189\taccu 90.7765\tValid: loss 1.3348\taccu 82.5324\n",
      "Epoch 33: val_acc did not improve\n",
      "33 0.001\n",
      "Epoch-34   20 batches\tloss 1.0523 (1.0893)\taccu 93.750 (92.188)\n",
      "Epoch-34   40 batches\tloss 1.0800 (1.1031)\taccu 93.750 (91.484)\n",
      "Epoch-34   60 batches\tloss 1.1005 (1.1000)\taccu 92.188 (91.536)\n",
      "Epoch-34   80 batches\tloss 1.1145 (1.0994)\taccu 95.312 (91.562)\n",
      "Epoch-34  100 batches\tloss 1.1293 (1.0965)\taccu 92.188 (91.656)\n",
      "Epoch-34  120 batches\tloss 1.0172 (1.0986)\taccu 93.750 (91.654)\n",
      "Epoch-34  140 batches\tloss 1.0402 (1.0998)\taccu 93.750 (91.663)\n",
      "Epoch-34  160 batches\tloss 1.0281 (1.1004)\taccu 95.312 (91.611)\n",
      "Epoch-34  180 batches\tloss 1.2501 (1.1003)\taccu 82.812 (91.641)\n",
      "Epoch-34  200 batches\tloss 1.1175 (1.1025)\taccu 89.062 (91.609)\n",
      "Epoch-34  220 batches\tloss 1.0693 (1.1020)\taccu 93.750 (91.598)\n",
      "Epoch-34  240 batches\tloss 1.0235 (1.1011)\taccu 92.188 (91.582)\n",
      "Epoch-34  260 batches\tloss 1.1336 (1.1017)\taccu 89.062 (91.538)\n",
      "Epoch-34  280 batches\tloss 1.1056 (1.1022)\taccu 92.188 (91.546)\n",
      "Epoch-34  300 batches\tloss 1.1424 (1.1027)\taccu 89.062 (91.510)\n",
      "Epoch-34  320 batches\tloss 1.1057 (1.1045)\taccu 90.625 (91.421)\n",
      "Epoch-34  340 batches\tloss 1.0781 (1.1050)\taccu 92.188 (91.411)\n",
      "Epoch-34  360 batches\tloss 1.2627 (1.1063)\taccu 84.375 (91.345)\n",
      "Epoch-34  380 batches\tloss 1.1040 (1.1081)\taccu 92.188 (91.258)\n",
      "Epoch-34  400 batches\tloss 1.0676 (1.1088)\taccu 93.750 (91.234)\n",
      "Epoch-34  420 batches\tloss 1.0602 (1.1090)\taccu 92.188 (91.228)\n",
      "Epoch-34  440 batches\tloss 1.2140 (1.1106)\taccu 87.500 (91.133)\n",
      "Epoch-34  460 batches\tloss 1.0541 (1.1109)\taccu 96.875 (91.097)\n",
      "Epoch-34  480 batches\tloss 1.2464 (1.1119)\taccu 85.938 (91.038)\n",
      "Epoch-34  128.4s\tTrain: loss 1.1134\taccu 90.9912\tValid: loss 1.3321\taccu 82.0460\n",
      "Epoch 34: val_acc did not improve\n",
      "34 0.001\n",
      "Epoch-35   20 batches\tloss 0.9942 (1.0890)\taccu 95.312 (91.406)\n",
      "Epoch-35   40 batches\tloss 1.1221 (1.0964)\taccu 93.750 (91.680)\n",
      "Epoch-35   60 batches\tloss 1.0749 (1.1056)\taccu 89.062 (91.536)\n",
      "Epoch-35   80 batches\tloss 1.0467 (1.1002)\taccu 95.312 (91.777)\n",
      "Epoch-35  100 batches\tloss 1.2644 (1.1050)\taccu 85.938 (91.406)\n",
      "Epoch-35  120 batches\tloss 1.1632 (1.1102)\taccu 87.500 (91.211)\n",
      "Epoch-35  140 batches\tloss 1.0323 (1.1086)\taccu 96.875 (91.239)\n",
      "Epoch-35  160 batches\tloss 1.1875 (1.1095)\taccu 89.062 (91.230)\n",
      "Epoch-35  180 batches\tloss 1.0981 (1.1089)\taccu 92.188 (91.224)\n",
      "Epoch-35  200 batches\tloss 1.0713 (1.1084)\taccu 93.750 (91.289)\n",
      "Epoch-35  220 batches\tloss 1.0219 (1.1070)\taccu 93.750 (91.349)\n",
      "Epoch-35  240 batches\tloss 1.1288 (1.1063)\taccu 90.625 (91.335)\n",
      "Epoch-35  260 batches\tloss 1.1642 (1.1052)\taccu 89.062 (91.418)\n",
      "Epoch-35  280 batches\tloss 1.1726 (1.1060)\taccu 89.062 (91.456)\n",
      "Epoch-35  300 batches\tloss 1.1162 (1.1083)\taccu 90.625 (91.370)\n",
      "Epoch-35  320 batches\tloss 1.0641 (1.1087)\taccu 93.750 (91.304)\n",
      "Epoch-35  340 batches\tloss 1.0852 (1.1090)\taccu 95.312 (91.296)\n",
      "Epoch-35  360 batches\tloss 1.0985 (1.1090)\taccu 93.750 (91.293)\n",
      "Epoch-35  380 batches\tloss 1.1785 (1.1090)\taccu 87.500 (91.299)\n",
      "Epoch-35  400 batches\tloss 1.1539 (1.1098)\taccu 85.938 (91.230)\n",
      "Epoch-35  420 batches\tloss 1.1076 (1.1101)\taccu 93.750 (91.190)\n",
      "Epoch-35  440 batches\tloss 1.2081 (1.1108)\taccu 84.375 (91.190)\n",
      "Epoch-35  460 batches\tloss 1.1815 (1.1113)\taccu 87.500 (91.141)\n",
      "Epoch-35  480 batches\tloss 1.1107 (1.1113)\taccu 90.625 (91.120)\n",
      "Epoch-35  128.1s\tTrain: loss 1.1110\taccu 91.1080\tValid: loss 1.3172\taccu 83.3874\n",
      "Epoch 35: val_acc improved from 83.1221 to 83.3874, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "35 0.001\n",
      "Epoch-36   20 batches\tloss 1.1578 (1.0705)\taccu 89.062 (92.266)\n",
      "Epoch-36   40 batches\tloss 0.9812 (1.0732)\taccu 96.875 (92.344)\n",
      "Epoch-36   60 batches\tloss 1.0514 (1.0689)\taccu 92.188 (92.396)\n",
      "Epoch-36   80 batches\tloss 1.2486 (1.0741)\taccu 87.500 (92.383)\n",
      "Epoch-36  100 batches\tloss 1.2409 (1.0763)\taccu 82.812 (92.438)\n",
      "Epoch-36  120 batches\tloss 1.0652 (1.0755)\taccu 92.188 (92.435)\n",
      "Epoch-36  140 batches\tloss 1.0912 (1.0771)\taccu 89.062 (92.366)\n",
      "Epoch-36  160 batches\tloss 1.1494 (1.0789)\taccu 89.062 (92.383)\n",
      "Epoch-36  180 batches\tloss 1.0916 (1.0818)\taccu 92.188 (92.248)\n",
      "Epoch-36  200 batches\tloss 1.1741 (1.0854)\taccu 90.625 (92.117)\n",
      "Epoch-36  220 batches\tloss 1.0876 (1.0849)\taccu 87.500 (92.138)\n",
      "Epoch-36  240 batches\tloss 1.1445 (1.0847)\taccu 90.625 (92.168)\n",
      "Epoch-36  260 batches\tloss 1.1250 (1.0859)\taccu 89.062 (92.109)\n",
      "Epoch-36  280 batches\tloss 1.1482 (1.0877)\taccu 89.062 (92.048)\n",
      "Epoch-36  300 batches\tloss 0.9843 (1.0876)\taccu 95.312 (92.125)\n",
      "Epoch-36  320 batches\tloss 1.1025 (1.0880)\taccu 92.188 (92.109)\n",
      "Epoch-36  340 batches\tloss 1.0442 (1.0891)\taccu 90.625 (92.004)\n",
      "Epoch-36  360 batches\tloss 1.1514 (1.0901)\taccu 84.375 (91.923)\n",
      "Epoch-36  380 batches\tloss 1.1787 (1.0924)\taccu 87.500 (91.854)\n",
      "Epoch-36  400 batches\tloss 1.1013 (1.0943)\taccu 93.750 (91.770)\n",
      "Epoch-36  420 batches\tloss 1.1986 (1.0958)\taccu 85.938 (91.708)\n",
      "Epoch-36  440 batches\tloss 1.1167 (1.0975)\taccu 89.062 (91.644)\n",
      "Epoch-36  460 batches\tloss 1.0795 (1.0989)\taccu 92.188 (91.617)\n",
      "Epoch-36  480 batches\tloss 1.2679 (1.1004)\taccu 85.938 (91.536)\n",
      "Epoch-36  128.1s\tTrain: loss 1.1002\taccu 91.5278\tValid: loss 1.3163\taccu 83.2400\n",
      "Epoch 36: val_acc did not improve\n",
      "36 0.001\n",
      "Epoch-37   20 batches\tloss 1.0600 (1.0779)\taccu 93.750 (92.578)\n",
      "Epoch-37   40 batches\tloss 1.0498 (1.0688)\taccu 92.188 (93.125)\n",
      "Epoch-37   60 batches\tloss 1.1698 (1.0718)\taccu 85.938 (92.865)\n",
      "Epoch-37   80 batches\tloss 1.0374 (1.0718)\taccu 93.750 (92.949)\n",
      "Epoch-37  100 batches\tloss 1.1199 (1.0708)\taccu 89.062 (92.969)\n",
      "Epoch-37  120 batches\tloss 1.0754 (1.0717)\taccu 90.625 (92.813)\n",
      "Epoch-37  140 batches\tloss 1.0830 (1.0783)\taccu 93.750 (92.623)\n",
      "Epoch-37  160 batches\tloss 1.0864 (1.0789)\taccu 90.625 (92.637)\n",
      "Epoch-37  180 batches\tloss 1.1545 (1.0817)\taccu 87.500 (92.483)\n",
      "Epoch-37  200 batches\tloss 1.2221 (1.0835)\taccu 84.375 (92.305)\n",
      "Epoch-37  220 batches\tloss 1.0878 (1.0844)\taccu 90.625 (92.259)\n",
      "Epoch-37  240 batches\tloss 1.0592 (1.0833)\taccu 92.188 (92.298)\n",
      "Epoch-37  260 batches\tloss 1.1370 (1.0864)\taccu 87.500 (92.139)\n",
      "Epoch-37  280 batches\tloss 1.0553 (1.0853)\taccu 92.188 (92.176)\n",
      "Epoch-37  300 batches\tloss 1.2069 (1.0867)\taccu 89.062 (92.146)\n",
      "Epoch-37  320 batches\tloss 1.0611 (1.0865)\taccu 93.750 (92.163)\n",
      "Epoch-37  340 batches\tloss 1.0601 (1.0862)\taccu 90.625 (92.165)\n",
      "Epoch-37  360 batches\tloss 1.1083 (1.0873)\taccu 89.062 (92.148)\n",
      "Epoch-37  380 batches\tloss 1.0355 (1.0893)\taccu 93.750 (92.076)\n",
      "Epoch-37  400 batches\tloss 1.1071 (1.0906)\taccu 92.188 (92.012)\n",
      "Epoch-37  420 batches\tloss 1.0277 (1.0910)\taccu 93.750 (91.994)\n",
      "Epoch-37  440 batches\tloss 1.0628 (1.0918)\taccu 93.750 (91.950)\n",
      "Epoch-37  460 batches\tloss 1.0304 (1.0927)\taccu 92.188 (91.919)\n",
      "Epoch-37  480 batches\tloss 1.1665 (1.0953)\taccu 92.188 (91.816)\n",
      "Epoch-37  122.3s\tTrain: loss 1.0959\taccu 91.7866\tValid: loss 1.3144\taccu 83.0336\n",
      "Epoch 37: val_acc did not improve\n",
      "37 0.001\n",
      "Epoch-38   20 batches\tloss 1.1061 (1.0597)\taccu 92.188 (93.828)\n",
      "Epoch-38   40 batches\tloss 1.0624 (1.0662)\taccu 93.750 (93.320)\n",
      "Epoch-38   60 batches\tloss 0.9762 (1.0704)\taccu 98.438 (93.177)\n",
      "Epoch-38   80 batches\tloss 1.0875 (1.0702)\taccu 92.188 (93.164)\n",
      "Epoch-38  100 batches\tloss 1.1065 (1.0714)\taccu 89.062 (93.062)\n",
      "Epoch-38  120 batches\tloss 1.0310 (1.0699)\taccu 93.750 (93.216)\n",
      "Epoch-38  140 batches\tloss 1.0595 (1.0689)\taccu 95.312 (93.281)\n",
      "Epoch-38  160 batches\tloss 1.0692 (1.0733)\taccu 93.750 (93.057)\n",
      "Epoch-38  180 batches\tloss 1.1329 (1.0740)\taccu 90.625 (92.977)\n",
      "Epoch-38  200 batches\tloss 1.1479 (1.0792)\taccu 92.188 (92.727)\n",
      "Epoch-38  220 batches\tloss 1.0540 (1.0796)\taccu 92.188 (92.685)\n",
      "Epoch-38  240 batches\tloss 1.1018 (1.0797)\taccu 90.625 (92.682)\n",
      "Epoch-38  260 batches\tloss 1.0662 (1.0802)\taccu 90.625 (92.626)\n",
      "Epoch-38  280 batches\tloss 1.0940 (1.0812)\taccu 90.625 (92.556)\n",
      "Epoch-38  300 batches\tloss 1.1236 (1.0823)\taccu 89.062 (92.464)\n",
      "Epoch-38  320 batches\tloss 1.1187 (1.0842)\taccu 92.188 (92.383)\n",
      "Epoch-38  340 batches\tloss 1.1046 (1.0849)\taccu 90.625 (92.344)\n",
      "Epoch-38  360 batches\tloss 1.0881 (1.0856)\taccu 90.625 (92.313)\n",
      "Epoch-38  380 batches\tloss 1.1360 (1.0870)\taccu 90.625 (92.245)\n",
      "Epoch-38  400 batches\tloss 1.1431 (1.0890)\taccu 89.062 (92.168)\n",
      "Epoch-38  420 batches\tloss 1.0650 (1.0901)\taccu 92.188 (92.117)\n",
      "Epoch-38  440 batches\tloss 1.0861 (1.0922)\taccu 92.188 (92.003)\n",
      "Epoch-38  460 batches\tloss 1.0285 (1.0936)\taccu 95.312 (91.919)\n",
      "Epoch-38  480 batches\tloss 1.1475 (1.0946)\taccu 89.062 (91.872)\n",
      "Epoch-38  93.7s\tTrain: loss 1.0951\taccu 91.8750\tValid: loss 1.3483\taccu 82.1639\n",
      "Epoch 38: val_acc did not improve\n",
      "38 0.001\n",
      "Epoch-39   20 batches\tloss 1.1169 (1.0851)\taccu 89.062 (92.031)\n",
      "Epoch-39   40 batches\tloss 1.1215 (1.0703)\taccu 90.625 (92.773)\n",
      "Epoch-39   60 batches\tloss 1.1359 (1.0700)\taccu 89.062 (92.656)\n",
      "Epoch-39   80 batches\tloss 1.0049 (1.0701)\taccu 95.312 (92.695)\n",
      "Epoch-39  100 batches\tloss 0.9808 (1.0659)\taccu 95.312 (92.938)\n",
      "Epoch-39  120 batches\tloss 1.0363 (1.0681)\taccu 93.750 (92.917)\n",
      "Epoch-39  140 batches\tloss 1.0344 (1.0668)\taccu 95.312 (93.025)\n",
      "Epoch-39  160 batches\tloss 1.1185 (1.0720)\taccu 90.625 (92.783)\n",
      "Epoch-39  180 batches\tloss 1.2168 (1.0719)\taccu 87.500 (92.821)\n",
      "Epoch-39  200 batches\tloss 1.1138 (1.0731)\taccu 92.188 (92.758)\n",
      "Epoch-39  220 batches\tloss 1.0872 (1.0756)\taccu 90.625 (92.628)\n",
      "Epoch-39  240 batches\tloss 1.1749 (1.0775)\taccu 87.500 (92.493)\n",
      "Epoch-39  260 batches\tloss 1.0972 (1.0797)\taccu 90.625 (92.344)\n",
      "Epoch-39  280 batches\tloss 1.1125 (1.0799)\taccu 89.062 (92.355)\n",
      "Epoch-39  300 batches\tloss 1.2206 (1.0820)\taccu 87.500 (92.203)\n",
      "Epoch-39  320 batches\tloss 0.9906 (1.0834)\taccu 95.312 (92.119)\n",
      "Epoch-39  340 batches\tloss 1.0845 (1.0836)\taccu 90.625 (92.096)\n",
      "Epoch-39  360 batches\tloss 1.0803 (1.0855)\taccu 93.750 (91.979)\n",
      "Epoch-39  380 batches\tloss 1.1588 (1.0871)\taccu 93.750 (91.937)\n",
      "Epoch-39  400 batches\tloss 0.9889 (1.0873)\taccu 95.312 (91.926)\n",
      "Epoch-39  420 batches\tloss 1.1358 (1.0877)\taccu 90.625 (91.908)\n",
      "Epoch-39  440 batches\tloss 1.1695 (1.0884)\taccu 90.625 (91.900)\n",
      "Epoch-39  460 batches\tloss 1.0182 (1.0888)\taccu 96.875 (91.919)\n",
      "Epoch-39  480 batches\tloss 1.2340 (1.0899)\taccu 87.500 (91.888)\n",
      "Epoch-39  93.4s\tTrain: loss 1.0907\taccu 91.8529\tValid: loss 1.3065\taccu 83.5201\n",
      "Epoch 39: val_acc improved from 83.3874 to 83.5201, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "39 0.001\n",
      "Epoch-40   20 batches\tloss 1.0221 (1.0498)\taccu 93.750 (93.984)\n",
      "Epoch-40   40 batches\tloss 1.0700 (1.0421)\taccu 92.188 (94.453)\n",
      "Epoch-40   60 batches\tloss 1.0883 (1.0498)\taccu 90.625 (93.880)\n",
      "Epoch-40   80 batches\tloss 1.0412 (1.0575)\taccu 95.312 (93.320)\n",
      "Epoch-40  100 batches\tloss 1.1477 (1.0648)\taccu 90.625 (92.969)\n",
      "Epoch-40  120 batches\tloss 1.0414 (1.0646)\taccu 95.312 (93.125)\n",
      "Epoch-40  140 batches\tloss 1.0647 (1.0640)\taccu 92.188 (93.025)\n",
      "Epoch-40  160 batches\tloss 1.0887 (1.0629)\taccu 90.625 (93.096)\n",
      "Epoch-40  180 batches\tloss 1.0919 (1.0608)\taccu 90.625 (93.203)\n",
      "Epoch-40  200 batches\tloss 1.0096 (1.0636)\taccu 92.188 (93.047)\n",
      "Epoch-40  220 batches\tloss 1.0157 (1.0649)\taccu 96.875 (92.969)\n",
      "Epoch-40  240 batches\tloss 1.0895 (1.0668)\taccu 92.188 (92.839)\n",
      "Epoch-40  260 batches\tloss 1.0315 (1.0689)\taccu 93.750 (92.764)\n",
      "Epoch-40  280 batches\tloss 1.2359 (1.0713)\taccu 85.938 (92.645)\n",
      "Epoch-40  300 batches\tloss 1.2585 (1.0739)\taccu 84.375 (92.542)\n",
      "Epoch-40  320 batches\tloss 1.0795 (1.0767)\taccu 92.188 (92.412)\n",
      "Epoch-40  340 batches\tloss 1.1171 (1.0777)\taccu 92.188 (92.316)\n",
      "Epoch-40  360 batches\tloss 1.1468 (1.0798)\taccu 89.062 (92.244)\n",
      "Epoch-40  380 batches\tloss 1.0668 (1.0797)\taccu 93.750 (92.253)\n",
      "Epoch-40  400 batches\tloss 1.1077 (1.0808)\taccu 93.750 (92.246)\n",
      "Epoch-40  420 batches\tloss 1.0668 (1.0815)\taccu 89.062 (92.225)\n",
      "Epoch-40  440 batches\tloss 1.0818 (1.0828)\taccu 90.625 (92.223)\n",
      "Epoch-40  460 batches\tloss 1.2356 (1.0839)\taccu 84.375 (92.181)\n",
      "Epoch-40  480 batches\tloss 1.0844 (1.0855)\taccu 90.625 (92.122)\n",
      "Epoch-40  123.8s\tTrain: loss 1.0867\taccu 92.0865\tValid: loss 1.3208\taccu 82.9894\n",
      "Epoch 40: val_acc did not improve\n",
      "40 0.001\n",
      "Epoch-41   20 batches\tloss 1.1188 (1.0597)\taccu 92.188 (93.203)\n",
      "Epoch-41   40 batches\tloss 1.0966 (1.0653)\taccu 87.500 (92.773)\n",
      "Epoch-41   60 batches\tloss 1.0994 (1.0688)\taccu 92.188 (92.943)\n",
      "Epoch-41   80 batches\tloss 1.0441 (1.0725)\taccu 93.750 (92.734)\n",
      "Epoch-41  100 batches\tloss 1.0069 (1.0705)\taccu 96.875 (92.750)\n",
      "Epoch-41  120 batches\tloss 1.1840 (1.0738)\taccu 85.938 (92.591)\n",
      "Epoch-41  140 batches\tloss 1.0829 (1.0733)\taccu 89.062 (92.600)\n",
      "Epoch-41  160 batches\tloss 1.1332 (1.0748)\taccu 89.062 (92.510)\n",
      "Epoch-41  180 batches\tloss 1.0892 (1.0742)\taccu 93.750 (92.483)\n",
      "Epoch-41  200 batches\tloss 0.9702 (1.0725)\taccu 95.312 (92.555)\n",
      "Epoch-41  220 batches\tloss 1.2167 (1.0732)\taccu 84.375 (92.578)\n",
      "Epoch-41  240 batches\tloss 1.0595 (1.0734)\taccu 92.188 (92.559)\n",
      "Epoch-41  260 batches\tloss 0.9988 (1.0737)\taccu 95.312 (92.560)\n",
      "Epoch-41  280 batches\tloss 1.0642 (1.0755)\taccu 95.312 (92.506)\n",
      "Epoch-41  300 batches\tloss 1.0777 (1.0758)\taccu 92.188 (92.510)\n",
      "Epoch-41  320 batches\tloss 1.0332 (1.0763)\taccu 92.188 (92.500)\n",
      "Epoch-41  340 batches\tloss 1.0982 (1.0779)\taccu 92.188 (92.413)\n",
      "Epoch-41  360 batches\tloss 1.0162 (1.0784)\taccu 93.750 (92.405)\n",
      "Epoch-41  380 batches\tloss 1.1148 (1.0802)\taccu 93.750 (92.319)\n",
      "Epoch-41  400 batches\tloss 1.2058 (1.0813)\taccu 89.062 (92.281)\n",
      "Epoch-41  420 batches\tloss 1.0668 (1.0824)\taccu 93.750 (92.214)\n",
      "Epoch-41  440 batches\tloss 1.0934 (1.0833)\taccu 90.625 (92.163)\n",
      "Epoch-41  460 batches\tloss 1.2321 (1.0853)\taccu 85.938 (92.055)\n",
      "Epoch-41  480 batches\tloss 1.0449 (1.0855)\taccu 92.188 (92.064)\n",
      "Epoch-41  127.6s\tTrain: loss 1.0860\taccu 92.0581\tValid: loss 1.3057\taccu 83.1515\n",
      "Epoch 41: val_acc did not improve\n",
      "41 0.001\n",
      "Epoch-42   20 batches\tloss 1.0283 (1.0575)\taccu 92.188 (93.672)\n",
      "Epoch-42   40 batches\tloss 1.0902 (1.0503)\taccu 90.625 (93.789)\n",
      "Epoch-42   60 batches\tloss 1.0465 (1.0524)\taccu 92.188 (93.594)\n",
      "Epoch-42   80 batches\tloss 1.0574 (1.0569)\taccu 92.188 (93.457)\n",
      "Epoch-42  100 batches\tloss 1.0654 (1.0565)\taccu 89.062 (93.531)\n",
      "Epoch-42  120 batches\tloss 1.1372 (1.0591)\taccu 89.062 (93.359)\n",
      "Epoch-42  140 batches\tloss 1.1330 (1.0589)\taccu 89.062 (93.270)\n",
      "Epoch-42  160 batches\tloss 1.1252 (1.0607)\taccu 87.500 (93.174)\n",
      "Epoch-42  180 batches\tloss 1.0273 (1.0637)\taccu 96.875 (93.056)\n",
      "Epoch-42  200 batches\tloss 1.1608 (1.0631)\taccu 87.500 (93.055)\n",
      "Epoch-42  220 batches\tloss 0.9842 (1.0642)\taccu 100.000 (93.040)\n",
      "Epoch-42  240 batches\tloss 1.0612 (1.0661)\taccu 95.312 (92.956)\n",
      "Epoch-42  260 batches\tloss 1.0763 (1.0667)\taccu 90.625 (92.951)\n",
      "Epoch-42  280 batches\tloss 1.1100 (1.0673)\taccu 92.188 (92.907)\n",
      "Epoch-42  300 batches\tloss 0.9913 (1.0686)\taccu 92.188 (92.818)\n",
      "Epoch-42  320 batches\tloss 1.1783 (1.0699)\taccu 85.938 (92.749)\n",
      "Epoch-42  340 batches\tloss 1.1693 (1.0726)\taccu 82.812 (92.610)\n",
      "Epoch-42  360 batches\tloss 1.0551 (1.0751)\taccu 93.750 (92.470)\n",
      "Epoch-42  380 batches\tloss 1.1106 (1.0751)\taccu 92.188 (92.500)\n",
      "Epoch-42  400 batches\tloss 1.0291 (1.0765)\taccu 98.438 (92.445)\n",
      "Epoch-42  420 batches\tloss 1.0737 (1.0777)\taccu 93.750 (92.333)\n",
      "Epoch-42  440 batches\tloss 1.1165 (1.0779)\taccu 87.500 (92.337)\n",
      "Epoch-42  460 batches\tloss 1.1826 (1.0791)\taccu 90.625 (92.317)\n",
      "Epoch-42  480 batches\tloss 1.0737 (1.0792)\taccu 93.750 (92.292)\n",
      "Epoch-42  127.7s\tTrain: loss 1.0801\taccu 92.2412\tValid: loss 1.3210\taccu 83.3137\n",
      "Epoch 42: val_acc did not improve\n",
      "42 0.001\n",
      "Epoch-43   20 batches\tloss 1.0168 (1.0592)\taccu 93.750 (93.750)\n",
      "Epoch-43   40 batches\tloss 1.0259 (1.0497)\taccu 95.312 (94.141)\n",
      "Epoch-43   60 batches\tloss 1.0513 (1.0481)\taccu 90.625 (93.932)\n",
      "Epoch-43   80 batches\tloss 1.1101 (1.0506)\taccu 92.188 (93.633)\n",
      "Epoch-43  100 batches\tloss 1.0226 (1.0526)\taccu 95.312 (93.438)\n",
      "Epoch-43  120 batches\tloss 1.1271 (1.0536)\taccu 87.500 (93.490)\n",
      "Epoch-43  140 batches\tloss 1.0560 (1.0548)\taccu 92.188 (93.438)\n",
      "Epoch-43  160 batches\tloss 1.0697 (1.0546)\taccu 93.750 (93.408)\n",
      "Epoch-43  180 batches\tloss 1.0762 (1.0557)\taccu 92.188 (93.333)\n",
      "Epoch-43  200 batches\tloss 0.9837 (1.0570)\taccu 98.438 (93.234)\n",
      "Epoch-43  220 batches\tloss 1.1322 (1.0598)\taccu 90.625 (93.139)\n",
      "Epoch-43  240 batches\tloss 0.9776 (1.0592)\taccu 98.438 (93.197)\n",
      "Epoch-43  260 batches\tloss 1.2195 (1.0607)\taccu 87.500 (93.155)\n",
      "Epoch-43  280 batches\tloss 1.1846 (1.0629)\taccu 84.375 (93.058)\n",
      "Epoch-43  300 batches\tloss 1.1142 (1.0634)\taccu 92.188 (93.010)\n",
      "Epoch-43  320 batches\tloss 1.1160 (1.0644)\taccu 85.938 (92.930)\n",
      "Epoch-43  340 batches\tloss 1.0647 (1.0643)\taccu 92.188 (92.946)\n",
      "Epoch-43  360 batches\tloss 1.0817 (1.0669)\taccu 92.188 (92.843)\n",
      "Epoch-43  380 batches\tloss 1.1514 (1.0688)\taccu 89.062 (92.808)\n",
      "Epoch-43  400 batches\tloss 1.0739 (1.0695)\taccu 92.188 (92.738)\n",
      "Epoch-43  420 batches\tloss 1.0990 (1.0703)\taccu 92.188 (92.708)\n",
      "Epoch-43  440 batches\tloss 1.0771 (1.0712)\taccu 93.750 (92.720)\n",
      "Epoch-43  460 batches\tloss 1.1024 (1.0733)\taccu 89.062 (92.612)\n",
      "Epoch-43  480 batches\tloss 1.1765 (1.0738)\taccu 82.812 (92.572)\n",
      "Epoch-43  127.6s\tTrain: loss 1.0744\taccu 92.5537\tValid: loss 1.3044\taccu 83.4906\n",
      "Epoch 43: val_acc did not improve\n",
      "43 0.001\n",
      "Epoch-44   20 batches\tloss 1.0281 (1.0393)\taccu 95.312 (93.516)\n",
      "Epoch-44   40 batches\tloss 1.0581 (1.0388)\taccu 92.188 (93.672)\n",
      "Epoch-44   60 batches\tloss 1.0787 (1.0449)\taccu 90.625 (93.307)\n",
      "Epoch-44   80 batches\tloss 1.1104 (1.0497)\taccu 95.312 (93.418)\n",
      "Epoch-44  100 batches\tloss 1.0555 (1.0491)\taccu 90.625 (93.594)\n",
      "Epoch-44  120 batches\tloss 1.0836 (1.0480)\taccu 90.625 (93.620)\n",
      "Epoch-44  140 batches\tloss 1.1574 (1.0500)\taccu 89.062 (93.627)\n",
      "Epoch-44  160 batches\tloss 1.0215 (1.0505)\taccu 95.312 (93.613)\n",
      "Epoch-44  180 batches\tloss 1.1521 (1.0522)\taccu 90.625 (93.568)\n",
      "Epoch-44  200 batches\tloss 1.1248 (1.0557)\taccu 90.625 (93.336)\n",
      "Epoch-44  220 batches\tloss 1.1316 (1.0590)\taccu 93.750 (93.260)\n",
      "Epoch-44  240 batches\tloss 0.9591 (1.0621)\taccu 98.438 (93.092)\n",
      "Epoch-44  260 batches\tloss 1.1826 (1.0643)\taccu 89.062 (93.029)\n",
      "Epoch-44  280 batches\tloss 1.1424 (1.0663)\taccu 90.625 (92.935)\n",
      "Epoch-44  300 batches\tloss 1.0675 (1.0678)\taccu 96.875 (92.943)\n",
      "Epoch-44  320 batches\tloss 1.1041 (1.0687)\taccu 90.625 (92.949)\n",
      "Epoch-44  340 batches\tloss 1.0630 (1.0695)\taccu 93.750 (92.904)\n",
      "Epoch-44  360 batches\tloss 1.0599 (1.0712)\taccu 92.188 (92.830)\n",
      "Epoch-44  380 batches\tloss 1.2569 (1.0742)\taccu 84.375 (92.710)\n",
      "Epoch-44  400 batches\tloss 1.0707 (1.0759)\taccu 96.875 (92.641)\n",
      "Epoch-44  420 batches\tloss 1.0487 (1.0751)\taccu 95.312 (92.712)\n",
      "Epoch-44  440 batches\tloss 1.1942 (1.0756)\taccu 84.375 (92.674)\n",
      "Epoch-44  460 batches\tloss 1.0605 (1.0761)\taccu 93.750 (92.646)\n",
      "Epoch-44  480 batches\tloss 1.0085 (1.0759)\taccu 93.750 (92.643)\n",
      "Epoch-44  127.9s\tTrain: loss 1.0763\taccu 92.6168\tValid: loss 1.3149\taccu 83.2252\n",
      "Epoch 44: val_acc did not improve\n",
      "44 0.001\n",
      "Epoch-45   20 batches\tloss 0.9725 (1.0518)\taccu 93.750 (93.594)\n",
      "Epoch-45   40 batches\tloss 0.9908 (1.0396)\taccu 93.750 (94.102)\n",
      "Epoch-45   60 batches\tloss 1.0397 (1.0458)\taccu 95.312 (93.932)\n",
      "Epoch-45   80 batches\tloss 1.0692 (1.0431)\taccu 93.750 (93.926)\n",
      "Epoch-45  100 batches\tloss 1.0179 (1.0460)\taccu 95.312 (93.672)\n",
      "Epoch-45  120 batches\tloss 0.9965 (1.0438)\taccu 98.438 (93.737)\n",
      "Epoch-45  140 batches\tloss 1.0568 (1.0449)\taccu 92.188 (93.728)\n",
      "Epoch-45  160 batches\tloss 1.1000 (1.0466)\taccu 93.750 (93.740)\n",
      "Epoch-45  180 batches\tloss 0.9926 (1.0453)\taccu 95.312 (93.793)\n",
      "Epoch-45  200 batches\tloss 1.0971 (1.0477)\taccu 90.625 (93.617)\n",
      "Epoch-45  220 batches\tloss 1.0913 (1.0501)\taccu 93.750 (93.509)\n",
      "Epoch-45  240 batches\tloss 1.1308 (1.0536)\taccu 87.500 (93.353)\n",
      "Epoch-45  260 batches\tloss 1.0880 (1.0538)\taccu 92.188 (93.341)\n",
      "Epoch-45  280 batches\tloss 1.0835 (1.0566)\taccu 90.625 (93.231)\n",
      "Epoch-45  300 batches\tloss 1.1446 (1.0601)\taccu 85.938 (93.115)\n",
      "Epoch-45  320 batches\tloss 1.1263 (1.0613)\taccu 89.062 (93.047)\n",
      "Epoch-45  340 batches\tloss 1.1728 (1.0622)\taccu 90.625 (93.024)\n",
      "Epoch-45  360 batches\tloss 1.2154 (1.0641)\taccu 85.938 (92.917)\n",
      "Epoch-45  380 batches\tloss 1.0073 (1.0644)\taccu 95.312 (92.891)\n",
      "Epoch-45  400 batches\tloss 0.9790 (1.0654)\taccu 95.312 (92.812)\n",
      "Epoch-45  420 batches\tloss 1.0580 (1.0661)\taccu 90.625 (92.779)\n",
      "Epoch-45  440 batches\tloss 1.0554 (1.0673)\taccu 95.312 (92.738)\n",
      "Epoch-45  460 batches\tloss 1.0418 (1.0696)\taccu 95.312 (92.643)\n",
      "Epoch-45  480 batches\tloss 1.0681 (1.0710)\taccu 93.750 (92.594)\n",
      "Epoch-45  127.9s\tTrain: loss 1.0712\taccu 92.6042\tValid: loss 1.3099\taccu 83.2842\n",
      "Epoch 45: val_acc did not improve\n",
      "45 0.001\n",
      "Epoch-46   20 batches\tloss 0.9837 (1.0281)\taccu 95.312 (94.531)\n",
      "Epoch-46   40 batches\tloss 1.0582 (1.0312)\taccu 89.062 (94.531)\n",
      "Epoch-46   60 batches\tloss 1.0863 (1.0322)\taccu 90.625 (94.375)\n",
      "Epoch-46   80 batches\tloss 1.0413 (1.0347)\taccu 89.062 (94.277)\n",
      "Epoch-46  100 batches\tloss 1.0655 (1.0418)\taccu 92.188 (93.984)\n",
      "Epoch-46  120 batches\tloss 1.0923 (1.0444)\taccu 89.062 (93.828)\n",
      "Epoch-46  140 batches\tloss 1.0719 (1.0442)\taccu 90.625 (93.783)\n",
      "Epoch-46  160 batches\tloss 1.0713 (1.0456)\taccu 85.938 (93.662)\n",
      "Epoch-46  180 batches\tloss 1.2325 (1.0497)\taccu 93.750 (93.568)\n",
      "Epoch-46  200 batches\tloss 1.0517 (1.0533)\taccu 92.188 (93.398)\n",
      "Epoch-46  220 batches\tloss 1.1165 (1.0538)\taccu 90.625 (93.381)\n",
      "Epoch-46  240 batches\tloss 1.0594 (1.0548)\taccu 95.312 (93.418)\n",
      "Epoch-46  260 batches\tloss 1.1820 (1.0584)\taccu 90.625 (93.191)\n",
      "Epoch-46  280 batches\tloss 1.0493 (1.0598)\taccu 93.750 (93.153)\n",
      "Epoch-46  300 batches\tloss 1.0019 (1.0603)\taccu 98.438 (93.083)\n",
      "Epoch-46  320 batches\tloss 1.0387 (1.0609)\taccu 95.312 (93.086)\n",
      "Epoch-46  340 batches\tloss 1.1167 (1.0614)\taccu 90.625 (93.038)\n",
      "Epoch-46  360 batches\tloss 1.0226 (1.0638)\taccu 96.875 (92.964)\n",
      "Epoch-46  380 batches\tloss 1.0357 (1.0630)\taccu 95.312 (93.026)\n",
      "Epoch-46  400 batches\tloss 1.1859 (1.0638)\taccu 87.500 (92.984)\n",
      "Epoch-46  420 batches\tloss 1.1602 (1.0642)\taccu 90.625 (92.961)\n",
      "Epoch-46  440 batches\tloss 1.1148 (1.0645)\taccu 90.625 (92.940)\n",
      "Epoch-46  460 batches\tloss 1.1478 (1.0650)\taccu 89.062 (92.928)\n",
      "Epoch-46  480 batches\tloss 1.0422 (1.0650)\taccu 95.312 (92.933)\n",
      "Epoch-46  128.4s\tTrain: loss 1.0649\taccu 92.9230\tValid: loss 1.3204\taccu 82.6651\n",
      "Epoch 46: val_acc did not improve\n",
      "46 0.001\n",
      "Epoch-47   20 batches\tloss 1.0079 (1.0287)\taccu 96.875 (94.766)\n",
      "Epoch-47   40 batches\tloss 1.0120 (1.0379)\taccu 93.750 (93.906)\n",
      "Epoch-47   60 batches\tloss 1.1007 (1.0302)\taccu 90.625 (94.297)\n",
      "Epoch-47   80 batches\tloss 1.0622 (1.0285)\taccu 92.188 (94.453)\n",
      "Epoch-47  100 batches\tloss 0.9878 (1.0322)\taccu 96.875 (94.250)\n",
      "Epoch-47  120 batches\tloss 1.0057 (1.0372)\taccu 95.312 (94.036)\n",
      "Epoch-47  140 batches\tloss 1.1243 (1.0390)\taccu 89.062 (93.996)\n",
      "Epoch-47  160 batches\tloss 1.1222 (1.0397)\taccu 95.312 (93.984)\n",
      "Epoch-47  180 batches\tloss 1.1352 (1.0439)\taccu 93.750 (93.854)\n",
      "Epoch-47  200 batches\tloss 0.9801 (1.0437)\taccu 95.312 (93.820)\n",
      "Epoch-47  220 batches\tloss 0.9477 (1.0474)\taccu 98.438 (93.722)\n",
      "Epoch-47  240 batches\tloss 1.0096 (1.0492)\taccu 96.875 (93.581)\n",
      "Epoch-47  260 batches\tloss 0.9703 (1.0502)\taccu 96.875 (93.492)\n",
      "Epoch-47  280 batches\tloss 1.1041 (1.0519)\taccu 92.188 (93.415)\n",
      "Epoch-47  300 batches\tloss 1.0670 (1.0516)\taccu 90.625 (93.375)\n",
      "Epoch-47  320 batches\tloss 1.0574 (1.0525)\taccu 90.625 (93.330)\n",
      "Epoch-47  340 batches\tloss 1.1123 (1.0538)\taccu 89.062 (93.281)\n",
      "Epoch-47  360 batches\tloss 1.0109 (1.0547)\taccu 93.750 (93.234)\n",
      "Epoch-47  380 batches\tloss 0.9839 (1.0560)\taccu 98.438 (93.162)\n",
      "Epoch-47  400 batches\tloss 1.0289 (1.0575)\taccu 95.312 (93.105)\n",
      "Epoch-47  420 batches\tloss 1.0761 (1.0587)\taccu 93.750 (93.069)\n",
      "Epoch-47  440 batches\tloss 1.0439 (1.0614)\taccu 93.750 (92.926)\n",
      "Epoch-47  460 batches\tloss 1.1120 (1.0632)\taccu 90.625 (92.853)\n",
      "Epoch-47  480 batches\tloss 1.1008 (1.0640)\taccu 92.188 (92.848)\n",
      "Epoch-47  118.7s\tTrain: loss 1.0646\taccu 92.8314\tValid: loss 1.3028\taccu 84.0360\n",
      "Epoch 47: val_acc improved from 83.5201 to 84.0360, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "47 0.001\n",
      "Epoch-48   20 batches\tloss 1.2823 (1.0665)\taccu 84.375 (92.734)\n",
      "Epoch-48   40 batches\tloss 1.0236 (1.0372)\taccu 95.312 (94.023)\n",
      "Epoch-48   60 batches\tloss 1.0671 (1.0350)\taccu 92.188 (94.141)\n",
      "Epoch-48   80 batches\tloss 1.0513 (1.0337)\taccu 93.750 (94.199)\n",
      "Epoch-48  100 batches\tloss 1.0264 (1.0351)\taccu 96.875 (94.125)\n",
      "Epoch-48  120 batches\tloss 1.0986 (1.0374)\taccu 92.188 (93.906)\n",
      "Epoch-48  140 batches\tloss 1.0597 (1.0383)\taccu 93.750 (93.929)\n",
      "Epoch-48  160 batches\tloss 0.9557 (1.0388)\taccu 98.438 (93.994)\n",
      "Epoch-48  180 batches\tloss 0.9881 (1.0399)\taccu 98.438 (93.906)\n",
      "Epoch-48  200 batches\tloss 1.0783 (1.0396)\taccu 93.750 (93.914)\n",
      "Epoch-48  220 batches\tloss 1.0790 (1.0409)\taccu 90.625 (93.913)\n",
      "Epoch-48  240 batches\tloss 1.0173 (1.0429)\taccu 96.875 (93.848)\n",
      "Epoch-48  260 batches\tloss 1.1149 (1.0431)\taccu 90.625 (93.858)\n",
      "Epoch-48  280 batches\tloss 1.1761 (1.0464)\taccu 85.938 (93.739)\n",
      "Epoch-48  300 batches\tloss 1.0730 (1.0488)\taccu 92.188 (93.620)\n",
      "Epoch-48  320 batches\tloss 1.1387 (1.0498)\taccu 90.625 (93.496)\n",
      "Epoch-48  340 batches\tloss 0.9970 (1.0512)\taccu 93.750 (93.470)\n",
      "Epoch-48  360 batches\tloss 1.0761 (1.0528)\taccu 93.750 (93.359)\n",
      "Epoch-48  380 batches\tloss 0.9890 (1.0536)\taccu 93.750 (93.294)\n",
      "Epoch-48  400 batches\tloss 1.0114 (1.0544)\taccu 98.438 (93.266)\n",
      "Epoch-48  420 batches\tloss 1.1522 (1.0575)\taccu 87.500 (93.166)\n",
      "Epoch-48  440 batches\tloss 1.0703 (1.0592)\taccu 92.188 (93.111)\n",
      "Epoch-48  460 batches\tloss 1.1257 (1.0608)\taccu 85.938 (93.006)\n",
      "Epoch-48  480 batches\tloss 0.9924 (1.0614)\taccu 95.312 (92.982)\n",
      "Epoch-48  93.3s\tTrain: loss 1.0621\taccu 92.9388\tValid: loss 1.3097\taccu 83.0778\n",
      "Epoch 48: val_acc did not improve\n",
      "48 0.001\n",
      "Epoch-49   20 batches\tloss 1.0000 (1.0524)\taccu 95.312 (92.656)\n",
      "Epoch-49   40 batches\tloss 0.9536 (1.0463)\taccu 100.000 (93.320)\n",
      "Epoch-49   60 batches\tloss 1.0499 (1.0417)\taccu 90.625 (93.359)\n",
      "Epoch-49   80 batches\tloss 0.9713 (1.0392)\taccu 100.000 (93.555)\n",
      "Epoch-49  100 batches\tloss 1.0419 (1.0412)\taccu 92.188 (93.562)\n",
      "Epoch-49  120 batches\tloss 1.1440 (1.0406)\taccu 89.062 (93.542)\n",
      "Epoch-49  140 batches\tloss 1.0606 (1.0432)\taccu 90.625 (93.404)\n",
      "Epoch-49  160 batches\tloss 1.0003 (1.0443)\taccu 96.875 (93.340)\n",
      "Epoch-49  180 batches\tloss 1.0575 (1.0443)\taccu 93.750 (93.394)\n",
      "Epoch-49  200 batches\tloss 1.0614 (1.0463)\taccu 93.750 (93.406)\n",
      "Epoch-49  220 batches\tloss 0.9854 (1.0480)\taccu 95.312 (93.324)\n",
      "Epoch-49  240 batches\tloss 1.0891 (1.0492)\taccu 90.625 (93.249)\n",
      "Epoch-49  260 batches\tloss 1.1152 (1.0509)\taccu 87.500 (93.149)\n",
      "Epoch-49  280 batches\tloss 1.0256 (1.0525)\taccu 93.750 (93.080)\n",
      "Epoch-49  300 batches\tloss 0.9885 (1.0523)\taccu 92.188 (93.156)\n",
      "Epoch-49  320 batches\tloss 1.1974 (1.0541)\taccu 90.625 (93.091)\n",
      "Epoch-49  340 batches\tloss 1.0310 (1.0533)\taccu 92.188 (93.125)\n",
      "Epoch-49  360 batches\tloss 1.0486 (1.0551)\taccu 96.875 (93.099)\n",
      "Epoch-49  380 batches\tloss 1.1193 (1.0568)\taccu 92.188 (93.059)\n",
      "Epoch-49  400 batches\tloss 1.0662 (1.0578)\taccu 90.625 (93.016)\n",
      "Epoch-49  420 batches\tloss 0.9824 (1.0593)\taccu 95.312 (92.928)\n",
      "Epoch-49  440 batches\tloss 1.1900 (1.0613)\taccu 89.062 (92.820)\n",
      "Epoch-49  460 batches\tloss 1.1709 (1.0613)\taccu 90.625 (92.836)\n",
      "Epoch-49  480 batches\tloss 1.0858 (1.0621)\taccu 92.188 (92.826)\n",
      "Epoch-49  93.2s\tTrain: loss 1.0634\taccu 92.7273\tValid: loss 1.3214\taccu 83.0189\n",
      "Epoch 49: val_acc did not improve\n",
      "49 0.001\n",
      "Epoch-50   20 batches\tloss 0.9651 (1.0154)\taccu 98.438 (95.625)\n",
      "Epoch-50   40 batches\tloss 0.9825 (1.0261)\taccu 96.875 (94.570)\n",
      "Epoch-50   60 batches\tloss 1.0437 (1.0348)\taccu 93.750 (94.245)\n",
      "Epoch-50   80 batches\tloss 1.0364 (1.0386)\taccu 90.625 (94.062)\n",
      "Epoch-50  100 batches\tloss 1.1118 (1.0391)\taccu 90.625 (93.953)\n",
      "Epoch-50  120 batches\tloss 1.1421 (1.0452)\taccu 89.062 (93.737)\n",
      "Epoch-50  140 batches\tloss 1.0183 (1.0448)\taccu 93.750 (93.783)\n",
      "Epoch-50  160 batches\tloss 1.0497 (1.0459)\taccu 96.875 (93.701)\n",
      "Epoch-50  180 batches\tloss 1.2082 (1.0467)\taccu 85.938 (93.707)\n",
      "Epoch-50  200 batches\tloss 1.1926 (1.0485)\taccu 84.375 (93.641)\n",
      "Epoch-50  220 batches\tloss 1.0164 (1.0490)\taccu 93.750 (93.558)\n",
      "Epoch-50  240 batches\tloss 0.9871 (1.0488)\taccu 98.438 (93.548)\n",
      "Epoch-50  260 batches\tloss 1.2327 (1.0502)\taccu 82.812 (93.474)\n",
      "Epoch-50  280 batches\tloss 1.0731 (1.0503)\taccu 93.750 (93.482)\n",
      "Epoch-50  300 batches\tloss 1.0013 (1.0515)\taccu 95.312 (93.417)\n",
      "Epoch-50  320 batches\tloss 1.1250 (1.0520)\taccu 90.625 (93.394)\n",
      "Epoch-50  340 batches\tloss 1.1023 (1.0539)\taccu 87.500 (93.359)\n",
      "Epoch-50  360 batches\tloss 1.1186 (1.0546)\taccu 89.062 (93.299)\n",
      "Epoch-50  380 batches\tloss 1.0790 (1.0547)\taccu 93.750 (93.273)\n",
      "Epoch-50  400 batches\tloss 0.9918 (1.0545)\taccu 98.438 (93.270)\n",
      "Epoch-50  420 batches\tloss 1.1335 (1.0561)\taccu 90.625 (93.192)\n",
      "Epoch-50  440 batches\tloss 1.0154 (1.0558)\taccu 95.312 (93.232)\n",
      "Epoch-50  460 batches\tloss 1.1525 (1.0568)\taccu 84.375 (93.203)\n",
      "Epoch-50  480 batches\tloss 1.0679 (1.0575)\taccu 92.188 (93.193)\n",
      "Epoch-50  126.8s\tTrain: loss 1.0589\taccu 93.0966\tValid: loss 1.3114\taccu 83.4611\n",
      "Epoch 50: val_acc did not improve\n",
      "50 0.001\n",
      "Epoch-51   20 batches\tloss 1.0172 (1.0497)\taccu 93.750 (93.594)\n",
      "Epoch-51   40 batches\tloss 1.1347 (1.0408)\taccu 89.062 (93.828)\n",
      "Epoch-51   60 batches\tloss 1.0297 (1.0436)\taccu 90.625 (93.958)\n",
      "Epoch-51   80 batches\tloss 1.0650 (1.0391)\taccu 93.750 (94.180)\n",
      "Epoch-51  100 batches\tloss 1.2704 (1.0433)\taccu 84.375 (93.766)\n",
      "Epoch-51  120 batches\tloss 0.9845 (1.0422)\taccu 95.312 (93.815)\n",
      "Epoch-51  140 batches\tloss 1.0502 (1.0450)\taccu 93.750 (93.717)\n",
      "Epoch-51  160 batches\tloss 1.0621 (1.0452)\taccu 98.438 (93.721)\n",
      "Epoch-51  180 batches\tloss 1.0462 (1.0442)\taccu 93.750 (93.733)\n",
      "Epoch-51  200 batches\tloss 1.0237 (1.0460)\taccu 95.312 (93.562)\n",
      "Epoch-51  220 batches\tloss 1.0800 (1.0456)\taccu 90.625 (93.565)\n",
      "Epoch-51  240 batches\tloss 1.0107 (1.0465)\taccu 95.312 (93.503)\n",
      "Epoch-51  260 batches\tloss 1.0590 (1.0492)\taccu 93.750 (93.389)\n",
      "Epoch-51  280 batches\tloss 1.0371 (1.0508)\taccu 93.750 (93.365)\n",
      "Epoch-51  300 batches\tloss 1.0880 (1.0517)\taccu 92.188 (93.318)\n",
      "Epoch-51  320 batches\tloss 1.0045 (1.0522)\taccu 95.312 (93.296)\n",
      "Epoch-51  340 batches\tloss 0.9969 (1.0536)\taccu 93.750 (93.203)\n",
      "Epoch-51  360 batches\tloss 0.9926 (1.0535)\taccu 96.875 (93.212)\n",
      "Epoch-51  380 batches\tloss 1.0262 (1.0529)\taccu 96.875 (93.244)\n",
      "Epoch-51  400 batches\tloss 1.1656 (1.0542)\taccu 87.500 (93.184)\n",
      "Epoch-51  420 batches\tloss 1.0654 (1.0554)\taccu 92.188 (93.158)\n",
      "Epoch-51  440 batches\tloss 1.1265 (1.0567)\taccu 92.188 (93.086)\n",
      "Epoch-51  460 batches\tloss 1.1767 (1.0573)\taccu 84.375 (93.047)\n",
      "Epoch-51  480 batches\tloss 1.1309 (1.0575)\taccu 92.188 (93.066)\n",
      "Epoch-51  128.0s\tTrain: loss 1.0581\taccu 93.0398\tValid: loss 1.3028\taccu 83.4758\n",
      "Epoch 51: val_acc did not improve\n",
      "51 0.001\n",
      "Epoch-52   20 batches\tloss 0.9748 (1.0168)\taccu 95.312 (94.531)\n",
      "Epoch-52   40 batches\tloss 1.0866 (1.0161)\taccu 89.062 (94.375)\n",
      "Epoch-52   60 batches\tloss 0.9119 (1.0169)\taccu 96.875 (94.714)\n",
      "Epoch-52   80 batches\tloss 1.0401 (1.0222)\taccu 93.750 (94.570)\n",
      "Epoch-52  100 batches\tloss 1.1868 (1.0240)\taccu 89.062 (94.531)\n",
      "Epoch-52  120 batches\tloss 1.1272 (1.0269)\taccu 90.625 (94.427)\n",
      "Epoch-52  140 batches\tloss 0.9463 (1.0307)\taccu 98.438 (94.308)\n",
      "Epoch-52  160 batches\tloss 1.0142 (1.0315)\taccu 96.875 (94.258)\n",
      "Epoch-52  180 batches\tloss 1.0345 (1.0323)\taccu 92.188 (94.245)\n",
      "Epoch-52  200 batches\tloss 1.0586 (1.0346)\taccu 90.625 (94.094)\n",
      "Epoch-52  220 batches\tloss 1.0588 (1.0348)\taccu 92.188 (94.084)\n",
      "Epoch-52  240 batches\tloss 1.0874 (1.0378)\taccu 93.750 (94.004)\n",
      "Epoch-52  260 batches\tloss 1.0128 (1.0390)\taccu 95.312 (94.038)\n",
      "Epoch-52  280 batches\tloss 1.0059 (1.0404)\taccu 98.438 (93.934)\n",
      "Epoch-52  300 batches\tloss 1.0707 (1.0412)\taccu 89.062 (93.885)\n",
      "Epoch-52  320 batches\tloss 1.0295 (1.0430)\taccu 95.312 (93.794)\n",
      "Epoch-52  340 batches\tloss 1.0026 (1.0425)\taccu 92.188 (93.842)\n",
      "Epoch-52  360 batches\tloss 0.9824 (1.0437)\taccu 96.875 (93.776)\n",
      "Epoch-52  380 batches\tloss 1.2217 (1.0452)\taccu 85.938 (93.692)\n",
      "Epoch-52  400 batches\tloss 1.0725 (1.0464)\taccu 92.188 (93.633)\n",
      "Epoch-52  420 batches\tloss 1.1158 (1.0474)\taccu 93.750 (93.605)\n",
      "Epoch-52  440 batches\tloss 1.1682 (1.0478)\taccu 85.938 (93.587)\n",
      "Epoch-52  460 batches\tloss 1.0504 (1.0486)\taccu 95.312 (93.529)\n",
      "Epoch-52  480 batches\tloss 1.1158 (1.0490)\taccu 89.062 (93.512)\n",
      "Epoch-52  127.6s\tTrain: loss 1.0507\taccu 93.4312\tValid: loss 1.3264\taccu 82.9009\n",
      "Epoch 52: val_acc did not improve\n",
      "52 0.001\n",
      "Epoch-53   20 batches\tloss 1.0542 (1.0576)\taccu 92.188 (93.281)\n",
      "Epoch-53   40 batches\tloss 0.9615 (1.0463)\taccu 98.438 (93.359)\n",
      "Epoch-53   60 batches\tloss 1.0649 (1.0445)\taccu 92.188 (93.516)\n",
      "Epoch-53   80 batches\tloss 0.9916 (1.0404)\taccu 95.312 (93.770)\n",
      "Epoch-53  100 batches\tloss 1.0170 (1.0372)\taccu 93.750 (93.891)\n",
      "Epoch-53  120 batches\tloss 1.0507 (1.0361)\taccu 92.188 (93.971)\n",
      "Epoch-53  140 batches\tloss 0.9689 (1.0346)\taccu 96.875 (94.074)\n",
      "Epoch-53  160 batches\tloss 1.1034 (1.0386)\taccu 90.625 (93.809)\n",
      "Epoch-53  180 batches\tloss 1.0031 (1.0402)\taccu 96.875 (93.793)\n",
      "Epoch-53  200 batches\tloss 1.0198 (1.0429)\taccu 95.312 (93.742)\n",
      "Epoch-53  220 batches\tloss 1.1203 (1.0451)\taccu 90.625 (93.672)\n",
      "Epoch-53  240 batches\tloss 1.0361 (1.0474)\taccu 92.188 (93.646)\n",
      "Epoch-53  260 batches\tloss 1.0591 (1.0502)\taccu 95.312 (93.528)\n",
      "Epoch-53  280 batches\tloss 1.2060 (1.0539)\taccu 85.938 (93.331)\n",
      "Epoch-53  300 batches\tloss 0.9990 (1.0544)\taccu 96.875 (93.292)\n",
      "Epoch-53  320 batches\tloss 1.0165 (1.0557)\taccu 96.875 (93.276)\n",
      "Epoch-53  340 batches\tloss 1.0958 (1.0559)\taccu 89.062 (93.212)\n",
      "Epoch-53  360 batches\tloss 1.0223 (1.0556)\taccu 93.750 (93.251)\n",
      "Epoch-53  380 batches\tloss 1.1716 (1.0551)\taccu 85.938 (93.273)\n",
      "Epoch-53  400 batches\tloss 1.1659 (1.0553)\taccu 90.625 (93.289)\n",
      "Epoch-53  420 batches\tloss 1.0932 (1.0561)\taccu 89.062 (93.225)\n",
      "Epoch-53  440 batches\tloss 1.0583 (1.0573)\taccu 93.750 (93.171)\n",
      "Epoch-53  460 batches\tloss 1.0246 (1.0568)\taccu 95.312 (93.220)\n",
      "Epoch-53  480 batches\tloss 1.1332 (1.0590)\taccu 95.312 (93.141)\n",
      "Epoch-53  127.8s\tTrain: loss 1.0592\taccu 93.1345\tValid: loss 1.3007\taccu 83.4611\n",
      "Epoch 53: val_acc did not improve\n",
      "53 0.001\n",
      "Epoch-54   20 batches\tloss 1.0245 (1.0151)\taccu 95.312 (95.547)\n",
      "Epoch-54   40 batches\tloss 1.0119 (1.0169)\taccu 93.750 (95.312)\n",
      "Epoch-54   60 batches\tloss 0.9430 (1.0211)\taccu 98.438 (95.104)\n",
      "Epoch-54   80 batches\tloss 0.9610 (1.0251)\taccu 96.875 (94.707)\n",
      "Epoch-54  100 batches\tloss 1.1245 (1.0288)\taccu 90.625 (94.516)\n",
      "Epoch-54  120 batches\tloss 0.9449 (1.0261)\taccu 98.438 (94.609)\n",
      "Epoch-54  140 batches\tloss 1.1840 (1.0308)\taccu 89.062 (94.397)\n",
      "Epoch-54  160 batches\tloss 1.1771 (1.0346)\taccu 89.062 (94.209)\n",
      "Epoch-54  180 batches\tloss 1.0318 (1.0357)\taccu 92.188 (94.149)\n",
      "Epoch-54  200 batches\tloss 0.9532 (1.0357)\taccu 96.875 (94.148)\n",
      "Epoch-54  220 batches\tloss 1.0336 (1.0352)\taccu 90.625 (94.084)\n",
      "Epoch-54  240 batches\tloss 1.1755 (1.0372)\taccu 89.062 (93.971)\n",
      "Epoch-54  260 batches\tloss 1.0918 (1.0404)\taccu 93.750 (93.876)\n",
      "Epoch-54  280 batches\tloss 1.0758 (1.0421)\taccu 90.625 (93.800)\n",
      "Epoch-54  300 batches\tloss 1.0466 (1.0429)\taccu 92.188 (93.729)\n",
      "Epoch-54  320 batches\tloss 1.1475 (1.0445)\taccu 90.625 (93.643)\n",
      "Epoch-54  340 batches\tloss 1.1335 (1.0444)\taccu 87.500 (93.695)\n",
      "Epoch-54  360 batches\tloss 1.0658 (1.0446)\taccu 90.625 (93.698)\n",
      "Epoch-54  380 batches\tloss 1.0349 (1.0446)\taccu 95.312 (93.713)\n",
      "Epoch-54  400 batches\tloss 1.1442 (1.0449)\taccu 90.625 (93.715)\n",
      "Epoch-54  420 batches\tloss 1.0559 (1.0450)\taccu 92.188 (93.690)\n",
      "Epoch-54  440 batches\tloss 1.0790 (1.0449)\taccu 90.625 (93.690)\n",
      "Epoch-54  460 batches\tloss 1.1180 (1.0459)\taccu 92.188 (93.675)\n",
      "Epoch-54  480 batches\tloss 1.1451 (1.0477)\taccu 87.500 (93.577)\n",
      "Epoch-54  127.6s\tTrain: loss 1.0490\taccu 93.5101\tValid: loss 1.3008\taccu 83.5201\n",
      "Epoch 54: val_acc did not improve\n",
      "54 0.001\n",
      "Epoch-55   20 batches\tloss 1.0121 (1.0249)\taccu 96.875 (94.219)\n",
      "Epoch-55   40 batches\tloss 1.1168 (1.0270)\taccu 92.188 (94.258)\n",
      "Epoch-55   60 batches\tloss 0.9892 (1.0299)\taccu 96.875 (94.297)\n",
      "Epoch-55   80 batches\tloss 0.9986 (1.0317)\taccu 98.438 (94.141)\n",
      "Epoch-55  100 batches\tloss 0.9915 (1.0333)\taccu 90.625 (93.922)\n",
      "Epoch-55  120 batches\tloss 1.0075 (1.0335)\taccu 95.312 (94.128)\n",
      "Epoch-55  140 batches\tloss 1.0504 (1.0345)\taccu 95.312 (94.062)\n",
      "Epoch-55  160 batches\tloss 1.0699 (1.0355)\taccu 92.188 (94.072)\n",
      "Epoch-55  180 batches\tloss 1.1156 (1.0340)\taccu 92.188 (94.158)\n",
      "Epoch-55  200 batches\tloss 1.0487 (1.0337)\taccu 95.312 (94.195)\n",
      "Epoch-55  220 batches\tloss 1.0825 (1.0362)\taccu 90.625 (94.098)\n",
      "Epoch-55  240 batches\tloss 0.9873 (1.0374)\taccu 96.875 (94.063)\n",
      "Epoch-55  260 batches\tloss 1.0582 (1.0392)\taccu 95.312 (94.008)\n",
      "Epoch-55  280 batches\tloss 1.0776 (1.0398)\taccu 92.188 (93.990)\n",
      "Epoch-55  300 batches\tloss 1.0346 (1.0401)\taccu 95.312 (93.953)\n",
      "Epoch-55  320 batches\tloss 1.0085 (1.0421)\taccu 95.312 (93.843)\n",
      "Epoch-55  340 batches\tloss 1.0759 (1.0431)\taccu 93.750 (93.833)\n",
      "Epoch-55  360 batches\tloss 1.1677 (1.0445)\taccu 89.062 (93.750)\n",
      "Epoch-55  380 batches\tloss 0.9670 (1.0445)\taccu 98.438 (93.729)\n",
      "Epoch-55  400 batches\tloss 1.0505 (1.0452)\taccu 92.188 (93.695)\n",
      "Epoch-55  420 batches\tloss 1.1189 (1.0457)\taccu 89.062 (93.679)\n",
      "Epoch-55  440 batches\tloss 1.0437 (1.0470)\taccu 93.750 (93.604)\n",
      "Epoch-55  460 batches\tloss 1.0566 (1.0479)\taccu 95.312 (93.570)\n",
      "Epoch-55  480 batches\tloss 0.9842 (1.0485)\taccu 96.875 (93.551)\n",
      "Epoch-55  127.7s\tTrain: loss 1.0494\taccu 93.5006\tValid: loss 1.3060\taccu 83.2400\n",
      "Epoch 55: val_acc did not improve\n",
      "55 0.001\n",
      "Epoch-56   20 batches\tloss 1.0790 (1.0357)\taccu 89.062 (94.375)\n",
      "Epoch-56   40 batches\tloss 0.9779 (1.0250)\taccu 93.750 (94.531)\n",
      "Epoch-56   60 batches\tloss 1.0007 (1.0175)\taccu 95.312 (94.870)\n",
      "Epoch-56   80 batches\tloss 1.0332 (1.0207)\taccu 93.750 (94.707)\n",
      "Epoch-56  100 batches\tloss 1.0455 (1.0201)\taccu 92.188 (94.719)\n",
      "Epoch-56  120 batches\tloss 0.9696 (1.0231)\taccu 98.438 (94.583)\n",
      "Epoch-56  140 batches\tloss 1.1147 (1.0268)\taccu 89.062 (94.397)\n",
      "Epoch-56  160 batches\tloss 1.0166 (1.0292)\taccu 92.188 (94.268)\n",
      "Epoch-56  180 batches\tloss 1.0205 (1.0301)\taccu 93.750 (94.193)\n",
      "Epoch-56  200 batches\tloss 1.0253 (1.0303)\taccu 95.312 (94.242)\n",
      "Epoch-56  220 batches\tloss 1.0830 (1.0304)\taccu 90.625 (94.233)\n",
      "Epoch-56  240 batches\tloss 0.9945 (1.0305)\taccu 95.312 (94.245)\n",
      "Epoch-56  260 batches\tloss 0.9702 (1.0311)\taccu 98.438 (94.207)\n",
      "Epoch-56  280 batches\tloss 1.0683 (1.0319)\taccu 92.188 (94.135)\n",
      "Epoch-56  300 batches\tloss 1.1239 (1.0334)\taccu 93.750 (94.125)\n",
      "Epoch-56  320 batches\tloss 1.0118 (1.0355)\taccu 95.312 (93.999)\n",
      "Epoch-56  340 batches\tloss 1.1466 (1.0381)\taccu 89.062 (93.869)\n",
      "Epoch-56  360 batches\tloss 1.0444 (1.0384)\taccu 92.188 (93.880)\n",
      "Epoch-56  380 batches\tloss 1.0915 (1.0398)\taccu 89.062 (93.799)\n",
      "Epoch-56  400 batches\tloss 1.0466 (1.0408)\taccu 93.750 (93.785)\n",
      "Epoch-56  420 batches\tloss 1.0277 (1.0420)\taccu 93.750 (93.709)\n",
      "Epoch-56  440 batches\tloss 1.0509 (1.0441)\taccu 93.750 (93.608)\n",
      "Epoch-56  460 batches\tloss 0.9599 (1.0450)\taccu 98.438 (93.573)\n",
      "Epoch-56  480 batches\tloss 1.0081 (1.0453)\taccu 95.312 (93.535)\n",
      "Epoch-56  121.7s\tTrain: loss 1.0457\taccu 93.4817\tValid: loss 1.3142\taccu 82.8272\n",
      "Epoch 56: val_acc did not improve\n",
      "56 0.001\n",
      "Epoch-57   20 batches\tloss 1.0570 (1.0048)\taccu 96.875 (95.703)\n",
      "Epoch-57   40 batches\tloss 1.0554 (1.0245)\taccu 90.625 (94.922)\n",
      "Epoch-57   60 batches\tloss 0.9894 (1.0279)\taccu 93.750 (94.505)\n",
      "Epoch-57   80 batches\tloss 1.0295 (1.0254)\taccu 93.750 (94.609)\n",
      "Epoch-57  100 batches\tloss 1.0443 (1.0260)\taccu 93.750 (94.516)\n",
      "Epoch-57  120 batches\tloss 0.9691 (1.0277)\taccu 96.875 (94.388)\n",
      "Epoch-57  140 batches\tloss 1.1474 (1.0294)\taccu 87.500 (94.342)\n",
      "Epoch-57  160 batches\tloss 1.1017 (1.0287)\taccu 92.188 (94.375)\n",
      "Epoch-57  180 batches\tloss 0.9970 (1.0296)\taccu 93.750 (94.349)\n",
      "Epoch-57  200 batches\tloss 0.9287 (1.0326)\taccu 98.438 (94.203)\n",
      "Epoch-57  220 batches\tloss 1.1540 (1.0359)\taccu 87.500 (94.084)\n",
      "Epoch-57  240 batches\tloss 1.0286 (1.0382)\taccu 95.312 (93.971)\n",
      "Epoch-57  260 batches\tloss 1.1363 (1.0400)\taccu 89.062 (93.882)\n",
      "Epoch-57  280 batches\tloss 1.0735 (1.0417)\taccu 92.188 (93.817)\n",
      "Epoch-57  300 batches\tloss 1.0714 (1.0425)\taccu 90.625 (93.766)\n",
      "Epoch-57  320 batches\tloss 1.1074 (1.0430)\taccu 90.625 (93.750)\n",
      "Epoch-57  340 batches\tloss 1.0996 (1.0444)\taccu 84.375 (93.686)\n",
      "Epoch-57  360 batches\tloss 1.0424 (1.0457)\taccu 93.750 (93.585)\n",
      "Epoch-57  380 batches\tloss 1.0447 (1.0452)\taccu 95.312 (93.618)\n",
      "Epoch-57  400 batches\tloss 1.2540 (1.0459)\taccu 84.375 (93.621)\n",
      "Epoch-57  420 batches\tloss 1.0938 (1.0471)\taccu 89.062 (93.538)\n",
      "Epoch-57  440 batches\tloss 0.9912 (1.0478)\taccu 98.438 (93.512)\n",
      "Epoch-57  460 batches\tloss 1.1197 (1.0483)\taccu 92.188 (93.492)\n",
      "Epoch-57  480 batches\tloss 1.1136 (1.0483)\taccu 92.188 (93.467)\n",
      "Epoch-57  90.2s\tTrain: loss 1.0491\taccu 93.3965\tValid: loss 1.3041\taccu 83.5201\n",
      "Epoch 57: val_acc did not improve\n",
      "57 0.001\n",
      "Epoch-58   20 batches\tloss 1.0808 (1.0253)\taccu 87.500 (94.141)\n",
      "Epoch-58   40 batches\tloss 1.0142 (1.0172)\taccu 93.750 (94.727)\n",
      "Epoch-58   60 batches\tloss 1.0131 (1.0222)\taccu 93.750 (94.661)\n",
      "Epoch-58   80 batches\tloss 1.0593 (1.0236)\taccu 93.750 (94.512)\n",
      "Epoch-58  100 batches\tloss 1.1040 (1.0262)\taccu 93.750 (94.391)\n",
      "Epoch-58  120 batches\tloss 0.9516 (1.0260)\taccu 98.438 (94.388)\n",
      "Epoch-58  140 batches\tloss 0.9496 (1.0280)\taccu 98.438 (94.297)\n",
      "Epoch-58  160 batches\tloss 1.1483 (1.0285)\taccu 87.500 (94.229)\n",
      "Epoch-58  180 batches\tloss 0.9890 (1.0282)\taccu 96.875 (94.236)\n",
      "Epoch-58  200 batches\tloss 0.9632 (1.0280)\taccu 96.875 (94.328)\n",
      "Epoch-58  220 batches\tloss 1.0034 (1.0288)\taccu 96.875 (94.290)\n",
      "Epoch-58  240 batches\tloss 0.9643 (1.0302)\taccu 96.875 (94.238)\n",
      "Epoch-58  260 batches\tloss 1.0481 (1.0293)\taccu 93.750 (94.303)\n",
      "Epoch-58  280 batches\tloss 1.0657 (1.0300)\taccu 93.750 (94.275)\n",
      "Epoch-58  300 batches\tloss 0.9937 (1.0304)\taccu 98.438 (94.292)\n",
      "Epoch-58  320 batches\tloss 1.0128 (1.0323)\taccu 96.875 (94.214)\n",
      "Epoch-58  340 batches\tloss 1.0324 (1.0331)\taccu 90.625 (94.141)\n",
      "Epoch-58  360 batches\tloss 0.9841 (1.0329)\taccu 98.438 (94.184)\n",
      "Epoch-58  380 batches\tloss 1.2018 (1.0347)\taccu 85.938 (94.124)\n",
      "Epoch-58  400 batches\tloss 0.9894 (1.0360)\taccu 96.875 (94.047)\n",
      "Epoch-58  420 batches\tloss 0.9749 (1.0356)\taccu 95.312 (94.077)\n",
      "Epoch-58  440 batches\tloss 1.0562 (1.0378)\taccu 89.062 (93.991)\n",
      "Epoch-58  460 batches\tloss 1.0010 (1.0392)\taccu 95.312 (93.899)\n",
      "Epoch-58  480 batches\tloss 1.0070 (1.0403)\taccu 96.875 (93.861)\n",
      "Epoch-58  89.5s\tTrain: loss 1.0417\taccu 93.8068\tValid: loss 1.3118\taccu 83.0778\n",
      "Epoch 58: val_acc did not improve\n",
      "58 0.001\n",
      "Epoch-59   20 batches\tloss 1.0501 (1.0085)\taccu 93.750 (95.156)\n",
      "Epoch-59   40 batches\tloss 1.0325 (1.0249)\taccu 93.750 (94.766)\n",
      "Epoch-59   60 batches\tloss 1.0926 (1.0282)\taccu 92.188 (94.557)\n",
      "Epoch-59   80 batches\tloss 1.0439 (1.0278)\taccu 93.750 (94.746)\n",
      "Epoch-59  100 batches\tloss 1.0157 (1.0207)\taccu 95.312 (95.016)\n",
      "Epoch-59  120 batches\tloss 1.0007 (1.0192)\taccu 95.312 (94.896)\n",
      "Epoch-59  140 batches\tloss 1.0591 (1.0220)\taccu 93.750 (94.654)\n",
      "Epoch-59  160 batches\tloss 1.0932 (1.0237)\taccu 93.750 (94.648)\n",
      "Epoch-59  180 batches\tloss 1.0077 (1.0228)\taccu 93.750 (94.618)\n",
      "Epoch-59  200 batches\tloss 0.9869 (1.0250)\taccu 100.000 (94.602)\n",
      "Epoch-59  220 batches\tloss 1.1521 (1.0269)\taccu 89.062 (94.510)\n",
      "Epoch-59  240 batches\tloss 1.0292 (1.0287)\taccu 93.750 (94.408)\n",
      "Epoch-59  260 batches\tloss 1.0410 (1.0308)\taccu 93.750 (94.273)\n",
      "Epoch-59  280 batches\tloss 1.1374 (1.0328)\taccu 90.625 (94.224)\n",
      "Epoch-59  300 batches\tloss 1.0233 (1.0332)\taccu 95.312 (94.203)\n",
      "Epoch-59  320 batches\tloss 1.0411 (1.0361)\taccu 95.312 (94.087)\n",
      "Epoch-59  340 batches\tloss 1.0288 (1.0381)\taccu 93.750 (93.980)\n",
      "Epoch-59  360 batches\tloss 1.1484 (1.0382)\taccu 85.938 (93.980)\n",
      "Epoch-59  380 batches\tloss 1.0003 (1.0385)\taccu 96.875 (93.997)\n",
      "Epoch-59  400 batches\tloss 1.0014 (1.0392)\taccu 96.875 (93.980)\n",
      "Epoch-59  420 batches\tloss 1.1074 (1.0406)\taccu 87.500 (93.925)\n",
      "Epoch-59  440 batches\tloss 1.0624 (1.0418)\taccu 89.062 (93.857)\n",
      "Epoch-59  460 batches\tloss 1.0608 (1.0426)\taccu 90.625 (93.825)\n",
      "Epoch-59  480 batches\tloss 1.0513 (1.0433)\taccu 93.750 (93.802)\n",
      "Epoch-59  89.4s\tTrain: loss 1.0444\taccu 93.7279\tValid: loss 1.3222\taccu 83.1368\n",
      "Epoch 59: val_acc did not improve\n",
      "59 0.001\n",
      "Epoch-60   20 batches\tloss 0.9941 (1.0244)\taccu 98.438 (94.453)\n",
      "Epoch-60   40 batches\tloss 1.0057 (1.0269)\taccu 98.438 (94.961)\n",
      "Epoch-60   60 batches\tloss 1.1576 (1.0285)\taccu 92.188 (94.818)\n",
      "Epoch-60   80 batches\tloss 1.0576 (1.0251)\taccu 95.312 (94.844)\n",
      "Epoch-60  100 batches\tloss 0.9831 (1.0233)\taccu 96.875 (94.703)\n",
      "Epoch-60  120 batches\tloss 1.0998 (1.0239)\taccu 92.188 (94.805)\n",
      "Epoch-60  140 batches\tloss 1.0590 (1.0221)\taccu 92.188 (94.844)\n",
      "Epoch-60  160 batches\tloss 1.1143 (1.0242)\taccu 87.500 (94.785)\n",
      "Epoch-60  180 batches\tloss 0.9845 (1.0243)\taccu 96.875 (94.748)\n",
      "Epoch-60  200 batches\tloss 0.9832 (1.0249)\taccu 96.875 (94.727)\n",
      "Epoch-60  220 batches\tloss 1.0670 (1.0262)\taccu 93.750 (94.638)\n",
      "Epoch-60  240 batches\tloss 1.0863 (1.0267)\taccu 92.188 (94.590)\n",
      "Epoch-60  260 batches\tloss 1.0551 (1.0284)\taccu 92.188 (94.537)\n",
      "Epoch-60  280 batches\tloss 1.1492 (1.0300)\taccu 87.500 (94.464)\n",
      "Epoch-60  300 batches\tloss 1.1072 (1.0303)\taccu 92.188 (94.453)\n",
      "Epoch-60  320 batches\tloss 1.0706 (1.0328)\taccu 95.312 (94.360)\n",
      "Epoch-60  340 batches\tloss 1.0180 (1.0343)\taccu 95.312 (94.237)\n",
      "Epoch-60  360 batches\tloss 1.0551 (1.0350)\taccu 90.625 (94.193)\n",
      "Epoch-60  380 batches\tloss 1.0586 (1.0352)\taccu 93.750 (94.165)\n",
      "Epoch-60  400 batches\tloss 1.1467 (1.0369)\taccu 87.500 (94.086)\n",
      "Epoch-60  420 batches\tloss 1.0464 (1.0376)\taccu 95.312 (94.074)\n",
      "Epoch-60  440 batches\tloss 1.0364 (1.0388)\taccu 92.188 (94.031)\n",
      "Epoch-60  460 batches\tloss 1.1057 (1.0404)\taccu 90.625 (93.971)\n",
      "Epoch-60  480 batches\tloss 1.0780 (1.0423)\taccu 92.188 (93.867)\n",
      "Epoch-60  89.4s\tTrain: loss 1.0426\taccu 93.8447\tValid: loss 1.3026\taccu 83.7706\n",
      "Epoch 60: val_acc did not improve\n",
      "60 0.0001\n",
      "Epoch-61   20 batches\tloss 0.9555 (1.0050)\taccu 96.875 (95.312)\n",
      "Epoch-61   40 batches\tloss 0.9963 (1.0034)\taccu 93.750 (94.922)\n",
      "Epoch-61   60 batches\tloss 0.9326 (1.0013)\taccu 98.438 (95.104)\n",
      "Epoch-61   80 batches\tloss 0.9269 (0.9977)\taccu 98.438 (95.391)\n",
      "Epoch-61  100 batches\tloss 0.9671 (0.9914)\taccu 98.438 (95.750)\n",
      "Epoch-61  120 batches\tloss 0.9658 (0.9869)\taccu 95.312 (95.964)\n",
      "Epoch-61  140 batches\tloss 0.9897 (0.9852)\taccu 95.312 (96.027)\n",
      "Epoch-61  160 batches\tloss 0.9242 (0.9848)\taccu 100.000 (96.035)\n",
      "Epoch-61  180 batches\tloss 0.9088 (0.9832)\taccu 98.438 (96.068)\n",
      "Epoch-61  200 batches\tloss 1.0500 (0.9816)\taccu 95.312 (96.102)\n",
      "Epoch-61  220 batches\tloss 0.9624 (0.9788)\taccu 96.875 (96.222)\n",
      "Epoch-61  240 batches\tloss 0.9240 (0.9767)\taccu 100.000 (96.315)\n",
      "Epoch-61  260 batches\tloss 0.9137 (0.9753)\taccu 100.000 (96.388)\n",
      "Epoch-61  280 batches\tloss 0.9720 (0.9757)\taccu 96.875 (96.328)\n",
      "Epoch-61  300 batches\tloss 0.9159 (0.9737)\taccu 100.000 (96.417)\n",
      "Epoch-61  320 batches\tloss 0.9528 (0.9729)\taccu 100.000 (96.489)\n",
      "Epoch-61  340 batches\tloss 0.9199 (0.9719)\taccu 100.000 (96.530)\n",
      "Epoch-61  360 batches\tloss 1.0405 (0.9714)\taccu 92.188 (96.576)\n",
      "Epoch-61  380 batches\tloss 0.9120 (0.9704)\taccu 100.000 (96.608)\n",
      "Epoch-61  400 batches\tloss 0.9398 (0.9693)\taccu 98.438 (96.664)\n",
      "Epoch-61  420 batches\tloss 0.9592 (0.9684)\taccu 98.438 (96.693)\n",
      "Epoch-61  440 batches\tloss 0.9746 (0.9679)\taccu 96.875 (96.697)\n",
      "Epoch-61  460 batches\tloss 0.9371 (0.9671)\taccu 100.000 (96.715)\n",
      "Epoch-61  480 batches\tloss 1.0238 (0.9672)\taccu 96.875 (96.693)\n",
      "Epoch-61  89.5s\tTrain: loss 0.9670\taccu 96.7077\tValid: loss 1.2368\taccu 85.4805\n",
      "Epoch 61: val_acc improved from 84.0360 to 85.4805, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "61 0.0001\n",
      "Epoch-62   20 batches\tloss 0.9761 (0.9382)\taccu 96.875 (98.203)\n",
      "Epoch-62   40 batches\tloss 0.9082 (0.9438)\taccu 96.875 (97.656)\n",
      "Epoch-62   60 batches\tloss 0.9664 (0.9446)\taccu 96.875 (97.448)\n",
      "Epoch-62   80 batches\tloss 0.9723 (0.9449)\taccu 93.750 (97.383)\n",
      "Epoch-62  100 batches\tloss 0.9190 (0.9463)\taccu 100.000 (97.312)\n",
      "Epoch-62  120 batches\tloss 0.9907 (0.9459)\taccu 98.438 (97.370)\n",
      "Epoch-62  140 batches\tloss 0.9049 (0.9449)\taccu 100.000 (97.411)\n",
      "Epoch-62  160 batches\tloss 0.9324 (0.9436)\taccu 98.438 (97.480)\n",
      "Epoch-62  180 batches\tloss 0.9256 (0.9448)\taccu 98.438 (97.439)\n",
      "Epoch-62  200 batches\tloss 0.9935 (0.9444)\taccu 96.875 (97.484)\n",
      "Epoch-62  220 batches\tloss 0.9407 (0.9430)\taccu 96.875 (97.571)\n",
      "Epoch-62  240 batches\tloss 0.9389 (0.9438)\taccu 95.312 (97.520)\n",
      "Epoch-62  260 batches\tloss 0.9213 (0.9442)\taccu 98.438 (97.482)\n",
      "Epoch-62  280 batches\tloss 0.9153 (0.9435)\taccu 98.438 (97.539)\n",
      "Epoch-62  300 batches\tloss 0.8957 (0.9428)\taccu 100.000 (97.578)\n",
      "Epoch-62  320 batches\tloss 0.9249 (0.9432)\taccu 98.438 (97.544)\n",
      "Epoch-62  340 batches\tloss 0.9996 (0.9430)\taccu 95.312 (97.546)\n",
      "Epoch-62  360 batches\tloss 0.9208 (0.9427)\taccu 98.438 (97.526)\n",
      "Epoch-62  380 batches\tloss 0.9374 (0.9423)\taccu 96.875 (97.512)\n",
      "Epoch-62  400 batches\tloss 0.9913 (0.9420)\taccu 95.312 (97.535)\n",
      "Epoch-62  420 batches\tloss 0.9059 (0.9413)\taccu 100.000 (97.578)\n",
      "Epoch-62  440 batches\tloss 0.9254 (0.9410)\taccu 98.438 (97.603)\n",
      "Epoch-62  460 batches\tloss 0.9452 (0.9410)\taccu 98.438 (97.595)\n",
      "Epoch-62  480 batches\tloss 0.9279 (0.9404)\taccu 98.438 (97.607)\n",
      "Epoch-62  89.7s\tTrain: loss 0.9398\taccu 97.6294\tValid: loss 1.2320\taccu 85.9522\n",
      "Epoch 62: val_acc improved from 85.4805 to 85.9522, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "62 0.0001\n",
      "Epoch-63   20 batches\tloss 0.8866 (0.9268)\taccu 98.438 (98.359)\n",
      "Epoch-63   40 batches\tloss 0.9275 (0.9307)\taccu 96.875 (98.164)\n",
      "Epoch-63   60 batches\tloss 0.8987 (0.9300)\taccu 98.438 (97.995)\n",
      "Epoch-63   80 batches\tloss 0.9088 (0.9316)\taccu 98.438 (97.949)\n",
      "Epoch-63  100 batches\tloss 0.8909 (0.9323)\taccu 100.000 (97.797)\n",
      "Epoch-63  120 batches\tloss 0.9225 (0.9311)\taccu 96.875 (97.734)\n",
      "Epoch-63  140 batches\tloss 0.9136 (0.9297)\taccu 100.000 (97.734)\n",
      "Epoch-63  160 batches\tloss 0.9495 (0.9292)\taccu 96.875 (97.725)\n",
      "Epoch-63  180 batches\tloss 0.9134 (0.9275)\taccu 100.000 (97.839)\n",
      "Epoch-63  200 batches\tloss 0.9141 (0.9282)\taccu 98.438 (97.805)\n",
      "Epoch-63  220 batches\tloss 0.8986 (0.9289)\taccu 100.000 (97.770)\n",
      "Epoch-63  240 batches\tloss 0.9449 (0.9292)\taccu 96.875 (97.773)\n",
      "Epoch-63  260 batches\tloss 0.9839 (0.9297)\taccu 96.875 (97.788)\n",
      "Epoch-63  280 batches\tloss 0.9033 (0.9296)\taccu 98.438 (97.779)\n",
      "Epoch-63  300 batches\tloss 0.9079 (0.9291)\taccu 98.438 (97.828)\n",
      "Epoch-63  320 batches\tloss 0.9005 (0.9289)\taccu 100.000 (97.832)\n",
      "Epoch-63  340 batches\tloss 0.9140 (0.9292)\taccu 98.438 (97.840)\n",
      "Epoch-63  360 batches\tloss 0.9146 (0.9289)\taccu 96.875 (97.865)\n",
      "Epoch-63  380 batches\tloss 0.9584 (0.9290)\taccu 100.000 (97.878)\n",
      "Epoch-63  400 batches\tloss 0.9436 (0.9294)\taccu 100.000 (97.863)\n",
      "Epoch-63  420 batches\tloss 0.9287 (0.9293)\taccu 96.875 (97.868)\n",
      "Epoch-63  440 batches\tloss 0.9582 (0.9294)\taccu 98.438 (97.862)\n",
      "Epoch-63  460 batches\tloss 0.8975 (0.9287)\taccu 98.438 (97.894)\n",
      "Epoch-63  480 batches\tloss 0.9048 (0.9286)\taccu 100.000 (97.884)\n",
      "Epoch-63  89.5s\tTrain: loss 0.9282\taccu 97.8946\tValid: loss 1.2265\taccu 86.0554\n",
      "Epoch 63: val_acc improved from 85.9522 to 86.0554, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "63 0.0001\n",
      "Epoch-64   20 batches\tloss 0.8894 (0.9254)\taccu 98.438 (98.281)\n",
      "Epoch-64   40 batches\tloss 0.8746 (0.9225)\taccu 100.000 (98.203)\n",
      "Epoch-64   60 batches\tloss 0.9150 (0.9201)\taccu 98.438 (98.229)\n",
      "Epoch-64   80 batches\tloss 0.9378 (0.9174)\taccu 96.875 (98.281)\n",
      "Epoch-64  100 batches\tloss 0.9152 (0.9156)\taccu 96.875 (98.344)\n",
      "Epoch-64  120 batches\tloss 0.9713 (0.9183)\taccu 95.312 (98.294)\n",
      "Epoch-64  140 batches\tloss 0.9126 (0.9201)\taccu 98.438 (98.248)\n",
      "Epoch-64  160 batches\tloss 0.9780 (0.9215)\taccu 96.875 (98.135)\n",
      "Epoch-64  180 batches\tloss 0.8974 (0.9207)\taccu 100.000 (98.160)\n",
      "Epoch-64  200 batches\tloss 0.8837 (0.9204)\taccu 100.000 (98.188)\n",
      "Epoch-64  220 batches\tloss 0.8678 (0.9197)\taccu 100.000 (98.232)\n",
      "Epoch-64  240 batches\tloss 0.9070 (0.9200)\taccu 100.000 (98.242)\n",
      "Epoch-64  260 batches\tloss 0.8986 (0.9199)\taccu 100.000 (98.245)\n",
      "Epoch-64  280 batches\tloss 0.9477 (0.9204)\taccu 96.875 (98.231)\n",
      "Epoch-64  300 batches\tloss 0.9084 (0.9201)\taccu 98.438 (98.245)\n",
      "Epoch-64  320 batches\tloss 0.8863 (0.9200)\taccu 98.438 (98.262)\n",
      "Epoch-64  340 batches\tloss 0.8883 (0.9195)\taccu 100.000 (98.281)\n",
      "Epoch-64  360 batches\tloss 0.9018 (0.9191)\taccu 96.875 (98.294)\n",
      "Epoch-64  380 batches\tloss 0.9806 (0.9196)\taccu 95.312 (98.277)\n",
      "Epoch-64  400 batches\tloss 0.9401 (0.9200)\taccu 96.875 (98.258)\n",
      "Epoch-64  420 batches\tloss 0.9477 (0.9204)\taccu 95.312 (98.225)\n",
      "Epoch-64  440 batches\tloss 1.0352 (0.9208)\taccu 93.750 (98.217)\n",
      "Epoch-64  460 batches\tloss 0.9217 (0.9205)\taccu 98.438 (98.244)\n",
      "Epoch-64  480 batches\tloss 0.9374 (0.9210)\taccu 98.438 (98.203)\n",
      "Epoch-64  89.3s\tTrain: loss 0.9209\taccu 98.1976\tValid: loss 1.2299\taccu 85.5837\n",
      "Epoch 64: val_acc did not improve\n",
      "64 0.0001\n",
      "Epoch-65   20 batches\tloss 0.8971 (0.9248)\taccu 100.000 (97.812)\n",
      "Epoch-65   40 batches\tloss 0.8871 (0.9205)\taccu 100.000 (98.008)\n",
      "Epoch-65   60 batches\tloss 0.9077 (0.9152)\taccu 100.000 (98.255)\n",
      "Epoch-65   80 batches\tloss 0.8901 (0.9202)\taccu 100.000 (98.164)\n",
      "Epoch-65  100 batches\tloss 0.8985 (0.9184)\taccu 100.000 (98.234)\n",
      "Epoch-65  120 batches\tloss 0.8964 (0.9193)\taccu 98.438 (98.151)\n",
      "Epoch-65  140 batches\tloss 0.9122 (0.9181)\taccu 96.875 (98.192)\n",
      "Epoch-65  160 batches\tloss 0.9191 (0.9179)\taccu 100.000 (98.213)\n",
      "Epoch-65  180 batches\tloss 0.8973 (0.9182)\taccu 100.000 (98.229)\n",
      "Epoch-65  200 batches\tloss 0.9006 (0.9171)\taccu 100.000 (98.273)\n",
      "Epoch-65  220 batches\tloss 0.9509 (0.9172)\taccu 98.438 (98.260)\n",
      "Epoch-65  240 batches\tloss 0.8988 (0.9178)\taccu 98.438 (98.255)\n",
      "Epoch-65  260 batches\tloss 0.8841 (0.9174)\taccu 100.000 (98.269)\n",
      "Epoch-65  280 batches\tloss 0.9003 (0.9184)\taccu 98.438 (98.253)\n",
      "Epoch-65  300 batches\tloss 0.9136 (0.9186)\taccu 96.875 (98.266)\n",
      "Epoch-65  320 batches\tloss 0.9534 (0.9185)\taccu 95.312 (98.267)\n",
      "Epoch-65  340 batches\tloss 0.8728 (0.9182)\taccu 100.000 (98.244)\n",
      "Epoch-65  360 batches\tloss 0.8805 (0.9179)\taccu 100.000 (98.234)\n",
      "Epoch-65  380 batches\tloss 0.8883 (0.9180)\taccu 98.438 (98.252)\n",
      "Epoch-65  400 batches\tloss 0.8942 (0.9178)\taccu 100.000 (98.258)\n",
      "Epoch-65  420 batches\tloss 0.9293 (0.9178)\taccu 95.312 (98.248)\n",
      "Epoch-65  440 batches\tloss 0.9550 (0.9184)\taccu 93.750 (98.235)\n",
      "Epoch-65  460 batches\tloss 0.9702 (0.9186)\taccu 96.875 (98.230)\n",
      "Epoch-65  480 batches\tloss 0.8830 (0.9192)\taccu 98.438 (98.197)\n",
      "Epoch-65  89.2s\tTrain: loss 0.9191\taccu 98.1976\tValid: loss 1.2302\taccu 85.7017\n",
      "Epoch 65: val_acc did not improve\n",
      "65 0.0001\n",
      "Epoch-66   20 batches\tloss 0.8649 (0.9107)\taccu 98.438 (98.359)\n",
      "Epoch-66   40 batches\tloss 0.8960 (0.9090)\taccu 98.438 (98.516)\n",
      "Epoch-66   60 batches\tloss 0.9434 (0.9074)\taccu 95.312 (98.672)\n",
      "Epoch-66   80 batches\tloss 0.8947 (0.9117)\taccu 100.000 (98.613)\n",
      "Epoch-66  100 batches\tloss 0.9247 (0.9142)\taccu 98.438 (98.547)\n",
      "Epoch-66  120 batches\tloss 0.8959 (0.9161)\taccu 100.000 (98.516)\n",
      "Epoch-66  140 batches\tloss 0.8823 (0.9162)\taccu 100.000 (98.516)\n",
      "Epoch-66  160 batches\tloss 0.9208 (0.9149)\taccu 96.875 (98.467)\n",
      "Epoch-66  180 batches\tloss 0.9169 (0.9140)\taccu 96.875 (98.490)\n",
      "Epoch-66  200 batches\tloss 0.8532 (0.9136)\taccu 100.000 (98.445)\n",
      "Epoch-66  220 batches\tloss 0.8857 (0.9129)\taccu 98.438 (98.445)\n",
      "Epoch-66  240 batches\tloss 0.9276 (0.9125)\taccu 96.875 (98.424)\n",
      "Epoch-66  260 batches\tloss 0.8904 (0.9121)\taccu 98.438 (98.419)\n",
      "Epoch-66  280 batches\tloss 0.9116 (0.9120)\taccu 98.438 (98.471)\n",
      "Epoch-66  300 batches\tloss 0.9356 (0.9118)\taccu 96.875 (98.464)\n",
      "Epoch-66  320 batches\tloss 0.9073 (0.9126)\taccu 96.875 (98.428)\n",
      "Epoch-66  340 batches\tloss 0.9279 (0.9127)\taccu 100.000 (98.442)\n",
      "Epoch-66  360 batches\tloss 0.8872 (0.9132)\taccu 100.000 (98.420)\n",
      "Epoch-66  380 batches\tloss 0.8795 (0.9129)\taccu 100.000 (98.421)\n",
      "Epoch-66  400 batches\tloss 0.9530 (0.9136)\taccu 96.875 (98.398)\n",
      "Epoch-66  420 batches\tloss 0.9658 (0.9134)\taccu 95.312 (98.404)\n",
      "Epoch-66  440 batches\tloss 0.9059 (0.9134)\taccu 96.875 (98.398)\n",
      "Epoch-66  460 batches\tloss 0.8851 (0.9131)\taccu 100.000 (98.407)\n",
      "Epoch-66  480 batches\tloss 0.8961 (0.9133)\taccu 100.000 (98.418)\n",
      "Epoch-66  89.4s\tTrain: loss 0.9131\taccu 98.4312\tValid: loss 1.2273\taccu 86.1881\n",
      "Epoch 66: val_acc improved from 86.0554 to 86.1881, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "66 0.0001\n",
      "Epoch-67   20 batches\tloss 0.8778 (0.8997)\taccu 100.000 (98.750)\n",
      "Epoch-67   40 batches\tloss 0.9099 (0.9017)\taccu 98.438 (98.750)\n",
      "Epoch-67   60 batches\tloss 0.8655 (0.9049)\taccu 98.438 (98.672)\n",
      "Epoch-67   80 batches\tloss 0.8885 (0.9065)\taccu 98.438 (98.594)\n",
      "Epoch-67  100 batches\tloss 0.9078 (0.9089)\taccu 98.438 (98.484)\n",
      "Epoch-67  120 batches\tloss 0.9118 (0.9097)\taccu 98.438 (98.438)\n",
      "Epoch-67  140 batches\tloss 0.8823 (0.9099)\taccu 100.000 (98.382)\n",
      "Epoch-67  160 batches\tloss 0.8811 (0.9099)\taccu 100.000 (98.389)\n",
      "Epoch-67  180 batches\tloss 0.9231 (0.9094)\taccu 98.438 (98.411)\n",
      "Epoch-67  200 batches\tloss 0.9526 (0.9099)\taccu 96.875 (98.352)\n",
      "Epoch-67  220 batches\tloss 0.8862 (0.9115)\taccu 100.000 (98.324)\n",
      "Epoch-67  240 batches\tloss 0.9500 (0.9124)\taccu 95.312 (98.320)\n",
      "Epoch-67  260 batches\tloss 0.9482 (0.9122)\taccu 98.438 (98.323)\n",
      "Epoch-67  280 batches\tloss 0.9056 (0.9115)\taccu 100.000 (98.359)\n",
      "Epoch-67  300 batches\tloss 0.8818 (0.9119)\taccu 98.438 (98.349)\n",
      "Epoch-67  320 batches\tloss 0.8943 (0.9114)\taccu 98.438 (98.398)\n",
      "Epoch-67  340 batches\tloss 0.8920 (0.9111)\taccu 100.000 (98.438)\n",
      "Epoch-67  360 batches\tloss 0.9364 (0.9112)\taccu 96.875 (98.438)\n",
      "Epoch-67  380 batches\tloss 0.9223 (0.9109)\taccu 96.875 (98.442)\n",
      "Epoch-67  400 batches\tloss 0.8851 (0.9113)\taccu 100.000 (98.449)\n",
      "Epoch-67  420 batches\tloss 0.9077 (0.9108)\taccu 100.000 (98.464)\n",
      "Epoch-67  440 batches\tloss 0.8871 (0.9110)\taccu 100.000 (98.480)\n",
      "Epoch-67  460 batches\tloss 0.8954 (0.9109)\taccu 100.000 (98.505)\n",
      "Epoch-67  480 batches\tloss 0.8828 (0.9108)\taccu 96.875 (98.525)\n",
      "Epoch-67  89.7s\tTrain: loss 0.9108\taccu 98.5417\tValid: loss 1.2259\taccu 86.2323\n",
      "Epoch 67: val_acc improved from 86.1881 to 86.2323, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "67 0.0001\n",
      "Epoch-68   20 batches\tloss 0.9629 (0.9095)\taccu 93.750 (98.359)\n",
      "Epoch-68   40 batches\tloss 0.8726 (0.9042)\taccu 100.000 (98.711)\n",
      "Epoch-68   60 batches\tloss 0.9471 (0.9061)\taccu 96.875 (98.672)\n",
      "Epoch-68   80 batches\tloss 0.9029 (0.9039)\taccu 98.438 (98.730)\n",
      "Epoch-68  100 batches\tloss 0.9839 (0.9047)\taccu 93.750 (98.562)\n",
      "Epoch-68  120 batches\tloss 0.8524 (0.9042)\taccu 100.000 (98.594)\n",
      "Epoch-68  140 batches\tloss 0.9122 (0.9022)\taccu 98.438 (98.683)\n",
      "Epoch-68  160 batches\tloss 0.9342 (0.9017)\taccu 95.312 (98.682)\n",
      "Epoch-68  180 batches\tloss 0.8893 (0.9032)\taccu 98.438 (98.655)\n",
      "Epoch-68  200 batches\tloss 0.8892 (0.9031)\taccu 98.438 (98.641)\n",
      "Epoch-68  220 batches\tloss 0.8695 (0.9034)\taccu 100.000 (98.665)\n",
      "Epoch-68  240 batches\tloss 0.8937 (0.9037)\taccu 100.000 (98.639)\n",
      "Epoch-68  260 batches\tloss 0.9485 (0.9042)\taccu 96.875 (98.630)\n",
      "Epoch-68  280 batches\tloss 0.8998 (0.9042)\taccu 98.438 (98.616)\n",
      "Epoch-68  300 batches\tloss 0.8919 (0.9043)\taccu 100.000 (98.599)\n",
      "Epoch-68  320 batches\tloss 0.8939 (0.9043)\taccu 98.438 (98.594)\n",
      "Epoch-68  340 batches\tloss 0.9224 (0.9051)\taccu 98.438 (98.562)\n",
      "Epoch-68  360 batches\tloss 0.9451 (0.9061)\taccu 96.875 (98.537)\n",
      "Epoch-68  380 batches\tloss 0.9356 (0.9065)\taccu 98.438 (98.532)\n",
      "Epoch-68  400 batches\tloss 0.8627 (0.9066)\taccu 100.000 (98.555)\n",
      "Epoch-68  420 batches\tloss 0.9396 (0.9065)\taccu 96.875 (98.557)\n",
      "Epoch-68  440 batches\tloss 0.9356 (0.9067)\taccu 95.312 (98.548)\n",
      "Epoch-68  460 batches\tloss 0.9370 (0.9066)\taccu 98.438 (98.556)\n",
      "Epoch-68  480 batches\tloss 0.9284 (0.9067)\taccu 98.438 (98.545)\n",
      "Epoch-68  89.4s\tTrain: loss 0.9064\taccu 98.5701\tValid: loss 1.2257\taccu 85.6574\n",
      "Epoch 68: val_acc did not improve\n",
      "68 0.0001\n",
      "Epoch-69   20 batches\tloss 0.9029 (0.9024)\taccu 96.875 (98.516)\n",
      "Epoch-69   40 batches\tloss 0.8685 (0.9021)\taccu 100.000 (98.477)\n",
      "Epoch-69   60 batches\tloss 0.9696 (0.9010)\taccu 98.438 (98.698)\n",
      "Epoch-69   80 batches\tloss 0.9161 (0.8996)\taccu 98.438 (98.652)\n",
      "Epoch-69  100 batches\tloss 0.8922 (0.9005)\taccu 100.000 (98.609)\n",
      "Epoch-69  120 batches\tloss 0.8983 (0.8998)\taccu 98.438 (98.698)\n",
      "Epoch-69  140 batches\tloss 0.8715 (0.8982)\taccu 100.000 (98.783)\n",
      "Epoch-69  160 batches\tloss 0.9669 (0.8986)\taccu 95.312 (98.750)\n",
      "Epoch-69  180 batches\tloss 0.8911 (0.8986)\taccu 95.312 (98.724)\n",
      "Epoch-69  200 batches\tloss 0.9198 (0.8994)\taccu 100.000 (98.727)\n",
      "Epoch-69  220 batches\tloss 0.9079 (0.8999)\taccu 98.438 (98.700)\n",
      "Epoch-69  240 batches\tloss 0.9182 (0.9021)\taccu 98.438 (98.633)\n",
      "Epoch-69  260 batches\tloss 0.9246 (0.9022)\taccu 98.438 (98.642)\n",
      "Epoch-69  280 batches\tloss 0.8998 (0.9032)\taccu 98.438 (98.605)\n",
      "Epoch-69  300 batches\tloss 0.8823 (0.9032)\taccu 100.000 (98.594)\n",
      "Epoch-69  320 batches\tloss 0.8550 (0.9030)\taccu 100.000 (98.613)\n",
      "Epoch-69  340 batches\tloss 0.9002 (0.9027)\taccu 96.875 (98.608)\n",
      "Epoch-69  360 batches\tloss 0.8926 (0.9033)\taccu 100.000 (98.624)\n",
      "Epoch-69  380 batches\tloss 0.9355 (0.9037)\taccu 100.000 (98.602)\n",
      "Epoch-69  400 batches\tloss 0.9052 (0.9036)\taccu 100.000 (98.613)\n",
      "Epoch-69  420 batches\tloss 0.8960 (0.9039)\taccu 96.875 (98.583)\n",
      "Epoch-69  440 batches\tloss 0.8876 (0.9044)\taccu 100.000 (98.562)\n",
      "Epoch-69  460 batches\tloss 0.8886 (0.9044)\taccu 100.000 (98.577)\n",
      "Epoch-69  480 batches\tloss 0.8951 (0.9044)\taccu 100.000 (98.587)\n",
      "Epoch-69  89.1s\tTrain: loss 0.9042\taccu 98.5827\tValid: loss 1.2251\taccu 86.2176\n",
      "Epoch 69: val_acc did not improve\n",
      "69 0.0001\n",
      "Epoch-70   20 batches\tloss 0.8788 (0.9070)\taccu 100.000 (98.672)\n",
      "Epoch-70   40 batches\tloss 0.9136 (0.9069)\taccu 98.438 (98.516)\n",
      "Epoch-70   60 batches\tloss 0.8842 (0.9078)\taccu 100.000 (98.438)\n",
      "Epoch-70   80 batches\tloss 0.8925 (0.9075)\taccu 96.875 (98.418)\n",
      "Epoch-70  100 batches\tloss 0.8968 (0.9072)\taccu 98.438 (98.391)\n",
      "Epoch-70  120 batches\tloss 0.9369 (0.9061)\taccu 98.438 (98.411)\n",
      "Epoch-70  140 batches\tloss 0.8856 (0.9052)\taccu 98.438 (98.438)\n",
      "Epoch-70  160 batches\tloss 0.8725 (0.9052)\taccu 100.000 (98.477)\n",
      "Epoch-70  180 batches\tloss 0.9145 (0.9037)\taccu 100.000 (98.516)\n",
      "Epoch-70  200 batches\tloss 0.8788 (0.9033)\taccu 98.438 (98.562)\n",
      "Epoch-70  220 batches\tloss 0.8949 (0.9027)\taccu 100.000 (98.622)\n",
      "Epoch-70  240 batches\tloss 0.9067 (0.9031)\taccu 98.438 (98.639)\n",
      "Epoch-70  260 batches\tloss 0.9053 (0.9022)\taccu 100.000 (98.696)\n",
      "Epoch-70  280 batches\tloss 0.9009 (0.9026)\taccu 98.438 (98.683)\n",
      "Epoch-70  300 batches\tloss 0.8667 (0.9023)\taccu 100.000 (98.703)\n",
      "Epoch-70  320 batches\tloss 0.9016 (0.9028)\taccu 98.438 (98.687)\n",
      "Epoch-70  340 batches\tloss 0.9231 (0.9031)\taccu 100.000 (98.718)\n",
      "Epoch-70  360 batches\tloss 0.9289 (0.9029)\taccu 98.438 (98.733)\n",
      "Epoch-70  380 batches\tloss 0.8673 (0.9031)\taccu 98.438 (98.729)\n",
      "Epoch-70  400 batches\tloss 0.8723 (0.9024)\taccu 98.438 (98.742)\n",
      "Epoch-70  420 batches\tloss 0.9189 (0.9021)\taccu 95.312 (98.735)\n",
      "Epoch-70  440 batches\tloss 0.8916 (0.9019)\taccu 100.000 (98.725)\n",
      "Epoch-70  460 batches\tloss 0.9499 (0.9023)\taccu 100.000 (98.716)\n",
      "Epoch-70  480 batches\tloss 0.8808 (0.9020)\taccu 98.438 (98.727)\n",
      "Epoch-70  89.5s\tTrain: loss 0.9019\taccu 98.7405\tValid: loss 1.2251\taccu 86.0702\n",
      "Epoch 70: val_acc did not improve\n",
      "70 0.0001\n",
      "Epoch-71   20 batches\tloss 0.8995 (0.8974)\taccu 98.438 (98.750)\n",
      "Epoch-71   40 batches\tloss 0.9109 (0.8957)\taccu 100.000 (98.945)\n",
      "Epoch-71   60 batches\tloss 0.8846 (0.9011)\taccu 100.000 (98.958)\n",
      "Epoch-71   80 batches\tloss 0.8837 (0.8986)\taccu 98.438 (98.926)\n",
      "Epoch-71  100 batches\tloss 0.9034 (0.8990)\taccu 96.875 (98.781)\n",
      "Epoch-71  120 batches\tloss 0.8701 (0.8986)\taccu 100.000 (98.802)\n",
      "Epoch-71  140 batches\tloss 0.9105 (0.8991)\taccu 98.438 (98.795)\n",
      "Epoch-71  160 batches\tloss 0.9072 (0.8995)\taccu 98.438 (98.760)\n",
      "Epoch-71  180 batches\tloss 0.8788 (0.8995)\taccu 98.438 (98.733)\n",
      "Epoch-71  200 batches\tloss 0.8719 (0.8991)\taccu 100.000 (98.742)\n",
      "Epoch-71  220 batches\tloss 0.9502 (0.8999)\taccu 96.875 (98.714)\n",
      "Epoch-71  240 batches\tloss 0.8747 (0.8990)\taccu 100.000 (98.730)\n",
      "Epoch-71  260 batches\tloss 0.9248 (0.8988)\taccu 98.438 (98.768)\n",
      "Epoch-71  280 batches\tloss 0.8693 (0.8985)\taccu 100.000 (98.767)\n",
      "Epoch-71  300 batches\tloss 0.8964 (0.8983)\taccu 98.438 (98.786)\n",
      "Epoch-71  320 batches\tloss 0.8797 (0.8979)\taccu 98.438 (98.794)\n",
      "Epoch-71  340 batches\tloss 0.8447 (0.8982)\taccu 100.000 (98.791)\n",
      "Epoch-71  360 batches\tloss 0.8903 (0.8981)\taccu 98.438 (98.811)\n",
      "Epoch-71  380 batches\tloss 0.8837 (0.8980)\taccu 100.000 (98.828)\n",
      "Epoch-71  400 batches\tloss 0.8608 (0.8983)\taccu 100.000 (98.840)\n",
      "Epoch-71  420 batches\tloss 0.8781 (0.8985)\taccu 98.438 (98.824)\n",
      "Epoch-71  440 batches\tloss 0.9175 (0.8988)\taccu 98.438 (98.817)\n",
      "Epoch-71  460 batches\tloss 0.9582 (0.8988)\taccu 98.438 (98.835)\n",
      "Epoch-71  480 batches\tloss 0.8859 (0.8987)\taccu 98.438 (98.818)\n",
      "Epoch-71  89.2s\tTrain: loss 0.8987\taccu 98.8289\tValid: loss 1.2256\taccu 85.8196\n",
      "Epoch 71: val_acc did not improve\n",
      "71 0.0001\n",
      "Epoch-72   20 batches\tloss 0.9098 (0.8935)\taccu 96.875 (98.594)\n",
      "Epoch-72   40 batches\tloss 0.8805 (0.8919)\taccu 100.000 (98.594)\n",
      "Epoch-72   60 batches\tloss 0.8818 (0.8931)\taccu 100.000 (98.568)\n",
      "Epoch-72   80 batches\tloss 0.9063 (0.8936)\taccu 100.000 (98.711)\n",
      "Epoch-72  100 batches\tloss 0.8790 (0.8916)\taccu 100.000 (98.766)\n",
      "Epoch-72  120 batches\tloss 0.9022 (0.8936)\taccu 98.438 (98.789)\n",
      "Epoch-72  140 batches\tloss 0.8947 (0.8942)\taccu 100.000 (98.806)\n",
      "Epoch-72  160 batches\tloss 0.8573 (0.8933)\taccu 100.000 (98.789)\n",
      "Epoch-72  180 batches\tloss 0.9009 (0.8945)\taccu 98.438 (98.785)\n",
      "Epoch-72  200 batches\tloss 0.8732 (0.8954)\taccu 100.000 (98.797)\n",
      "Epoch-72  220 batches\tloss 0.8952 (0.8953)\taccu 100.000 (98.835)\n",
      "Epoch-72  240 batches\tloss 0.9085 (0.8954)\taccu 96.875 (98.835)\n",
      "Epoch-72  260 batches\tloss 0.8665 (0.8958)\taccu 100.000 (98.834)\n",
      "Epoch-72  280 batches\tloss 0.9553 (0.8964)\taccu 95.312 (98.800)\n",
      "Epoch-72  300 batches\tloss 0.8565 (0.8967)\taccu 100.000 (98.771)\n",
      "Epoch-72  320 batches\tloss 0.9596 (0.8978)\taccu 96.875 (98.711)\n",
      "Epoch-72  340 batches\tloss 0.8984 (0.8976)\taccu 98.438 (98.727)\n",
      "Epoch-72  360 batches\tloss 0.9029 (0.8973)\taccu 96.875 (98.737)\n",
      "Epoch-72  380 batches\tloss 0.8708 (0.8970)\taccu 100.000 (98.758)\n",
      "Epoch-72  400 batches\tloss 0.9353 (0.8975)\taccu 96.875 (98.762)\n",
      "Epoch-72  420 batches\tloss 0.8701 (0.8974)\taccu 100.000 (98.783)\n",
      "Epoch-72  440 batches\tloss 0.8697 (0.8976)\taccu 100.000 (98.789)\n",
      "Epoch-72  460 batches\tloss 0.8774 (0.8975)\taccu 98.438 (98.767)\n",
      "Epoch-72  480 batches\tloss 0.9343 (0.8977)\taccu 96.875 (98.757)\n",
      "Epoch-72  89.4s\tTrain: loss 0.8978\taccu 98.7405\tValid: loss 1.2243\taccu 85.9080\n",
      "Epoch 72: val_acc did not improve\n",
      "72 0.0001\n",
      "Epoch-73   20 batches\tloss 0.9028 (0.9039)\taccu 98.438 (97.812)\n",
      "Epoch-73   40 batches\tloss 0.8674 (0.8955)\taccu 100.000 (98.633)\n",
      "Epoch-73   60 batches\tloss 0.9073 (0.8945)\taccu 100.000 (98.776)\n",
      "Epoch-73   80 batches\tloss 0.9177 (0.8941)\taccu 96.875 (98.828)\n",
      "Epoch-73  100 batches\tloss 0.8938 (0.8925)\taccu 100.000 (98.875)\n",
      "Epoch-73  120 batches\tloss 0.8736 (0.8922)\taccu 100.000 (98.893)\n",
      "Epoch-73  140 batches\tloss 0.8945 (0.8944)\taccu 100.000 (98.828)\n",
      "Epoch-73  160 batches\tloss 0.9325 (0.8952)\taccu 98.438 (98.760)\n",
      "Epoch-73  180 batches\tloss 0.9075 (0.8947)\taccu 96.875 (98.767)\n",
      "Epoch-73  200 batches\tloss 0.9015 (0.8943)\taccu 98.438 (98.812)\n",
      "Epoch-73  220 batches\tloss 0.8519 (0.8948)\taccu 98.438 (98.786)\n",
      "Epoch-73  240 batches\tloss 0.8708 (0.8946)\taccu 100.000 (98.815)\n",
      "Epoch-73  260 batches\tloss 0.8777 (0.8942)\taccu 100.000 (98.858)\n",
      "Epoch-73  280 batches\tloss 0.8548 (0.8941)\taccu 100.000 (98.845)\n",
      "Epoch-73  300 batches\tloss 0.9071 (0.8943)\taccu 100.000 (98.844)\n",
      "Epoch-73  320 batches\tloss 0.8845 (0.8945)\taccu 100.000 (98.823)\n",
      "Epoch-73  340 batches\tloss 0.9140 (0.8949)\taccu 96.875 (98.796)\n",
      "Epoch-73  360 batches\tloss 0.9087 (0.8958)\taccu 100.000 (98.798)\n",
      "Epoch-73  380 batches\tloss 0.8842 (0.8963)\taccu 98.438 (98.775)\n",
      "Epoch-73  400 batches\tloss 0.9458 (0.8961)\taccu 95.312 (98.773)\n",
      "Epoch-73  420 batches\tloss 0.9522 (0.8967)\taccu 95.312 (98.754)\n",
      "Epoch-73  440 batches\tloss 0.8698 (0.8966)\taccu 98.438 (98.746)\n",
      "Epoch-73  460 batches\tloss 0.8799 (0.8970)\taccu 98.438 (98.733)\n",
      "Epoch-73  480 batches\tloss 0.8896 (0.8972)\taccu 100.000 (98.737)\n",
      "Epoch-73  89.2s\tTrain: loss 0.8975\taccu 98.7374\tValid: loss 1.2231\taccu 86.2471\n",
      "Epoch 73: val_acc improved from 86.2323 to 86.2471, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "73 0.0001\n",
      "Epoch-74   20 batches\tloss 0.8812 (0.8941)\taccu 100.000 (98.906)\n",
      "Epoch-74   40 batches\tloss 0.8575 (0.8887)\taccu 100.000 (99.180)\n",
      "Epoch-74   60 batches\tloss 0.8695 (0.8891)\taccu 98.438 (98.984)\n",
      "Epoch-74   80 batches\tloss 0.8921 (0.8870)\taccu 98.438 (99.121)\n",
      "Epoch-74  100 batches\tloss 0.8846 (0.8882)\taccu 100.000 (99.125)\n",
      "Epoch-74  120 batches\tloss 0.9004 (0.8896)\taccu 100.000 (99.063)\n",
      "Epoch-74  140 batches\tloss 0.9400 (0.8904)\taccu 98.438 (99.096)\n",
      "Epoch-74  160 batches\tloss 0.8782 (0.8910)\taccu 98.438 (99.102)\n",
      "Epoch-74  180 batches\tloss 0.8732 (0.8910)\taccu 100.000 (99.132)\n",
      "Epoch-74  200 batches\tloss 0.8981 (0.8908)\taccu 100.000 (99.078)\n",
      "Epoch-74  220 batches\tloss 0.9186 (0.8921)\taccu 98.438 (99.020)\n",
      "Epoch-74  240 batches\tloss 0.8754 (0.8921)\taccu 100.000 (99.023)\n",
      "Epoch-74  260 batches\tloss 0.8497 (0.8920)\taccu 100.000 (99.002)\n",
      "Epoch-74  280 batches\tloss 0.8655 (0.8927)\taccu 100.000 (98.962)\n",
      "Epoch-74  300 batches\tloss 0.8786 (0.8934)\taccu 96.875 (98.927)\n",
      "Epoch-74  320 batches\tloss 0.9375 (0.8938)\taccu 95.312 (98.916)\n",
      "Epoch-74  340 batches\tloss 0.8836 (0.8940)\taccu 98.438 (98.906)\n",
      "Epoch-74  360 batches\tloss 0.8598 (0.8936)\taccu 100.000 (98.924)\n",
      "Epoch-74  380 batches\tloss 0.9252 (0.8939)\taccu 98.438 (98.914)\n",
      "Epoch-74  400 batches\tloss 0.9008 (0.8938)\taccu 96.875 (98.922)\n",
      "Epoch-74  420 batches\tloss 0.8911 (0.8939)\taccu 100.000 (98.910)\n",
      "Epoch-74  440 batches\tloss 0.8811 (0.8946)\taccu 98.438 (98.878)\n",
      "Epoch-74  460 batches\tloss 0.8726 (0.8946)\taccu 100.000 (98.893)\n",
      "Epoch-74  480 batches\tloss 0.9270 (0.8949)\taccu 98.438 (98.880)\n",
      "Epoch-74  89.8s\tTrain: loss 0.8950\taccu 98.8794\tValid: loss 1.2254\taccu 86.0554\n",
      "Epoch 74: val_acc did not improve\n",
      "74 0.0001\n",
      "Epoch-75   20 batches\tloss 0.9183 (0.8970)\taccu 98.438 (99.062)\n",
      "Epoch-75   40 batches\tloss 0.9033 (0.8935)\taccu 98.438 (98.984)\n",
      "Epoch-75   60 batches\tloss 0.9143 (0.8956)\taccu 98.438 (98.984)\n",
      "Epoch-75   80 batches\tloss 0.9170 (0.8974)\taccu 96.875 (98.848)\n",
      "Epoch-75  100 batches\tloss 0.8901 (0.8972)\taccu 98.438 (98.797)\n",
      "Epoch-75  120 batches\tloss 0.8428 (0.8955)\taccu 100.000 (98.893)\n",
      "Epoch-75  140 batches\tloss 0.8850 (0.8939)\taccu 98.438 (98.895)\n",
      "Epoch-75  160 batches\tloss 0.9180 (0.8939)\taccu 98.438 (98.896)\n",
      "Epoch-75  180 batches\tloss 0.8700 (0.8933)\taccu 98.438 (98.889)\n",
      "Epoch-75  200 batches\tloss 0.8867 (0.8929)\taccu 100.000 (98.938)\n",
      "Epoch-75  220 batches\tloss 0.8806 (0.8927)\taccu 98.438 (98.913)\n",
      "Epoch-75  240 batches\tloss 0.8860 (0.8926)\taccu 100.000 (98.919)\n",
      "Epoch-75  260 batches\tloss 0.9135 (0.8926)\taccu 96.875 (98.912)\n",
      "Epoch-75  280 batches\tloss 0.8614 (0.8927)\taccu 98.438 (98.917)\n",
      "Epoch-75  300 batches\tloss 0.9545 (0.8928)\taccu 96.875 (98.911)\n",
      "Epoch-75  320 batches\tloss 0.9037 (0.8932)\taccu 98.438 (98.916)\n",
      "Epoch-75  340 batches\tloss 0.9281 (0.8926)\taccu 98.438 (98.938)\n",
      "Epoch-75  360 batches\tloss 0.9103 (0.8924)\taccu 98.438 (98.937)\n",
      "Epoch-75  380 batches\tloss 0.8988 (0.8923)\taccu 100.000 (98.951)\n",
      "Epoch-75  400 batches\tloss 0.8871 (0.8921)\taccu 98.438 (98.965)\n",
      "Epoch-75  420 batches\tloss 0.8967 (0.8920)\taccu 95.312 (98.958)\n",
      "Epoch-75  440 batches\tloss 0.8740 (0.8920)\taccu 100.000 (98.963)\n",
      "Epoch-75  460 batches\tloss 0.9077 (0.8921)\taccu 98.438 (98.957)\n",
      "Epoch-75  480 batches\tloss 0.9146 (0.8926)\taccu 98.438 (98.929)\n",
      "Epoch-75  89.4s\tTrain: loss 0.8926\taccu 98.9236\tValid: loss 1.2242\taccu 86.1586\n",
      "Epoch 75: val_acc did not improve\n",
      "75 0.0001\n",
      "Epoch-76   20 batches\tloss 0.8840 (0.8853)\taccu 100.000 (99.453)\n",
      "Epoch-76   40 batches\tloss 0.8580 (0.8852)\taccu 100.000 (99.258)\n",
      "Epoch-76   60 batches\tloss 0.8776 (0.8871)\taccu 98.438 (99.141)\n",
      "Epoch-76   80 batches\tloss 0.9211 (0.8877)\taccu 100.000 (99.180)\n",
      "Epoch-76  100 batches\tloss 0.8701 (0.8888)\taccu 98.438 (99.109)\n",
      "Epoch-76  120 batches\tloss 0.8548 (0.8893)\taccu 98.438 (99.076)\n",
      "Epoch-76  140 batches\tloss 0.9120 (0.8879)\taccu 100.000 (99.096)\n",
      "Epoch-76  160 batches\tloss 0.9106 (0.8882)\taccu 100.000 (99.141)\n",
      "Epoch-76  180 batches\tloss 0.8914 (0.8882)\taccu 100.000 (99.089)\n",
      "Epoch-76  200 batches\tloss 0.8969 (0.8887)\taccu 98.438 (99.078)\n",
      "Epoch-76  220 batches\tloss 0.9013 (0.8891)\taccu 96.875 (99.062)\n",
      "Epoch-76  240 batches\tloss 0.8728 (0.8886)\taccu 100.000 (99.049)\n",
      "Epoch-76  260 batches\tloss 0.8898 (0.8882)\taccu 100.000 (99.075)\n",
      "Epoch-76  280 batches\tloss 0.9333 (0.8883)\taccu 100.000 (99.062)\n",
      "Epoch-76  300 batches\tloss 0.9413 (0.8890)\taccu 95.312 (99.042)\n",
      "Epoch-76  320 batches\tloss 0.8678 (0.8893)\taccu 100.000 (99.014)\n",
      "Epoch-76  340 batches\tloss 0.8720 (0.8895)\taccu 100.000 (99.035)\n",
      "Epoch-76  360 batches\tloss 0.8554 (0.8894)\taccu 100.000 (99.045)\n",
      "Epoch-76  380 batches\tloss 0.8791 (0.8899)\taccu 100.000 (99.038)\n",
      "Epoch-76  400 batches\tloss 0.8697 (0.8898)\taccu 100.000 (99.039)\n",
      "Epoch-76  420 batches\tloss 0.8724 (0.8897)\taccu 100.000 (99.029)\n",
      "Epoch-76  440 batches\tloss 0.8966 (0.8895)\taccu 100.000 (99.052)\n",
      "Epoch-76  460 batches\tloss 0.8966 (0.8899)\taccu 98.438 (99.042)\n",
      "Epoch-76  480 batches\tloss 0.8722 (0.8900)\taccu 100.000 (99.033)\n",
      "Epoch-76  89.5s\tTrain: loss 0.8899\taccu 99.0309\tValid: loss 1.2218\taccu 85.8343\n",
      "Epoch 76: val_acc did not improve\n",
      "76 0.0001\n",
      "Epoch-77   20 batches\tloss 0.9059 (0.8912)\taccu 100.000 (98.906)\n",
      "Epoch-77   40 batches\tloss 0.8761 (0.8820)\taccu 100.000 (99.297)\n",
      "Epoch-77   60 batches\tloss 0.8531 (0.8834)\taccu 100.000 (99.401)\n",
      "Epoch-77   80 batches\tloss 0.8968 (0.8836)\taccu 98.438 (99.336)\n",
      "Epoch-77  100 batches\tloss 0.8932 (0.8863)\taccu 98.438 (99.266)\n",
      "Epoch-77  120 batches\tloss 0.9029 (0.8873)\taccu 98.438 (99.167)\n",
      "Epoch-77  140 batches\tloss 0.8885 (0.8872)\taccu 100.000 (99.185)\n",
      "Epoch-77  160 batches\tloss 0.9034 (0.8869)\taccu 100.000 (99.170)\n",
      "Epoch-77  180 batches\tloss 0.8962 (0.8869)\taccu 100.000 (99.184)\n",
      "Epoch-77  200 batches\tloss 0.9015 (0.8867)\taccu 98.438 (99.172)\n",
      "Epoch-77  220 batches\tloss 0.8673 (0.8874)\taccu 100.000 (99.134)\n",
      "Epoch-77  240 batches\tloss 0.9088 (0.8882)\taccu 100.000 (99.089)\n",
      "Epoch-77  260 batches\tloss 0.9082 (0.8887)\taccu 100.000 (99.050)\n",
      "Epoch-77  280 batches\tloss 0.8844 (0.8890)\taccu 100.000 (99.074)\n",
      "Epoch-77  300 batches\tloss 0.8876 (0.8885)\taccu 98.438 (99.068)\n",
      "Epoch-77  320 batches\tloss 0.9506 (0.8886)\taccu 95.312 (99.058)\n",
      "Epoch-77  340 batches\tloss 0.8888 (0.8890)\taccu 100.000 (99.040)\n",
      "Epoch-77  360 batches\tloss 0.8760 (0.8886)\taccu 100.000 (99.054)\n",
      "Epoch-77  380 batches\tloss 0.9436 (0.8889)\taccu 96.875 (99.034)\n",
      "Epoch-77  400 batches\tloss 0.8744 (0.8885)\taccu 100.000 (99.047)\n",
      "Epoch-77  420 batches\tloss 0.8625 (0.8883)\taccu 100.000 (99.033)\n",
      "Epoch-77  440 batches\tloss 0.8792 (0.8884)\taccu 98.438 (99.038)\n",
      "Epoch-77  460 batches\tloss 0.9121 (0.8886)\taccu 96.875 (99.025)\n",
      "Epoch-77  480 batches\tloss 0.9078 (0.8888)\taccu 98.438 (99.027)\n",
      "Epoch-77  90.5s\tTrain: loss 0.8891\taccu 99.0183\tValid: loss 1.2218\taccu 85.8785\n",
      "Epoch 77: val_acc did not improve\n",
      "77 0.0001\n",
      "Epoch-78   20 batches\tloss 0.8739 (0.8870)\taccu 100.000 (99.141)\n",
      "Epoch-78   40 batches\tloss 0.9392 (0.8900)\taccu 98.438 (98.984)\n",
      "Epoch-78   60 batches\tloss 0.9637 (0.8894)\taccu 96.875 (99.010)\n",
      "Epoch-78   80 batches\tloss 0.9020 (0.8888)\taccu 100.000 (98.984)\n",
      "Epoch-78  100 batches\tloss 0.8934 (0.8864)\taccu 96.875 (99.000)\n",
      "Epoch-78  120 batches\tloss 0.9141 (0.8857)\taccu 96.875 (98.997)\n",
      "Epoch-78  140 batches\tloss 0.8495 (0.8863)\taccu 100.000 (98.996)\n",
      "Epoch-78  160 batches\tloss 0.8520 (0.8857)\taccu 100.000 (99.004)\n",
      "Epoch-78  180 batches\tloss 0.8719 (0.8857)\taccu 100.000 (99.028)\n",
      "Epoch-78  200 batches\tloss 0.8638 (0.8854)\taccu 98.438 (99.070)\n",
      "Epoch-78  220 batches\tloss 0.8728 (0.8861)\taccu 100.000 (99.041)\n",
      "Epoch-78  240 batches\tloss 0.9491 (0.8866)\taccu 98.438 (99.023)\n",
      "Epoch-78  260 batches\tloss 0.9339 (0.8867)\taccu 96.875 (99.026)\n",
      "Epoch-78  280 batches\tloss 0.9112 (0.8876)\taccu 98.438 (98.968)\n",
      "Epoch-78  300 batches\tloss 0.8576 (0.8872)\taccu 100.000 (98.984)\n",
      "Epoch-78  320 batches\tloss 0.9278 (0.8873)\taccu 96.875 (98.989)\n",
      "Epoch-78  340 batches\tloss 0.8847 (0.8877)\taccu 100.000 (98.984)\n",
      "Epoch-78  360 batches\tloss 0.8953 (0.8880)\taccu 100.000 (98.976)\n",
      "Epoch-78  380 batches\tloss 0.8752 (0.8882)\taccu 98.438 (98.980)\n",
      "Epoch-78  400 batches\tloss 0.8735 (0.8882)\taccu 98.438 (98.984)\n",
      "Epoch-78  420 batches\tloss 0.8493 (0.8880)\taccu 100.000 (98.999)\n",
      "Epoch-78  440 batches\tloss 0.8721 (0.8884)\taccu 98.438 (98.988)\n",
      "Epoch-78  460 batches\tloss 0.8667 (0.8887)\taccu 100.000 (98.998)\n",
      "Epoch-78  480 batches\tloss 0.8759 (0.8888)\taccu 100.000 (98.991)\n",
      "Epoch-78  89.5s\tTrain: loss 0.8886\taccu 99.0057\tValid: loss 1.2249\taccu 86.3208\n",
      "Epoch 78: val_acc improved from 86.2471 to 86.3208, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "78 0.0001\n",
      "Epoch-79   20 batches\tloss 0.8509 (0.8879)\taccu 100.000 (98.906)\n",
      "Epoch-79   40 batches\tloss 0.8987 (0.8863)\taccu 100.000 (99.219)\n",
      "Epoch-79   60 batches\tloss 0.9242 (0.8847)\taccu 98.438 (99.323)\n",
      "Epoch-79   80 batches\tloss 0.8418 (0.8830)\taccu 100.000 (99.336)\n",
      "Epoch-79  100 batches\tloss 0.8750 (0.8819)\taccu 100.000 (99.375)\n",
      "Epoch-79  120 batches\tloss 0.8750 (0.8819)\taccu 96.875 (99.349)\n",
      "Epoch-79  140 batches\tloss 0.9261 (0.8825)\taccu 98.438 (99.263)\n",
      "Epoch-79  160 batches\tloss 0.8627 (0.8837)\taccu 100.000 (99.199)\n",
      "Epoch-79  180 batches\tloss 0.9132 (0.8837)\taccu 96.875 (99.219)\n",
      "Epoch-79  200 batches\tloss 0.8911 (0.8841)\taccu 98.438 (99.219)\n",
      "Epoch-79  220 batches\tloss 0.8695 (0.8849)\taccu 98.438 (99.169)\n",
      "Epoch-79  240 batches\tloss 0.8531 (0.8850)\taccu 100.000 (99.128)\n",
      "Epoch-79  260 batches\tloss 0.8734 (0.8850)\taccu 100.000 (99.117)\n",
      "Epoch-79  280 batches\tloss 0.9584 (0.8857)\taccu 98.438 (99.113)\n",
      "Epoch-79  300 batches\tloss 0.8852 (0.8856)\taccu 98.438 (99.109)\n",
      "Epoch-79  320 batches\tloss 0.8784 (0.8853)\taccu 100.000 (99.131)\n",
      "Epoch-79  340 batches\tloss 0.8755 (0.8861)\taccu 98.438 (99.108)\n",
      "Epoch-79  360 batches\tloss 0.9140 (0.8862)\taccu 98.438 (99.097)\n",
      "Epoch-79  380 batches\tloss 0.8355 (0.8866)\taccu 100.000 (99.079)\n",
      "Epoch-79  400 batches\tloss 0.8972 (0.8865)\taccu 98.438 (99.082)\n",
      "Epoch-79  420 batches\tloss 0.8832 (0.8866)\taccu 100.000 (99.089)\n",
      "Epoch-79  440 batches\tloss 0.8982 (0.8869)\taccu 96.875 (99.059)\n",
      "Epoch-79  460 batches\tloss 0.8744 (0.8871)\taccu 100.000 (99.042)\n",
      "Epoch-79  480 batches\tloss 0.8945 (0.8873)\taccu 98.438 (99.033)\n",
      "Epoch-79  89.7s\tTrain: loss 0.8875\taccu 99.0215\tValid: loss 1.2279\taccu 85.9670\n",
      "Epoch 79: val_acc did not improve\n",
      "79 0.0001\n",
      "Epoch-80   20 batches\tloss 0.8879 (0.8826)\taccu 95.312 (99.141)\n",
      "Epoch-80   40 batches\tloss 0.8822 (0.8808)\taccu 100.000 (99.336)\n",
      "Epoch-80   60 batches\tloss 0.8483 (0.8804)\taccu 100.000 (99.245)\n",
      "Epoch-80   80 batches\tloss 0.8302 (0.8777)\taccu 100.000 (99.277)\n",
      "Epoch-80  100 batches\tloss 0.9053 (0.8797)\taccu 96.875 (99.188)\n",
      "Epoch-80  120 batches\tloss 0.8977 (0.8804)\taccu 98.438 (99.128)\n",
      "Epoch-80  140 batches\tloss 0.8670 (0.8804)\taccu 100.000 (99.163)\n",
      "Epoch-80  160 batches\tloss 0.9017 (0.8821)\taccu 98.438 (99.121)\n",
      "Epoch-80  180 batches\tloss 0.8664 (0.8829)\taccu 100.000 (99.106)\n",
      "Epoch-80  200 batches\tloss 0.8650 (0.8835)\taccu 100.000 (99.094)\n",
      "Epoch-80  220 batches\tloss 0.8978 (0.8840)\taccu 100.000 (99.105)\n",
      "Epoch-80  240 batches\tloss 0.8577 (0.8835)\taccu 100.000 (99.115)\n",
      "Epoch-80  260 batches\tloss 0.8626 (0.8832)\taccu 98.438 (99.111)\n",
      "Epoch-80  280 batches\tloss 0.8875 (0.8828)\taccu 100.000 (99.141)\n",
      "Epoch-80  300 batches\tloss 0.8481 (0.8829)\taccu 98.438 (99.125)\n",
      "Epoch-80  320 batches\tloss 0.8543 (0.8835)\taccu 100.000 (99.121)\n",
      "Epoch-80  340 batches\tloss 0.8769 (0.8834)\taccu 100.000 (99.131)\n",
      "Epoch-80  360 batches\tloss 0.8798 (0.8838)\taccu 98.438 (99.110)\n",
      "Epoch-80  380 batches\tloss 0.8924 (0.8842)\taccu 100.000 (99.116)\n",
      "Epoch-80  400 batches\tloss 0.8622 (0.8843)\taccu 100.000 (99.094)\n",
      "Epoch-80  420 batches\tloss 0.8441 (0.8837)\taccu 100.000 (99.111)\n",
      "Epoch-80  440 batches\tloss 0.8699 (0.8839)\taccu 100.000 (99.102)\n",
      "Epoch-80  460 batches\tloss 0.9103 (0.8841)\taccu 100.000 (99.096)\n",
      "Epoch-80  480 batches\tloss 0.8952 (0.8842)\taccu 100.000 (99.105)\n",
      "Epoch-80  89.6s\tTrain: loss 0.8842\taccu 99.1099\tValid: loss 1.2273\taccu 85.9670\n",
      "Epoch 80: val_acc did not improve\n",
      "80 0.0001\n",
      "Epoch-81   20 batches\tloss 0.9008 (0.8833)\taccu 100.000 (99.375)\n",
      "Epoch-81   40 batches\tloss 0.9160 (0.8808)\taccu 96.875 (99.414)\n",
      "Epoch-81   60 batches\tloss 0.8643 (0.8838)\taccu 100.000 (99.193)\n",
      "Epoch-81   80 batches\tloss 0.8498 (0.8836)\taccu 100.000 (99.121)\n",
      "Epoch-81  100 batches\tloss 0.8591 (0.8834)\taccu 100.000 (99.203)\n",
      "Epoch-81  120 batches\tloss 0.8495 (0.8842)\taccu 98.438 (99.180)\n",
      "Epoch-81  140 batches\tloss 0.8635 (0.8836)\taccu 100.000 (99.185)\n",
      "Epoch-81  160 batches\tloss 0.8706 (0.8836)\taccu 100.000 (99.180)\n",
      "Epoch-81  180 batches\tloss 0.9331 (0.8835)\taccu 95.312 (99.175)\n",
      "Epoch-81  200 batches\tloss 0.8428 (0.8827)\taccu 100.000 (99.172)\n",
      "Epoch-81  220 batches\tloss 0.8635 (0.8819)\taccu 100.000 (99.190)\n",
      "Epoch-81  240 batches\tloss 0.8697 (0.8821)\taccu 100.000 (99.180)\n",
      "Epoch-81  260 batches\tloss 0.8731 (0.8826)\taccu 100.000 (99.165)\n",
      "Epoch-81  280 batches\tloss 0.8722 (0.8828)\taccu 100.000 (99.191)\n",
      "Epoch-81  300 batches\tloss 0.8880 (0.8828)\taccu 98.438 (99.208)\n",
      "Epoch-81  320 batches\tloss 0.8933 (0.8826)\taccu 98.438 (99.209)\n",
      "Epoch-81  340 batches\tloss 0.8933 (0.8830)\taccu 98.438 (99.191)\n",
      "Epoch-81  360 batches\tloss 0.8823 (0.8835)\taccu 98.438 (99.184)\n",
      "Epoch-81  380 batches\tloss 0.8687 (0.8834)\taccu 100.000 (99.169)\n",
      "Epoch-81  400 batches\tloss 0.8518 (0.8835)\taccu 100.000 (99.160)\n",
      "Epoch-81  420 batches\tloss 0.8563 (0.8834)\taccu 100.000 (99.170)\n",
      "Epoch-81  440 batches\tloss 0.8599 (0.8835)\taccu 100.000 (99.155)\n",
      "Epoch-81  460 batches\tloss 0.8586 (0.8840)\taccu 100.000 (99.141)\n",
      "Epoch-81  480 batches\tloss 0.8844 (0.8840)\taccu 100.000 (99.134)\n",
      "Epoch-81  89.4s\tTrain: loss 0.8841\taccu 99.1288\tValid: loss 1.2248\taccu 86.1881\n",
      "Epoch 81: val_acc did not improve\n",
      "81 0.0001\n",
      "Epoch-82   20 batches\tloss 0.8749 (0.8794)\taccu 98.438 (99.297)\n",
      "Epoch-82   40 batches\tloss 0.8520 (0.8808)\taccu 100.000 (99.258)\n",
      "Epoch-82   60 batches\tloss 0.8844 (0.8795)\taccu 98.438 (99.245)\n",
      "Epoch-82   80 batches\tloss 0.8938 (0.8807)\taccu 96.875 (99.219)\n",
      "Epoch-82  100 batches\tloss 0.8420 (0.8815)\taccu 100.000 (99.109)\n",
      "Epoch-82  120 batches\tloss 0.9487 (0.8821)\taccu 98.438 (99.115)\n",
      "Epoch-82  140 batches\tloss 0.8666 (0.8831)\taccu 98.438 (99.051)\n",
      "Epoch-82  160 batches\tloss 0.8540 (0.8824)\taccu 100.000 (99.062)\n",
      "Epoch-82  180 batches\tloss 0.8360 (0.8821)\taccu 100.000 (99.097)\n",
      "Epoch-82  200 batches\tloss 0.9068 (0.8824)\taccu 98.438 (99.070)\n",
      "Epoch-82  220 batches\tloss 0.8840 (0.8828)\taccu 100.000 (99.112)\n",
      "Epoch-82  240 batches\tloss 0.8677 (0.8831)\taccu 100.000 (99.128)\n",
      "Epoch-82  260 batches\tloss 0.8779 (0.8830)\taccu 100.000 (99.147)\n",
      "Epoch-82  280 batches\tloss 0.8693 (0.8829)\taccu 100.000 (99.174)\n",
      "Epoch-82  300 batches\tloss 0.8560 (0.8824)\taccu 100.000 (99.198)\n",
      "Epoch-82  320 batches\tloss 0.8689 (0.8822)\taccu 100.000 (99.209)\n",
      "Epoch-82  340 batches\tloss 0.8667 (0.8820)\taccu 100.000 (99.210)\n",
      "Epoch-82  360 batches\tloss 0.9312 (0.8821)\taccu 100.000 (99.214)\n",
      "Epoch-82  380 batches\tloss 0.8733 (0.8826)\taccu 98.438 (99.174)\n",
      "Epoch-82  400 batches\tloss 0.8600 (0.8825)\taccu 98.438 (99.168)\n",
      "Epoch-82  420 batches\tloss 0.8708 (0.8824)\taccu 100.000 (99.174)\n",
      "Epoch-82  440 batches\tloss 0.8631 (0.8827)\taccu 100.000 (99.176)\n",
      "Epoch-82  460 batches\tloss 0.8826 (0.8829)\taccu 100.000 (99.171)\n",
      "Epoch-82  480 batches\tloss 0.8937 (0.8830)\taccu 96.875 (99.170)\n",
      "Epoch-82  89.0s\tTrain: loss 0.8832\taccu 99.1667\tValid: loss 1.2225\taccu 85.8343\n",
      "Epoch 82: val_acc did not improve\n",
      "82 0.0001\n",
      "Epoch-83   20 batches\tloss 0.8964 (0.8752)\taccu 98.438 (99.375)\n",
      "Epoch-83   40 batches\tloss 0.8455 (0.8778)\taccu 100.000 (99.258)\n",
      "Epoch-83   60 batches\tloss 0.9467 (0.8801)\taccu 95.312 (99.167)\n",
      "Epoch-83   80 batches\tloss 0.8454 (0.8812)\taccu 100.000 (99.219)\n",
      "Epoch-83  100 batches\tloss 0.8776 (0.8818)\taccu 98.438 (99.156)\n",
      "Epoch-83  120 batches\tloss 0.8668 (0.8812)\taccu 100.000 (99.128)\n",
      "Epoch-83  140 batches\tloss 0.8648 (0.8813)\taccu 100.000 (99.163)\n",
      "Epoch-83  160 batches\tloss 0.8582 (0.8805)\taccu 100.000 (99.199)\n",
      "Epoch-83  180 batches\tloss 0.8995 (0.8797)\taccu 98.438 (99.184)\n",
      "Epoch-83  200 batches\tloss 0.8606 (0.8800)\taccu 100.000 (99.195)\n",
      "Epoch-83  220 batches\tloss 0.8893 (0.8798)\taccu 100.000 (99.212)\n",
      "Epoch-83  240 batches\tloss 0.8893 (0.8799)\taccu 100.000 (99.212)\n",
      "Epoch-83  260 batches\tloss 0.8979 (0.8800)\taccu 96.875 (99.201)\n",
      "Epoch-83  280 batches\tloss 0.9168 (0.8801)\taccu 98.438 (99.208)\n",
      "Epoch-83  300 batches\tloss 0.8403 (0.8806)\taccu 100.000 (99.193)\n",
      "Epoch-83  320 batches\tloss 0.8424 (0.8806)\taccu 100.000 (99.180)\n",
      "Epoch-83  340 batches\tloss 0.8769 (0.8808)\taccu 98.438 (99.182)\n",
      "Epoch-83  360 batches\tloss 0.8567 (0.8812)\taccu 100.000 (99.180)\n",
      "Epoch-83  380 batches\tloss 0.8806 (0.8815)\taccu 98.438 (99.153)\n",
      "Epoch-83  400 batches\tloss 0.8635 (0.8814)\taccu 100.000 (99.145)\n",
      "Epoch-83  420 batches\tloss 0.8974 (0.8814)\taccu 96.875 (99.129)\n",
      "Epoch-83  440 batches\tloss 0.9037 (0.8813)\taccu 100.000 (99.148)\n",
      "Epoch-83  460 batches\tloss 0.9190 (0.8816)\taccu 98.438 (99.141)\n",
      "Epoch-83  480 batches\tloss 0.8886 (0.8813)\taccu 100.000 (99.157)\n",
      "Epoch-83  89.0s\tTrain: loss 0.8816\taccu 99.1509\tValid: loss 1.2221\taccu 86.2471\n",
      "Epoch 83: val_acc did not improve\n",
      "83 0.0001\n",
      "Epoch-84   20 batches\tloss 0.9089 (0.8748)\taccu 96.875 (99.219)\n",
      "Epoch-84   40 batches\tloss 0.8659 (0.8737)\taccu 100.000 (99.297)\n",
      "Epoch-84   60 batches\tloss 0.8708 (0.8779)\taccu 100.000 (99.271)\n",
      "Epoch-84   80 batches\tloss 0.8758 (0.8774)\taccu 98.438 (99.277)\n",
      "Epoch-84  100 batches\tloss 0.8813 (0.8788)\taccu 100.000 (99.266)\n",
      "Epoch-84  120 batches\tloss 0.9294 (0.8800)\taccu 96.875 (99.167)\n",
      "Epoch-84  140 batches\tloss 0.8524 (0.8793)\taccu 100.000 (99.185)\n",
      "Epoch-84  160 batches\tloss 0.9149 (0.8793)\taccu 98.438 (99.160)\n",
      "Epoch-84  180 batches\tloss 0.8486 (0.8782)\taccu 100.000 (99.184)\n",
      "Epoch-84  200 batches\tloss 0.8962 (0.8787)\taccu 100.000 (99.195)\n",
      "Epoch-84  220 batches\tloss 0.9189 (0.8788)\taccu 98.438 (99.205)\n",
      "Epoch-84  240 batches\tloss 0.8894 (0.8791)\taccu 98.438 (99.193)\n",
      "Epoch-84  260 batches\tloss 0.9017 (0.8802)\taccu 98.438 (99.171)\n",
      "Epoch-84  280 batches\tloss 0.8932 (0.8805)\taccu 98.438 (99.163)\n",
      "Epoch-84  300 batches\tloss 0.8758 (0.8808)\taccu 100.000 (99.161)\n",
      "Epoch-84  320 batches\tloss 0.8876 (0.8810)\taccu 98.438 (99.160)\n",
      "Epoch-84  340 batches\tloss 0.9031 (0.8810)\taccu 98.438 (99.182)\n",
      "Epoch-84  360 batches\tloss 0.8902 (0.8808)\taccu 98.438 (99.188)\n",
      "Epoch-84  380 batches\tloss 0.9560 (0.8809)\taccu 95.312 (99.190)\n",
      "Epoch-84  400 batches\tloss 0.8679 (0.8810)\taccu 100.000 (99.207)\n",
      "Epoch-84  420 batches\tloss 0.8836 (0.8811)\taccu 98.438 (99.189)\n",
      "Epoch-84  440 batches\tloss 0.8979 (0.8812)\taccu 96.875 (99.169)\n",
      "Epoch-84  460 batches\tloss 0.8844 (0.8812)\taccu 98.438 (99.175)\n",
      "Epoch-84  480 batches\tloss 0.9064 (0.8815)\taccu 100.000 (99.167)\n",
      "Epoch-84  89.3s\tTrain: loss 0.8815\taccu 99.1667\tValid: loss 1.2247\taccu 86.1733\n",
      "Epoch 84: val_acc did not improve\n",
      "84 0.0001\n",
      "Epoch-85   20 batches\tloss 0.8776 (0.8820)\taccu 98.438 (98.984)\n",
      "Epoch-85   40 batches\tloss 0.8755 (0.8837)\taccu 100.000 (99.023)\n",
      "Epoch-85   60 batches\tloss 0.8685 (0.8814)\taccu 100.000 (99.141)\n",
      "Epoch-85   80 batches\tloss 0.8476 (0.8792)\taccu 100.000 (99.160)\n",
      "Epoch-85  100 batches\tloss 0.8779 (0.8763)\taccu 98.438 (99.203)\n",
      "Epoch-85  120 batches\tloss 0.8882 (0.8785)\taccu 98.438 (99.102)\n",
      "Epoch-85  140 batches\tloss 0.8913 (0.8780)\taccu 98.438 (99.141)\n",
      "Epoch-85  160 batches\tloss 0.9569 (0.8788)\taccu 95.312 (99.141)\n",
      "Epoch-85  180 batches\tloss 0.9222 (0.8788)\taccu 96.875 (99.115)\n",
      "Epoch-85  200 batches\tloss 0.8618 (0.8801)\taccu 100.000 (99.117)\n",
      "Epoch-85  220 batches\tloss 0.8981 (0.8801)\taccu 98.438 (99.148)\n",
      "Epoch-85  240 batches\tloss 0.9331 (0.8799)\taccu 95.312 (99.134)\n",
      "Epoch-85  260 batches\tloss 0.8618 (0.8804)\taccu 100.000 (99.165)\n",
      "Epoch-85  280 batches\tloss 0.8754 (0.8802)\taccu 98.438 (99.152)\n",
      "Epoch-85  300 batches\tloss 0.8579 (0.8804)\taccu 100.000 (99.125)\n",
      "Epoch-85  320 batches\tloss 0.8802 (0.8805)\taccu 98.438 (99.121)\n",
      "Epoch-85  340 batches\tloss 0.8544 (0.8803)\taccu 100.000 (99.104)\n",
      "Epoch-85  360 batches\tloss 0.9002 (0.8804)\taccu 98.438 (99.097)\n",
      "Epoch-85  380 batches\tloss 0.8712 (0.8804)\taccu 100.000 (99.112)\n",
      "Epoch-85  400 batches\tloss 0.9027 (0.8812)\taccu 98.438 (99.098)\n",
      "Epoch-85  420 batches\tloss 0.8662 (0.8809)\taccu 100.000 (99.115)\n",
      "Epoch-85  440 batches\tloss 0.8489 (0.8808)\taccu 100.000 (99.130)\n",
      "Epoch-85  460 batches\tloss 0.8885 (0.8808)\taccu 100.000 (99.130)\n",
      "Epoch-85  480 batches\tloss 0.9017 (0.8810)\taccu 98.438 (99.134)\n",
      "Epoch-85  88.9s\tTrain: loss 0.8809\taccu 99.1414\tValid: loss 1.2222\taccu 86.1733\n",
      "Epoch 85: val_acc did not improve\n",
      "85 0.0001\n",
      "Epoch-86   20 batches\tloss 0.8667 (0.8786)\taccu 100.000 (99.375)\n",
      "Epoch-86   40 batches\tloss 0.8521 (0.8759)\taccu 100.000 (99.414)\n",
      "Epoch-86   60 batches\tloss 0.8627 (0.8763)\taccu 100.000 (99.375)\n",
      "Epoch-86   80 batches\tloss 0.8502 (0.8752)\taccu 100.000 (99.395)\n",
      "Epoch-86  100 batches\tloss 0.8740 (0.8769)\taccu 100.000 (99.422)\n",
      "Epoch-86  120 batches\tloss 0.8732 (0.8770)\taccu 98.438 (99.349)\n",
      "Epoch-86  140 batches\tloss 0.9393 (0.8769)\taccu 100.000 (99.342)\n",
      "Epoch-86  160 batches\tloss 0.8308 (0.8764)\taccu 100.000 (99.316)\n",
      "Epoch-86  180 batches\tloss 0.9297 (0.8770)\taccu 98.438 (99.306)\n",
      "Epoch-86  200 batches\tloss 0.8924 (0.8779)\taccu 100.000 (99.289)\n",
      "Epoch-86  220 batches\tloss 0.8878 (0.8782)\taccu 100.000 (99.276)\n",
      "Epoch-86  240 batches\tloss 0.8936 (0.8786)\taccu 100.000 (99.277)\n",
      "Epoch-86  260 batches\tloss 0.9216 (0.8790)\taccu 98.438 (99.273)\n",
      "Epoch-86  280 batches\tloss 0.8873 (0.8791)\taccu 98.438 (99.291)\n",
      "Epoch-86  300 batches\tloss 0.8930 (0.8793)\taccu 100.000 (99.297)\n",
      "Epoch-86  320 batches\tloss 0.8815 (0.8797)\taccu 98.438 (99.287)\n",
      "Epoch-86  340 batches\tloss 0.8350 (0.8794)\taccu 100.000 (99.292)\n",
      "Epoch-86  360 batches\tloss 0.8429 (0.8794)\taccu 100.000 (99.288)\n",
      "Epoch-86  380 batches\tloss 0.8845 (0.8794)\taccu 98.438 (99.280)\n",
      "Epoch-86  400 batches\tloss 0.8558 (0.8790)\taccu 100.000 (99.277)\n",
      "Epoch-86  420 batches\tloss 0.9148 (0.8794)\taccu 100.000 (99.282)\n",
      "Epoch-86  440 batches\tloss 0.8807 (0.8791)\taccu 100.000 (99.283)\n",
      "Epoch-86  460 batches\tloss 0.9361 (0.8794)\taccu 100.000 (99.280)\n",
      "Epoch-86  480 batches\tloss 0.8874 (0.8797)\taccu 100.000 (99.264)\n",
      "Epoch-86  89.2s\tTrain: loss 0.8797\taccu 99.2740\tValid: loss 1.2350\taccu 85.4363\n",
      "Epoch 86: val_acc did not improve\n",
      "86 0.0001\n",
      "Epoch-87   20 batches\tloss 0.8530 (0.8816)\taccu 98.438 (99.219)\n",
      "Epoch-87   40 batches\tloss 0.8353 (0.8833)\taccu 100.000 (99.062)\n",
      "Epoch-87   60 batches\tloss 0.8740 (0.8824)\taccu 100.000 (99.115)\n",
      "Epoch-87   80 batches\tloss 0.8862 (0.8809)\taccu 98.438 (99.082)\n",
      "Epoch-87  100 batches\tloss 0.8776 (0.8780)\taccu 98.438 (99.094)\n",
      "Epoch-87  120 batches\tloss 0.8778 (0.8780)\taccu 100.000 (99.115)\n",
      "Epoch-87  140 batches\tloss 0.8493 (0.8771)\taccu 98.438 (99.185)\n",
      "Epoch-87  160 batches\tloss 0.8498 (0.8771)\taccu 100.000 (99.238)\n",
      "Epoch-87  180 batches\tloss 0.8730 (0.8768)\taccu 100.000 (99.253)\n",
      "Epoch-87  200 batches\tloss 0.8738 (0.8766)\taccu 100.000 (99.266)\n",
      "Epoch-87  220 batches\tloss 0.9205 (0.8767)\taccu 95.312 (99.240)\n",
      "Epoch-87  240 batches\tloss 0.8784 (0.8772)\taccu 98.438 (99.232)\n",
      "Epoch-87  260 batches\tloss 0.8701 (0.8772)\taccu 98.438 (99.237)\n",
      "Epoch-87  280 batches\tloss 0.9397 (0.8771)\taccu 98.438 (99.252)\n",
      "Epoch-87  300 batches\tloss 0.8912 (0.8773)\taccu 98.438 (99.250)\n",
      "Epoch-87  320 batches\tloss 0.8691 (0.8773)\taccu 100.000 (99.238)\n",
      "Epoch-87  340 batches\tloss 0.9037 (0.8777)\taccu 96.875 (99.210)\n",
      "Epoch-87  360 batches\tloss 0.8663 (0.8782)\taccu 100.000 (99.201)\n",
      "Epoch-87  380 batches\tloss 0.8869 (0.8784)\taccu 100.000 (99.190)\n",
      "Epoch-87  400 batches\tloss 0.8329 (0.8784)\taccu 100.000 (99.203)\n",
      "Epoch-87  420 batches\tloss 0.9120 (0.8783)\taccu 96.875 (99.193)\n",
      "Epoch-87  440 batches\tloss 0.9066 (0.8787)\taccu 96.875 (99.194)\n",
      "Epoch-87  460 batches\tloss 0.8848 (0.8788)\taccu 98.438 (99.178)\n",
      "Epoch-87  480 batches\tloss 0.9190 (0.8788)\taccu 96.875 (99.180)\n",
      "Epoch-87  89.5s\tTrain: loss 0.8790\taccu 99.1698\tValid: loss 1.2267\taccu 86.2323\n",
      "Epoch 87: val_acc did not improve\n",
      "87 0.0001\n",
      "Epoch-88   20 batches\tloss 0.8566 (0.8741)\taccu 100.000 (99.453)\n",
      "Epoch-88   40 batches\tloss 0.8882 (0.8736)\taccu 100.000 (99.219)\n",
      "Epoch-88   60 batches\tloss 0.8582 (0.8759)\taccu 100.000 (99.115)\n",
      "Epoch-88   80 batches\tloss 0.8606 (0.8771)\taccu 100.000 (99.082)\n",
      "Epoch-88  100 batches\tloss 0.8524 (0.8756)\taccu 98.438 (99.156)\n",
      "Epoch-88  120 batches\tloss 0.8968 (0.8765)\taccu 96.875 (99.154)\n",
      "Epoch-88  140 batches\tloss 0.8596 (0.8757)\taccu 100.000 (99.185)\n",
      "Epoch-88  160 batches\tloss 0.8339 (0.8748)\taccu 100.000 (99.219)\n",
      "Epoch-88  180 batches\tloss 0.8952 (0.8755)\taccu 98.438 (99.201)\n",
      "Epoch-88  200 batches\tloss 0.8721 (0.8768)\taccu 100.000 (99.188)\n",
      "Epoch-88  220 batches\tloss 0.8827 (0.8768)\taccu 100.000 (99.183)\n",
      "Epoch-88  240 batches\tloss 0.8831 (0.8765)\taccu 98.438 (99.219)\n",
      "Epoch-88  260 batches\tloss 0.8792 (0.8763)\taccu 100.000 (99.207)\n",
      "Epoch-88  280 batches\tloss 0.8555 (0.8763)\taccu 100.000 (99.219)\n",
      "Epoch-88  300 batches\tloss 0.8456 (0.8765)\taccu 100.000 (99.208)\n",
      "Epoch-88  320 batches\tloss 0.8509 (0.8764)\taccu 100.000 (99.199)\n",
      "Epoch-88  340 batches\tloss 0.8921 (0.8762)\taccu 100.000 (99.214)\n",
      "Epoch-88  360 batches\tloss 0.8690 (0.8763)\taccu 98.438 (99.201)\n",
      "Epoch-88  380 batches\tloss 0.8596 (0.8759)\taccu 100.000 (99.211)\n",
      "Epoch-88  400 batches\tloss 0.8793 (0.8760)\taccu 100.000 (99.211)\n",
      "Epoch-88  420 batches\tloss 0.8317 (0.8756)\taccu 100.000 (99.219)\n",
      "Epoch-88  440 batches\tloss 0.8484 (0.8755)\taccu 100.000 (99.219)\n",
      "Epoch-88  460 batches\tloss 0.9002 (0.8761)\taccu 98.438 (99.209)\n",
      "Epoch-88  480 batches\tloss 0.8962 (0.8766)\taccu 96.875 (99.167)\n",
      "Epoch-88  89.4s\tTrain: loss 0.8770\taccu 99.1698\tValid: loss 1.2259\taccu 86.1439\n",
      "Epoch 88: val_acc did not improve\n",
      "88 0.0001\n",
      "Epoch-89   20 batches\tloss 0.8573 (0.8759)\taccu 100.000 (99.141)\n",
      "Epoch-89   40 batches\tloss 0.8720 (0.8780)\taccu 100.000 (99.180)\n",
      "Epoch-89   60 batches\tloss 0.8766 (0.8810)\taccu 100.000 (99.167)\n",
      "Epoch-89   80 batches\tloss 0.8774 (0.8799)\taccu 100.000 (99.238)\n",
      "Epoch-89  100 batches\tloss 0.8895 (0.8803)\taccu 100.000 (99.250)\n",
      "Epoch-89  120 batches\tloss 0.8647 (0.8796)\taccu 98.438 (99.245)\n",
      "Epoch-89  140 batches\tloss 0.8733 (0.8787)\taccu 100.000 (99.308)\n",
      "Epoch-89  160 batches\tloss 0.9213 (0.8804)\taccu 98.438 (99.297)\n",
      "Epoch-89  180 batches\tloss 0.8774 (0.8798)\taccu 100.000 (99.323)\n",
      "Epoch-89  200 batches\tloss 0.8560 (0.8788)\taccu 100.000 (99.328)\n",
      "Epoch-89  220 batches\tloss 0.8607 (0.8778)\taccu 100.000 (99.347)\n",
      "Epoch-89  240 batches\tloss 0.8827 (0.8772)\taccu 96.875 (99.336)\n",
      "Epoch-89  260 batches\tloss 0.8658 (0.8768)\taccu 100.000 (99.339)\n",
      "Epoch-89  280 batches\tloss 0.9187 (0.8769)\taccu 98.438 (99.325)\n",
      "Epoch-89  300 batches\tloss 0.8739 (0.8769)\taccu 100.000 (99.318)\n",
      "Epoch-89  320 batches\tloss 0.8649 (0.8770)\taccu 98.438 (99.302)\n",
      "Epoch-89  340 batches\tloss 0.8564 (0.8769)\taccu 100.000 (99.311)\n",
      "Epoch-89  360 batches\tloss 0.9039 (0.8771)\taccu 95.312 (99.293)\n",
      "Epoch-89  380 batches\tloss 0.8641 (0.8770)\taccu 98.438 (99.293)\n",
      "Epoch-89  400 batches\tloss 0.8828 (0.8773)\taccu 98.438 (99.289)\n",
      "Epoch-89  420 batches\tloss 0.8756 (0.8773)\taccu 100.000 (99.293)\n",
      "Epoch-89  440 batches\tloss 0.8864 (0.8779)\taccu 98.438 (99.293)\n",
      "Epoch-89  460 batches\tloss 0.8855 (0.8781)\taccu 98.438 (99.273)\n",
      "Epoch-89  480 batches\tloss 0.8786 (0.8781)\taccu 98.438 (99.264)\n",
      "Epoch-89  89.4s\tTrain: loss 0.8780\taccu 99.2551\tValid: loss 1.2256\taccu 85.9522\n",
      "Epoch 89: val_acc did not improve\n",
      "89 0.0001\n",
      "Epoch-90   20 batches\tloss 0.9004 (0.8819)\taccu 98.438 (98.906)\n",
      "Epoch-90   40 batches\tloss 0.8386 (0.8747)\taccu 100.000 (99.141)\n",
      "Epoch-90   60 batches\tloss 0.8954 (0.8764)\taccu 96.875 (99.036)\n",
      "Epoch-90   80 batches\tloss 0.8981 (0.8772)\taccu 98.438 (99.121)\n",
      "Epoch-90  100 batches\tloss 0.8557 (0.8757)\taccu 100.000 (99.188)\n",
      "Epoch-90  120 batches\tloss 0.8552 (0.8759)\taccu 100.000 (99.206)\n",
      "Epoch-90  140 batches\tloss 0.8629 (0.8739)\taccu 100.000 (99.263)\n",
      "Epoch-90  160 batches\tloss 0.9354 (0.8750)\taccu 96.875 (99.238)\n",
      "Epoch-90  180 batches\tloss 0.8456 (0.8752)\taccu 100.000 (99.245)\n",
      "Epoch-90  200 batches\tloss 0.9005 (0.8750)\taccu 98.438 (99.234)\n",
      "Epoch-90  220 batches\tloss 0.8480 (0.8749)\taccu 100.000 (99.197)\n",
      "Epoch-90  240 batches\tloss 0.8600 (0.8754)\taccu 100.000 (99.180)\n",
      "Epoch-90  260 batches\tloss 0.8598 (0.8761)\taccu 100.000 (99.165)\n",
      "Epoch-90  280 batches\tloss 0.9438 (0.8758)\taccu 100.000 (99.208)\n",
      "Epoch-90  300 batches\tloss 0.8550 (0.8757)\taccu 98.438 (99.219)\n",
      "Epoch-90  320 batches\tloss 0.8826 (0.8756)\taccu 100.000 (99.238)\n",
      "Epoch-90  340 batches\tloss 0.8922 (0.8760)\taccu 100.000 (99.256)\n",
      "Epoch-90  360 batches\tloss 0.8974 (0.8764)\taccu 100.000 (99.219)\n",
      "Epoch-90  380 batches\tloss 0.8849 (0.8761)\taccu 98.438 (99.235)\n",
      "Epoch-90  400 batches\tloss 0.8715 (0.8761)\taccu 100.000 (99.223)\n",
      "Epoch-90  420 batches\tloss 0.8347 (0.8761)\taccu 100.000 (99.234)\n",
      "Epoch-90  440 batches\tloss 0.9177 (0.8763)\taccu 98.438 (99.222)\n",
      "Epoch-90  460 batches\tloss 0.8623 (0.8764)\taccu 98.438 (99.229)\n",
      "Epoch-90  480 batches\tloss 0.8634 (0.8767)\taccu 100.000 (99.193)\n",
      "Epoch-90  89.5s\tTrain: loss 0.8766\taccu 99.1951\tValid: loss 1.2259\taccu 85.9228\n",
      "Epoch 90: val_acc did not improve\n",
      "90 1e-05\n",
      "Epoch-91   20 batches\tloss 0.8647 (0.8647)\taccu 98.438 (99.453)\n",
      "Epoch-91   40 batches\tloss 0.8445 (0.8718)\taccu 100.000 (99.297)\n",
      "Epoch-91   60 batches\tloss 0.8737 (0.8702)\taccu 98.438 (99.375)\n",
      "Epoch-91   80 batches\tloss 0.8660 (0.8713)\taccu 100.000 (99.336)\n",
      "Epoch-91  100 batches\tloss 0.8758 (0.8739)\taccu 98.438 (99.281)\n",
      "Epoch-91  120 batches\tloss 0.8489 (0.8737)\taccu 100.000 (99.297)\n",
      "Epoch-91  140 batches\tloss 0.8873 (0.8745)\taccu 98.438 (99.330)\n",
      "Epoch-91  160 batches\tloss 0.8552 (0.8741)\taccu 100.000 (99.326)\n",
      "Epoch-91  180 batches\tloss 0.8875 (0.8729)\taccu 98.438 (99.358)\n",
      "Epoch-91  200 batches\tloss 0.8630 (0.8725)\taccu 100.000 (99.359)\n",
      "Epoch-91  220 batches\tloss 0.8639 (0.8722)\taccu 100.000 (99.361)\n",
      "Epoch-91  240 batches\tloss 0.8412 (0.8719)\taccu 100.000 (99.362)\n",
      "Epoch-91  260 batches\tloss 0.8431 (0.8718)\taccu 100.000 (99.351)\n",
      "Epoch-91  280 batches\tloss 0.8968 (0.8719)\taccu 96.875 (99.364)\n",
      "Epoch-91  300 batches\tloss 0.8920 (0.8715)\taccu 96.875 (99.370)\n",
      "Epoch-91  320 batches\tloss 0.8528 (0.8714)\taccu 100.000 (99.370)\n",
      "Epoch-91  340 batches\tloss 0.8670 (0.8713)\taccu 98.438 (99.357)\n",
      "Epoch-91  360 batches\tloss 0.8585 (0.8710)\taccu 98.438 (99.349)\n",
      "Epoch-91  380 batches\tloss 0.8457 (0.8709)\taccu 98.438 (99.354)\n",
      "Epoch-91  400 batches\tloss 0.8717 (0.8710)\taccu 98.438 (99.352)\n",
      "Epoch-91  420 batches\tloss 0.8619 (0.8706)\taccu 100.000 (99.371)\n",
      "Epoch-91  440 batches\tloss 0.8443 (0.8701)\taccu 100.000 (99.386)\n",
      "Epoch-91  460 batches\tloss 0.8653 (0.8697)\taccu 100.000 (99.402)\n",
      "Epoch-91  480 batches\tloss 0.8516 (0.8694)\taccu 100.000 (99.401)\n",
      "Epoch-91  89.5s\tTrain: loss 0.8693\taccu 99.4003\tValid: loss 1.2227\taccu 86.1586\n",
      "Epoch 91: val_acc did not improve\n",
      "91 1e-05\n",
      "Epoch-92   20 batches\tloss 0.8782 (0.8658)\taccu 98.438 (99.531)\n",
      "Epoch-92   40 batches\tloss 0.8750 (0.8677)\taccu 100.000 (99.453)\n",
      "Epoch-92   60 batches\tloss 0.8739 (0.8681)\taccu 98.438 (99.557)\n",
      "Epoch-92   80 batches\tloss 0.8486 (0.8703)\taccu 100.000 (99.336)\n",
      "Epoch-92  100 batches\tloss 0.8495 (0.8692)\taccu 100.000 (99.297)\n",
      "Epoch-92  120 batches\tloss 0.9063 (0.8688)\taccu 100.000 (99.336)\n",
      "Epoch-92  140 batches\tloss 0.8904 (0.8689)\taccu 98.438 (99.342)\n",
      "Epoch-92  160 batches\tloss 0.8396 (0.8684)\taccu 100.000 (99.365)\n",
      "Epoch-92  180 batches\tloss 0.8797 (0.8689)\taccu 100.000 (99.392)\n",
      "Epoch-92  200 batches\tloss 0.8690 (0.8690)\taccu 100.000 (99.406)\n",
      "Epoch-92  220 batches\tloss 0.8346 (0.8688)\taccu 100.000 (99.403)\n",
      "Epoch-92  240 batches\tloss 0.8882 (0.8683)\taccu 98.438 (99.421)\n",
      "Epoch-92  260 batches\tloss 0.8568 (0.8686)\taccu 98.438 (99.399)\n",
      "Epoch-92  280 batches\tloss 0.8617 (0.8681)\taccu 98.438 (99.414)\n",
      "Epoch-92  300 batches\tloss 0.8698 (0.8683)\taccu 98.438 (99.406)\n",
      "Epoch-92  320 batches\tloss 0.8558 (0.8678)\taccu 100.000 (99.443)\n",
      "Epoch-92  340 batches\tloss 0.8498 (0.8677)\taccu 100.000 (99.453)\n",
      "Epoch-92  360 batches\tloss 0.8788 (0.8680)\taccu 100.000 (99.453)\n",
      "Epoch-92  380 batches\tloss 0.8498 (0.8680)\taccu 100.000 (99.465)\n",
      "Epoch-92  400 batches\tloss 0.8872 (0.8680)\taccu 100.000 (99.461)\n",
      "Epoch-92  420 batches\tloss 0.8336 (0.8680)\taccu 100.000 (99.475)\n",
      "Epoch-92  440 batches\tloss 0.8719 (0.8681)\taccu 100.000 (99.471)\n",
      "Epoch-92  460 batches\tloss 0.8376 (0.8679)\taccu 100.000 (99.474)\n",
      "Epoch-92  480 batches\tloss 0.8970 (0.8678)\taccu 98.438 (99.486)\n",
      "Epoch-92  89.7s\tTrain: loss 0.8680\taccu 99.4855\tValid: loss 1.2125\taccu 86.5713\n",
      "Epoch 92: val_acc improved from 86.3208 to 86.5713, saving model to ./SGN/pretrained/SGN\\1_best.pth\n",
      "92 1e-05\n",
      "Epoch-93   20 batches\tloss 0.8844 (0.8705)\taccu 100.000 (99.375)\n",
      "Epoch-93   40 batches\tloss 0.8590 (0.8699)\taccu 100.000 (99.336)\n",
      "Epoch-93   60 batches\tloss 0.8528 (0.8680)\taccu 100.000 (99.427)\n",
      "Epoch-93   80 batches\tloss 0.9021 (0.8697)\taccu 100.000 (99.453)\n",
      "Epoch-93  100 batches\tloss 0.8377 (0.8686)\taccu 100.000 (99.438)\n",
      "Epoch-93  120 batches\tloss 0.8581 (0.8688)\taccu 100.000 (99.401)\n",
      "Epoch-93  140 batches\tloss 0.8273 (0.8687)\taccu 100.000 (99.386)\n",
      "Epoch-93  160 batches\tloss 0.8736 (0.8682)\taccu 100.000 (99.404)\n",
      "Epoch-93  180 batches\tloss 0.8720 (0.8684)\taccu 100.000 (99.384)\n",
      "Epoch-93  200 batches\tloss 0.8471 (0.8685)\taccu 100.000 (99.383)\n",
      "Epoch-93  220 batches\tloss 0.8811 (0.8695)\taccu 98.438 (99.354)\n",
      "Epoch-93  240 batches\tloss 0.8883 (0.8698)\taccu 100.000 (99.382)\n",
      "Epoch-93  260 batches\tloss 0.8644 (0.8702)\taccu 100.000 (99.369)\n",
      "Epoch-93  280 batches\tloss 0.8843 (0.8697)\taccu 98.438 (99.381)\n",
      "Epoch-93  300 batches\tloss 0.8549 (0.8695)\taccu 98.438 (99.385)\n",
      "Epoch-93  320 batches\tloss 0.8823 (0.8688)\taccu 98.438 (99.399)\n",
      "Epoch-93  340 batches\tloss 0.8785 (0.8692)\taccu 100.000 (99.393)\n",
      "Epoch-93  360 batches\tloss 0.8642 (0.8693)\taccu 98.438 (99.397)\n",
      "Epoch-93  380 batches\tloss 0.8577 (0.8689)\taccu 100.000 (99.404)\n",
      "Epoch-93  400 batches\tloss 0.8608 (0.8687)\taccu 98.438 (99.395)\n",
      "Epoch-93  420 batches\tloss 0.9040 (0.8690)\taccu 96.875 (99.375)\n",
      "Epoch-93  440 batches\tloss 0.9133 (0.8690)\taccu 96.875 (99.364)\n",
      "Epoch-93  460 batches\tloss 0.8483 (0.8689)\taccu 100.000 (99.375)\n",
      "Epoch-93  480 batches\tloss 0.8630 (0.8688)\taccu 100.000 (99.382)\n",
      "Epoch-93  89.9s\tTrain: loss 0.8688\taccu 99.3750\tValid: loss 1.2220\taccu 86.1881\n",
      "Epoch 93: val_acc did not improve\n",
      "93 1e-05\n",
      "Epoch-94   20 batches\tloss 0.8468 (0.8692)\taccu 100.000 (99.609)\n",
      "Epoch-94   40 batches\tloss 0.8612 (0.8644)\taccu 100.000 (99.609)\n",
      "Epoch-94   60 batches\tloss 0.8663 (0.8663)\taccu 98.438 (99.583)\n",
      "Epoch-94   80 batches\tloss 0.8782 (0.8679)\taccu 100.000 (99.453)\n",
      "Epoch-94  100 batches\tloss 0.8685 (0.8663)\taccu 100.000 (99.500)\n",
      "Epoch-94  120 batches\tloss 0.9026 (0.8648)\taccu 98.438 (99.544)\n",
      "Epoch-94  140 batches\tloss 0.8748 (0.8639)\taccu 100.000 (99.576)\n",
      "Epoch-94  160 batches\tloss 0.8679 (0.8648)\taccu 98.438 (99.541)\n",
      "Epoch-94  180 batches\tloss 0.8563 (0.8648)\taccu 100.000 (99.557)\n",
      "Epoch-94  200 batches\tloss 0.8852 (0.8649)\taccu 96.875 (99.531)\n",
      "Epoch-94  220 batches\tloss 0.8616 (0.8649)\taccu 100.000 (99.510)\n",
      "Epoch-94  240 batches\tloss 0.8442 (0.8653)\taccu 100.000 (99.473)\n",
      "Epoch-94  260 batches\tloss 0.8768 (0.8657)\taccu 100.000 (99.447)\n",
      "Epoch-94  280 batches\tloss 0.8647 (0.8661)\taccu 100.000 (99.431)\n",
      "Epoch-94  300 batches\tloss 0.8592 (0.8664)\taccu 98.438 (99.422)\n",
      "Epoch-94  320 batches\tloss 0.8654 (0.8668)\taccu 100.000 (99.434)\n",
      "Epoch-94  340 batches\tloss 0.8578 (0.8671)\taccu 100.000 (99.439)\n",
      "Epoch-94  360 batches\tloss 0.8559 (0.8668)\taccu 100.000 (99.431)\n",
      "Epoch-94  380 batches\tloss 0.9107 (0.8667)\taccu 100.000 (99.449)\n",
      "Epoch-94  400 batches\tloss 0.8665 (0.8668)\taccu 100.000 (99.434)\n",
      "Epoch-94  420 batches\tloss 0.8725 (0.8670)\taccu 100.000 (99.438)\n",
      "Epoch-94  440 batches\tloss 0.8848 (0.8671)\taccu 100.000 (99.446)\n",
      "Epoch-94  460 batches\tloss 0.9074 (0.8669)\taccu 96.875 (99.443)\n",
      "Epoch-94  480 batches\tloss 0.8852 (0.8668)\taccu 100.000 (99.450)\n",
      "Epoch-94  89.5s\tTrain: loss 0.8668\taccu 99.4445\tValid: loss 1.2202\taccu 86.2618\n",
      "Epoch 94: val_acc did not improve\n",
      "94 1e-05\n",
      "Epoch-95   20 batches\tloss 0.8399 (0.8612)\taccu 100.000 (99.688)\n",
      "Epoch-95   40 batches\tloss 0.8557 (0.8601)\taccu 100.000 (99.766)\n",
      "Epoch-95   60 batches\tloss 0.8587 (0.8620)\taccu 100.000 (99.818)\n",
      "Epoch-95   80 batches\tloss 0.8320 (0.8622)\taccu 100.000 (99.707)\n",
      "Epoch-95  100 batches\tloss 0.8789 (0.8613)\taccu 98.438 (99.703)\n",
      "Epoch-95  120 batches\tloss 0.8710 (0.8621)\taccu 100.000 (99.674)\n",
      "Epoch-95  140 batches\tloss 0.8636 (0.8623)\taccu 100.000 (99.688)\n",
      "Epoch-95  160 batches\tloss 0.8485 (0.8626)\taccu 100.000 (99.658)\n",
      "Epoch-95  180 batches\tloss 0.8633 (0.8626)\taccu 100.000 (99.661)\n",
      "Epoch-95  200 batches\tloss 0.8508 (0.8638)\taccu 100.000 (99.633)\n",
      "Epoch-95  220 batches\tloss 0.8724 (0.8639)\taccu 100.000 (99.631)\n",
      "Epoch-95  240 batches\tloss 0.8447 (0.8638)\taccu 100.000 (99.642)\n",
      "Epoch-95  260 batches\tloss 0.8585 (0.8641)\taccu 98.438 (99.615)\n",
      "Epoch-95  280 batches\tloss 0.8651 (0.8638)\taccu 100.000 (99.609)\n",
      "Epoch-95  300 batches\tloss 0.8743 (0.8640)\taccu 98.438 (99.589)\n",
      "Epoch-95  320 batches\tloss 0.8773 (0.8639)\taccu 100.000 (99.604)\n",
      "Epoch-95  340 batches\tloss 0.8820 (0.8641)\taccu 98.438 (99.582)\n",
      "Epoch-95  360 batches\tloss 0.8277 (0.8639)\taccu 100.000 (99.588)\n",
      "Epoch-95  380 batches\tloss 0.8511 (0.8637)\taccu 100.000 (99.597)\n",
      "Epoch-95  400 batches\tloss 0.8518 (0.8636)\taccu 100.000 (99.598)\n",
      "Epoch-95  420 batches\tloss 0.8849 (0.8639)\taccu 98.438 (99.591)\n",
      "Epoch-95  440 batches\tloss 0.8358 (0.8641)\taccu 98.438 (99.570)\n",
      "Epoch-95  460 batches\tloss 0.8902 (0.8643)\taccu 98.438 (99.562)\n",
      "Epoch-95  480 batches\tloss 0.8487 (0.8642)\taccu 100.000 (99.564)\n",
      "Epoch-95  89.1s\tTrain: loss 0.8643\taccu 99.5644\tValid: loss 1.2191\taccu 86.1439\n",
      "Epoch 95: val_acc did not improve\n",
      "95 1e-05\n",
      "Epoch-96   20 batches\tloss 0.8798 (0.8605)\taccu 100.000 (99.453)\n",
      "Epoch-96   40 batches\tloss 0.8839 (0.8644)\taccu 100.000 (99.531)\n",
      "Epoch-96   60 batches\tloss 0.8604 (0.8648)\taccu 98.438 (99.505)\n",
      "Epoch-96   80 batches\tloss 0.9095 (0.8687)\taccu 96.875 (99.375)\n",
      "Epoch-96  100 batches\tloss 0.8499 (0.8671)\taccu 100.000 (99.391)\n",
      "Epoch-96  120 batches\tloss 0.8622 (0.8677)\taccu 98.438 (99.388)\n",
      "Epoch-96  140 batches\tloss 0.8581 (0.8682)\taccu 100.000 (99.364)\n",
      "Epoch-96  160 batches\tloss 0.8737 (0.8678)\taccu 100.000 (99.395)\n",
      "Epoch-96  180 batches\tloss 0.8804 (0.8681)\taccu 98.438 (99.349)\n",
      "Epoch-96  200 batches\tloss 0.8552 (0.8678)\taccu 100.000 (99.336)\n",
      "Epoch-96  220 batches\tloss 0.8668 (0.8675)\taccu 100.000 (99.368)\n",
      "Epoch-96  240 batches\tloss 0.8604 (0.8674)\taccu 98.438 (99.382)\n",
      "Epoch-96  260 batches\tloss 0.8626 (0.8671)\taccu 100.000 (99.405)\n",
      "Epoch-96  280 batches\tloss 0.8311 (0.8669)\taccu 100.000 (99.408)\n",
      "Epoch-96  300 batches\tloss 0.9177 (0.8675)\taccu 96.875 (99.401)\n",
      "Epoch-96  320 batches\tloss 0.8789 (0.8669)\taccu 100.000 (99.395)\n",
      "Epoch-96  340 batches\tloss 0.8591 (0.8666)\taccu 100.000 (99.393)\n",
      "Epoch-96  360 batches\tloss 0.8701 (0.8666)\taccu 100.000 (99.405)\n",
      "Epoch-96  380 batches\tloss 0.8625 (0.8666)\taccu 100.000 (99.396)\n",
      "Epoch-96  400 batches\tloss 0.8432 (0.8667)\taccu 100.000 (99.402)\n",
      "Epoch-96  420 batches\tloss 0.8326 (0.8674)\taccu 100.000 (99.379)\n",
      "Epoch-96  440 batches\tloss 0.8517 (0.8671)\taccu 100.000 (99.396)\n",
      "Epoch-96  460 batches\tloss 0.8770 (0.8674)\taccu 98.438 (99.402)\n",
      "Epoch-96  480 batches\tloss 0.8495 (0.8673)\taccu 98.438 (99.414)\n",
      "Epoch-96  89.0s\tTrain: loss 0.8672\taccu 99.4160\tValid: loss 1.2152\taccu 86.4682\n",
      "Epoch 96: val_acc did not improve\n",
      "96 1e-05\n",
      "Epoch-97   20 batches\tloss 0.8743 (0.8643)\taccu 100.000 (99.375)\n",
      "Epoch-97   40 batches\tloss 0.8417 (0.8623)\taccu 100.000 (99.453)\n",
      "Epoch-97   60 batches\tloss 0.8517 (0.8631)\taccu 100.000 (99.505)\n",
      "Epoch-97   80 batches\tloss 0.8433 (0.8645)\taccu 100.000 (99.453)\n",
      "Epoch-97  100 batches\tloss 0.8370 (0.8638)\taccu 100.000 (99.406)\n",
      "Epoch-97  120 batches\tloss 0.8519 (0.8651)\taccu 98.438 (99.349)\n",
      "Epoch-97  140 batches\tloss 0.8577 (0.8646)\taccu 100.000 (99.386)\n",
      "Epoch-97  160 batches\tloss 0.8954 (0.8644)\taccu 98.438 (99.414)\n",
      "Epoch-97  180 batches\tloss 0.8663 (0.8655)\taccu 100.000 (99.366)\n",
      "Epoch-97  200 batches\tloss 0.8678 (0.8655)\taccu 100.000 (99.398)\n",
      "Epoch-97  220 batches\tloss 0.8427 (0.8653)\taccu 100.000 (99.425)\n",
      "Epoch-97  240 batches\tloss 0.8279 (0.8651)\taccu 100.000 (99.427)\n",
      "Epoch-97  260 batches\tloss 0.8675 (0.8653)\taccu 100.000 (99.429)\n",
      "Epoch-97  280 batches\tloss 0.8613 (0.8653)\taccu 100.000 (99.420)\n",
      "Epoch-97  300 batches\tloss 0.8461 (0.8650)\taccu 100.000 (99.443)\n",
      "Epoch-97  320 batches\tloss 0.8797 (0.8652)\taccu 96.875 (99.443)\n",
      "Epoch-97  340 batches\tloss 0.8528 (0.8654)\taccu 100.000 (99.458)\n",
      "Epoch-97  360 batches\tloss 0.8524 (0.8653)\taccu 100.000 (99.462)\n",
      "Epoch-97  380 batches\tloss 0.9012 (0.8653)\taccu 98.438 (99.465)\n",
      "Epoch-97  400 batches\tloss 0.8404 (0.8652)\taccu 100.000 (99.465)\n",
      "Epoch-97  420 batches\tloss 0.8363 (0.8651)\taccu 100.000 (99.472)\n",
      "Epoch-97  440 batches\tloss 0.8410 (0.8652)\taccu 100.000 (99.474)\n",
      "Epoch-97  460 batches\tloss 0.8485 (0.8651)\taccu 98.438 (99.474)\n",
      "Epoch-97  480 batches\tloss 0.8911 (0.8656)\taccu 100.000 (99.463)\n",
      "Epoch-97  89.4s\tTrain: loss 0.8659\taccu 99.4476\tValid: loss 1.2281\taccu 85.7901\n",
      "Epoch 97: val_acc did not improve\n",
      "97 1e-05\n",
      "Epoch-98   20 batches\tloss 0.8394 (0.8616)\taccu 100.000 (99.297)\n",
      "Epoch-98   40 batches\tloss 0.8417 (0.8611)\taccu 100.000 (99.531)\n",
      "Epoch-98   60 batches\tloss 0.8851 (0.8639)\taccu 98.438 (99.505)\n",
      "Epoch-98   80 batches\tloss 0.8421 (0.8640)\taccu 100.000 (99.551)\n",
      "Epoch-98  100 batches\tloss 0.8268 (0.8633)\taccu 100.000 (99.531)\n",
      "Epoch-98  120 batches\tloss 0.8509 (0.8634)\taccu 100.000 (99.531)\n",
      "Epoch-98  140 batches\tloss 0.8597 (0.8634)\taccu 98.438 (99.542)\n",
      "Epoch-98  160 batches\tloss 0.8954 (0.8631)\taccu 100.000 (99.551)\n",
      "Epoch-98  180 batches\tloss 0.8795 (0.8632)\taccu 98.438 (99.557)\n",
      "Epoch-98  200 batches\tloss 0.8873 (0.8633)\taccu 98.438 (99.531)\n",
      "Epoch-98  220 batches\tloss 0.8855 (0.8642)\taccu 100.000 (99.524)\n",
      "Epoch-98  240 batches\tloss 0.8583 (0.8651)\taccu 98.438 (99.486)\n",
      "Epoch-98  260 batches\tloss 0.8676 (0.8647)\taccu 100.000 (99.507)\n",
      "Epoch-98  280 batches\tloss 0.9342 (0.8651)\taccu 100.000 (99.492)\n",
      "Epoch-98  300 batches\tloss 0.9212 (0.8652)\taccu 95.312 (99.490)\n",
      "Epoch-98  320 batches\tloss 0.8908 (0.8650)\taccu 98.438 (99.492)\n",
      "Epoch-98  340 batches\tloss 0.9097 (0.8654)\taccu 98.438 (99.485)\n",
      "Epoch-98  360 batches\tloss 0.9153 (0.8655)\taccu 96.875 (99.484)\n",
      "Epoch-98  380 batches\tloss 0.8476 (0.8651)\taccu 100.000 (99.498)\n",
      "Epoch-98  400 batches\tloss 0.8610 (0.8652)\taccu 98.438 (99.496)\n",
      "Epoch-98  420 batches\tloss 0.8550 (0.8654)\taccu 98.438 (99.490)\n",
      "Epoch-98  440 batches\tloss 0.8507 (0.8651)\taccu 100.000 (99.496)\n",
      "Epoch-98  460 batches\tloss 0.8903 (0.8657)\taccu 98.438 (99.487)\n",
      "Epoch-98  480 batches\tloss 0.8799 (0.8658)\taccu 98.438 (99.469)\n",
      "Epoch-98  89.4s\tTrain: loss 0.8661\taccu 99.4697\tValid: loss 1.2193\taccu 85.9817\n",
      "Epoch 98: val_acc did not improve\n",
      "98 1e-05\n",
      "Epoch-99   20 batches\tloss 0.9084 (0.8631)\taccu 98.438 (99.531)\n",
      "Epoch-99   40 batches\tloss 0.8613 (0.8679)\taccu 100.000 (99.375)\n",
      "Epoch-99   60 batches\tloss 0.8634 (0.8700)\taccu 100.000 (99.349)\n",
      "Epoch-99   80 batches\tloss 0.8438 (0.8690)\taccu 100.000 (99.414)\n",
      "Epoch-99  100 batches\tloss 0.8816 (0.8673)\taccu 96.875 (99.438)\n",
      "Epoch-99  120 batches\tloss 0.8550 (0.8674)\taccu 100.000 (99.492)\n",
      "Epoch-99  140 batches\tloss 0.9453 (0.8678)\taccu 96.875 (99.464)\n",
      "Epoch-99  160 batches\tloss 0.8645 (0.8672)\taccu 100.000 (99.502)\n",
      "Epoch-99  180 batches\tloss 0.8780 (0.8660)\taccu 100.000 (99.523)\n",
      "Epoch-99  200 batches\tloss 0.8370 (0.8652)\taccu 100.000 (99.516)\n",
      "Epoch-99  220 batches\tloss 0.8608 (0.8649)\taccu 100.000 (99.538)\n",
      "Epoch-99  240 batches\tloss 0.8509 (0.8646)\taccu 100.000 (99.538)\n",
      "Epoch-99  260 batches\tloss 0.8656 (0.8652)\taccu 100.000 (99.501)\n",
      "Epoch-99  280 batches\tloss 0.8610 (0.8652)\taccu 100.000 (99.487)\n",
      "Epoch-99  300 batches\tloss 0.8438 (0.8650)\taccu 98.438 (99.479)\n",
      "Epoch-99  320 batches\tloss 0.8739 (0.8652)\taccu 96.875 (99.453)\n",
      "Epoch-99  340 batches\tloss 0.8504 (0.8651)\taccu 100.000 (99.449)\n",
      "Epoch-99  360 batches\tloss 0.8683 (0.8655)\taccu 100.000 (99.440)\n",
      "Epoch-99  380 batches\tloss 0.8804 (0.8654)\taccu 100.000 (99.457)\n",
      "Epoch-99  400 batches\tloss 0.8690 (0.8656)\taccu 100.000 (99.434)\n",
      "Epoch-99  420 batches\tloss 0.8715 (0.8658)\taccu 100.000 (99.427)\n",
      "Epoch-99  440 batches\tloss 0.8558 (0.8655)\taccu 100.000 (99.435)\n",
      "Epoch-99  460 batches\tloss 0.9100 (0.8655)\taccu 100.000 (99.453)\n",
      "Epoch-99  480 batches\tloss 0.8447 (0.8654)\taccu 100.000 (99.453)\n",
      "Epoch-99  89.3s\tTrain: loss 0.8655\taccu 99.4571\tValid: loss 1.2218\taccu 86.2323\n",
      "Epoch 99: val_acc did not improve\n",
      "99 1e-05\n",
      "Epoch-100  20 batches\tloss 0.8593 (0.8639)\taccu 98.438 (99.297)\n",
      "Epoch-100  40 batches\tloss 0.8759 (0.8654)\taccu 98.438 (99.141)\n",
      "Epoch-100  60 batches\tloss 0.8742 (0.8664)\taccu 100.000 (99.323)\n",
      "Epoch-100  80 batches\tloss 0.8944 (0.8665)\taccu 100.000 (99.375)\n",
      "Epoch-100 100 batches\tloss 0.8683 (0.8647)\taccu 98.438 (99.406)\n",
      "Epoch-100 120 batches\tloss 0.8893 (0.8652)\taccu 98.438 (99.388)\n",
      "Epoch-100 140 batches\tloss 0.8742 (0.8649)\taccu 98.438 (99.420)\n",
      "Epoch-100 160 batches\tloss 0.8969 (0.8652)\taccu 100.000 (99.414)\n",
      "Epoch-100 180 batches\tloss 0.9007 (0.8661)\taccu 96.875 (99.349)\n",
      "Epoch-100 200 batches\tloss 0.9236 (0.8668)\taccu 95.312 (99.312)\n",
      "Epoch-100 220 batches\tloss 0.8904 (0.8661)\taccu 98.438 (99.325)\n",
      "Epoch-100 240 batches\tloss 0.8747 (0.8660)\taccu 100.000 (99.323)\n",
      "Epoch-100 260 batches\tloss 0.8808 (0.8663)\taccu 100.000 (99.351)\n",
      "Epoch-100 280 batches\tloss 0.8691 (0.8667)\taccu 98.438 (99.358)\n",
      "Epoch-100 300 batches\tloss 0.8510 (0.8663)\taccu 100.000 (99.380)\n",
      "Epoch-100 320 batches\tloss 0.8604 (0.8663)\taccu 100.000 (99.395)\n",
      "Epoch-100 340 batches\tloss 0.8446 (0.8661)\taccu 100.000 (99.403)\n",
      "Epoch-100 360 batches\tloss 0.8870 (0.8662)\taccu 98.438 (99.388)\n",
      "Epoch-100 380 batches\tloss 0.8509 (0.8660)\taccu 100.000 (99.396)\n",
      "Epoch-100 400 batches\tloss 0.8563 (0.8660)\taccu 100.000 (99.387)\n",
      "Epoch-100 420 batches\tloss 0.8578 (0.8659)\taccu 100.000 (99.386)\n",
      "Epoch-100 440 batches\tloss 0.8734 (0.8661)\taccu 98.438 (99.375)\n",
      "Epoch-100 460 batches\tloss 0.8855 (0.8661)\taccu 98.438 (99.378)\n",
      "Epoch-100 480 batches\tloss 0.8824 (0.8664)\taccu 100.000 (99.365)\n",
      "Epoch-100 89.2s\tTrain: loss 0.8664\taccu 99.3655\tValid: loss 1.2202\taccu 86.1586\n",
      "Epoch 100: val_acc did not improve\n",
      "100 1e-05\n",
      "Epoch-101  20 batches\tloss 0.8421 (0.8589)\taccu 100.000 (99.375)\n",
      "Epoch-101  40 batches\tloss 0.8637 (0.8609)\taccu 100.000 (99.570)\n",
      "Epoch-101  60 batches\tloss 0.8484 (0.8614)\taccu 100.000 (99.557)\n",
      "Epoch-101  80 batches\tloss 0.8617 (0.8634)\taccu 100.000 (99.453)\n",
      "Epoch-101 100 batches\tloss 0.8712 (0.8637)\taccu 98.438 (99.438)\n",
      "Epoch-101 120 batches\tloss 0.8659 (0.8635)\taccu 100.000 (99.505)\n",
      "Epoch-101 140 batches\tloss 0.8633 (0.8636)\taccu 100.000 (99.509)\n",
      "Epoch-101 160 batches\tloss 0.8642 (0.8639)\taccu 100.000 (99.502)\n",
      "Epoch-101 180 batches\tloss 0.8704 (0.8642)\taccu 100.000 (99.505)\n",
      "Epoch-101 200 batches\tloss 0.8541 (0.8642)\taccu 100.000 (99.531)\n",
      "Epoch-101 220 batches\tloss 0.8713 (0.8640)\taccu 98.438 (99.517)\n",
      "Epoch-101 240 batches\tloss 0.8542 (0.8640)\taccu 98.438 (99.505)\n",
      "Epoch-101 260 batches\tloss 0.8632 (0.8638)\taccu 100.000 (99.525)\n",
      "Epoch-101 280 batches\tloss 0.8650 (0.8646)\taccu 100.000 (99.503)\n",
      "Epoch-101 300 batches\tloss 0.8730 (0.8655)\taccu 100.000 (99.479)\n",
      "Epoch-101 320 batches\tloss 0.8652 (0.8651)\taccu 100.000 (99.497)\n",
      "Epoch-101 340 batches\tloss 0.9095 (0.8652)\taccu 98.438 (99.490)\n",
      "Epoch-101 360 batches\tloss 0.8389 (0.8655)\taccu 100.000 (99.488)\n",
      "Epoch-101 380 batches\tloss 0.8798 (0.8653)\taccu 98.438 (99.498)\n",
      "Epoch-101 400 batches\tloss 0.8508 (0.8653)\taccu 100.000 (99.492)\n",
      "Epoch-101 420 batches\tloss 0.8359 (0.8654)\taccu 100.000 (99.494)\n",
      "Epoch-101 440 batches\tloss 0.9090 (0.8655)\taccu 98.438 (99.485)\n",
      "Epoch-101 460 batches\tloss 0.8483 (0.8658)\taccu 100.000 (99.490)\n",
      "Epoch-101 480 batches\tloss 0.8488 (0.8663)\taccu 100.000 (99.479)\n",
      "Epoch-101 89.3s\tTrain: loss 0.8664\taccu 99.4823\tValid: loss 1.2191\taccu 86.0407\n",
      "Epoch 101: val_acc did not improve\n",
      "101 1e-05\n",
      "Epoch-102  20 batches\tloss 0.8528 (0.8598)\taccu 100.000 (99.609)\n",
      "Epoch-102  40 batches\tloss 0.8578 (0.8610)\taccu 100.000 (99.570)\n",
      "Epoch-102  60 batches\tloss 0.8605 (0.8609)\taccu 100.000 (99.427)\n",
      "Epoch-102  80 batches\tloss 0.8618 (0.8599)\taccu 100.000 (99.453)\n",
      "Epoch-102 100 batches\tloss 0.8748 (0.8618)\taccu 100.000 (99.375)\n",
      "Epoch-102 120 batches\tloss 0.8663 (0.8632)\taccu 100.000 (99.362)\n",
      "Epoch-102 140 batches\tloss 0.8881 (0.8636)\taccu 100.000 (99.386)\n",
      "Epoch-102 160 batches\tloss 0.8925 (0.8639)\taccu 100.000 (99.375)\n",
      "Epoch-102 180 batches\tloss 0.8407 (0.8640)\taccu 100.000 (99.401)\n",
      "Epoch-102 200 batches\tloss 0.8726 (0.8642)\taccu 100.000 (99.383)\n",
      "Epoch-102 220 batches\tloss 0.8785 (0.8642)\taccu 96.875 (99.389)\n",
      "Epoch-102 240 batches\tloss 0.8727 (0.8644)\taccu 98.438 (99.388)\n",
      "Epoch-102 260 batches\tloss 0.8625 (0.8644)\taccu 100.000 (99.417)\n",
      "Epoch-102 280 batches\tloss 0.8541 (0.8643)\taccu 100.000 (99.420)\n",
      "Epoch-102 300 batches\tloss 0.8738 (0.8646)\taccu 100.000 (99.427)\n",
      "Epoch-102 320 batches\tloss 0.8552 (0.8644)\taccu 98.438 (99.438)\n",
      "Epoch-102 340 batches\tloss 0.8507 (0.8644)\taccu 100.000 (99.444)\n",
      "Epoch-102 360 batches\tloss 0.8821 (0.8645)\taccu 100.000 (99.444)\n",
      "Epoch-102 380 batches\tloss 0.8597 (0.8646)\taccu 100.000 (99.433)\n",
      "Epoch-102 400 batches\tloss 0.8999 (0.8646)\taccu 98.438 (99.449)\n",
      "Epoch-102 420 batches\tloss 0.8401 (0.8644)\taccu 100.000 (99.461)\n",
      "Epoch-102 440 batches\tloss 0.8741 (0.8647)\taccu 100.000 (99.457)\n",
      "Epoch-102 460 batches\tloss 0.8959 (0.8646)\taccu 98.438 (99.460)\n",
      "Epoch-102 480 batches\tloss 0.8651 (0.8645)\taccu 100.000 (99.466)\n",
      "Epoch-102 89.7s\tTrain: loss 0.8646\taccu 99.4539\tValid: loss 1.2249\taccu 85.9670\n",
      "Epoch 102: val_acc did not improve\n",
      "102 1e-05\n",
      "Epoch-103  20 batches\tloss 0.8373 (0.8655)\taccu 100.000 (99.531)\n",
      "Epoch-103  40 batches\tloss 0.8545 (0.8608)\taccu 100.000 (99.688)\n",
      "Epoch-103  60 batches\tloss 0.8600 (0.8614)\taccu 98.438 (99.688)\n",
      "Epoch-103  80 batches\tloss 0.8483 (0.8614)\taccu 100.000 (99.648)\n",
      "Epoch-103 100 batches\tloss 0.8476 (0.8624)\taccu 100.000 (99.672)\n",
      "Epoch-103 120 batches\tloss 0.8347 (0.8631)\taccu 98.438 (99.635)\n",
      "Epoch-103 140 batches\tloss 0.8495 (0.8635)\taccu 100.000 (99.632)\n",
      "Epoch-103 160 batches\tloss 0.8551 (0.8647)\taccu 100.000 (99.580)\n",
      "Epoch-103 180 batches\tloss 0.8778 (0.8645)\taccu 100.000 (99.592)\n",
      "Epoch-103 200 batches\tloss 0.9128 (0.8646)\taccu 98.438 (99.586)\n",
      "Epoch-103 220 batches\tloss 0.8676 (0.8646)\taccu 100.000 (99.609)\n",
      "Epoch-103 240 batches\tloss 0.8528 (0.8647)\taccu 100.000 (99.609)\n",
      "Epoch-103 260 batches\tloss 0.8517 (0.8648)\taccu 100.000 (99.609)\n",
      "Epoch-103 280 batches\tloss 0.8780 (0.8650)\taccu 100.000 (99.587)\n",
      "Epoch-103 300 batches\tloss 0.8825 (0.8651)\taccu 100.000 (99.589)\n",
      "Epoch-103 320 batches\tloss 0.8626 (0.8651)\taccu 100.000 (99.590)\n",
      "Epoch-103 340 batches\tloss 0.8761 (0.8652)\taccu 100.000 (99.582)\n",
      "Epoch-103 360 batches\tloss 0.8996 (0.8653)\taccu 100.000 (99.570)\n",
      "Epoch-103 380 batches\tloss 0.8863 (0.8655)\taccu 98.438 (99.548)\n",
      "Epoch-103 400 batches\tloss 0.8592 (0.8651)\taccu 100.000 (99.535)\n",
      "Epoch-103 420 batches\tloss 0.8822 (0.8651)\taccu 96.875 (99.528)\n",
      "Epoch-103 440 batches\tloss 0.8772 (0.8653)\taccu 98.438 (99.517)\n",
      "Epoch-103 460 batches\tloss 0.8584 (0.8654)\taccu 100.000 (99.507)\n",
      "Epoch-103 480 batches\tloss 0.9201 (0.8655)\taccu 98.438 (99.495)\n",
      "Epoch-103 89.3s\tTrain: loss 0.8657\taccu 99.4981\tValid: loss 1.2187\taccu 86.2913\n",
      "Epoch 103: val_acc did not improve\n",
      "103 1e-05\n",
      "Epoch-104  20 batches\tloss 0.9362 (0.8595)\taccu 98.438 (99.844)\n",
      "Epoch-104  40 batches\tloss 0.8598 (0.8678)\taccu 100.000 (99.453)\n",
      "Epoch-104  60 batches\tloss 0.8556 (0.8694)\taccu 100.000 (99.427)\n",
      "Epoch-104  80 batches\tloss 0.8452 (0.8671)\taccu 100.000 (99.492)\n",
      "Epoch-104 100 batches\tloss 0.8914 (0.8680)\taccu 98.438 (99.500)\n",
      "Epoch-104 120 batches\tloss 0.8648 (0.8674)\taccu 98.438 (99.466)\n",
      "Epoch-104 140 batches\tloss 0.8534 (0.8663)\taccu 98.438 (99.475)\n",
      "Epoch-104 160 batches\tloss 0.9717 (0.8670)\taccu 96.875 (99.492)\n",
      "Epoch-104 180 batches\tloss 0.8551 (0.8674)\taccu 100.000 (99.436)\n",
      "Epoch-104 200 batches\tloss 0.8921 (0.8677)\taccu 98.438 (99.422)\n",
      "Epoch-104 220 batches\tloss 0.8862 (0.8665)\taccu 98.438 (99.446)\n",
      "Epoch-104 240 batches\tloss 0.8346 (0.8661)\taccu 100.000 (99.440)\n",
      "Epoch-104 260 batches\tloss 0.8784 (0.8664)\taccu 98.438 (99.453)\n",
      "Epoch-104 280 batches\tloss 0.8739 (0.8661)\taccu 100.000 (99.459)\n",
      "Epoch-104 300 batches\tloss 0.8607 (0.8658)\taccu 98.438 (99.464)\n",
      "Epoch-104 320 batches\tloss 0.8218 (0.8653)\taccu 100.000 (99.473)\n",
      "Epoch-104 340 batches\tloss 0.8568 (0.8651)\taccu 100.000 (99.472)\n",
      "Epoch-104 360 batches\tloss 0.8696 (0.8651)\taccu 100.000 (99.466)\n",
      "Epoch-104 380 batches\tloss 0.9239 (0.8657)\taccu 98.438 (99.461)\n",
      "Epoch-104 400 batches\tloss 0.8647 (0.8659)\taccu 98.438 (99.438)\n",
      "Epoch-104 420 batches\tloss 0.8828 (0.8658)\taccu 100.000 (99.446)\n",
      "Epoch-104 440 batches\tloss 0.8672 (0.8656)\taccu 100.000 (99.450)\n",
      "Epoch-104 460 batches\tloss 0.8632 (0.8657)\taccu 98.438 (99.446)\n",
      "Epoch-104 480 batches\tloss 0.9100 (0.8655)\taccu 98.438 (99.456)\n",
      "Epoch-104 89.6s\tTrain: loss 0.8654\taccu 99.4634\tValid: loss 1.2257\taccu 86.0407\n",
      "Epoch 104: val_acc did not improve\n",
      "104 1e-05\n",
      "Epoch-105  20 batches\tloss 0.8587 (0.8507)\taccu 98.438 (99.531)\n",
      "Epoch-105  40 batches\tloss 0.8632 (0.8535)\taccu 100.000 (99.609)\n",
      "Epoch-105  60 batches\tloss 0.8550 (0.8562)\taccu 100.000 (99.583)\n",
      "Epoch-105  80 batches\tloss 0.8638 (0.8578)\taccu 98.438 (99.570)\n",
      "Epoch-105 100 batches\tloss 0.8786 (0.8598)\taccu 100.000 (99.516)\n",
      "Epoch-105 120 batches\tloss 0.8496 (0.8607)\taccu 100.000 (99.518)\n",
      "Epoch-105 140 batches\tloss 0.8639 (0.8621)\taccu 100.000 (99.520)\n",
      "Epoch-105 160 batches\tloss 0.9012 (0.8624)\taccu 98.438 (99.531)\n",
      "Epoch-105 180 batches\tloss 0.8224 (0.8629)\taccu 100.000 (99.505)\n",
      "Epoch-105 200 batches\tloss 0.8688 (0.8633)\taccu 98.438 (99.500)\n",
      "Epoch-105 220 batches\tloss 0.8663 (0.8638)\taccu 100.000 (99.482)\n",
      "Epoch-105 240 batches\tloss 0.8649 (0.8645)\taccu 100.000 (99.486)\n",
      "Epoch-105 260 batches\tloss 0.8857 (0.8645)\taccu 96.875 (99.465)\n",
      "Epoch-105 280 batches\tloss 0.8498 (0.8650)\taccu 100.000 (99.453)\n",
      "Epoch-105 300 batches\tloss 0.8729 (0.8650)\taccu 98.438 (99.474)\n",
      "Epoch-105 320 batches\tloss 0.8780 (0.8651)\taccu 100.000 (99.478)\n",
      "Epoch-105 340 batches\tloss 0.9116 (0.8653)\taccu 98.438 (99.472)\n",
      "Epoch-105 360 batches\tloss 0.8481 (0.8655)\taccu 100.000 (99.470)\n",
      "Epoch-105 380 batches\tloss 0.8672 (0.8650)\taccu 96.875 (99.482)\n",
      "Epoch-105 400 batches\tloss 0.8907 (0.8649)\taccu 96.875 (99.480)\n",
      "Epoch-105 420 batches\tloss 0.8523 (0.8647)\taccu 100.000 (99.494)\n",
      "Epoch-105 440 batches\tloss 0.8709 (0.8645)\taccu 100.000 (99.489)\n",
      "Epoch-105 460 batches\tloss 0.8639 (0.8642)\taccu 98.438 (99.490)\n",
      "Epoch-105 480 batches\tloss 0.8499 (0.8642)\taccu 100.000 (99.499)\n",
      "Epoch-105 89.4s\tTrain: loss 0.8645\taccu 99.5044\tValid: loss 1.2194\taccu 86.0849\n",
      "Epoch 105: val_acc did not improve\n",
      "105 1e-05\n",
      "Epoch-106  20 batches\tloss 0.8300 (0.8624)\taccu 100.000 (99.609)\n",
      "Epoch-106  40 batches\tloss 0.8709 (0.8636)\taccu 100.000 (99.453)\n",
      "Epoch-106  60 batches\tloss 0.8770 (0.8632)\taccu 100.000 (99.557)\n",
      "Epoch-106  80 batches\tloss 0.8715 (0.8636)\taccu 98.438 (99.512)\n",
      "Epoch-106 100 batches\tloss 0.8748 (0.8646)\taccu 98.438 (99.438)\n",
      "Epoch-106 120 batches\tloss 0.8463 (0.8653)\taccu 100.000 (99.492)\n",
      "Epoch-106 140 batches\tloss 0.8822 (0.8662)\taccu 98.438 (99.475)\n",
      "Epoch-106 160 batches\tloss 0.8651 (0.8665)\taccu 100.000 (99.453)\n",
      "Epoch-106 180 batches\tloss 0.9155 (0.8660)\taccu 95.312 (99.436)\n",
      "Epoch-106 200 batches\tloss 0.8573 (0.8653)\taccu 98.438 (99.453)\n",
      "Epoch-106 220 batches\tloss 0.8774 (0.8659)\taccu 100.000 (99.432)\n",
      "Epoch-106 240 batches\tloss 0.8183 (0.8658)\taccu 100.000 (99.421)\n",
      "Epoch-106 260 batches\tloss 0.8963 (0.8654)\taccu 98.438 (99.429)\n",
      "Epoch-106 280 batches\tloss 0.8450 (0.8649)\taccu 100.000 (99.442)\n",
      "Epoch-106 300 batches\tloss 0.8745 (0.8646)\taccu 100.000 (99.443)\n",
      "Epoch-106 320 batches\tloss 0.8320 (0.8641)\taccu 100.000 (99.448)\n",
      "Epoch-106 340 batches\tloss 0.8726 (0.8639)\taccu 98.438 (99.467)\n",
      "Epoch-106 360 batches\tloss 0.8405 (0.8636)\taccu 100.000 (99.475)\n",
      "Epoch-106 380 batches\tloss 0.8545 (0.8637)\taccu 100.000 (99.457)\n",
      "Epoch-106 400 batches\tloss 0.8690 (0.8638)\taccu 100.000 (99.453)\n",
      "Epoch-106 420 batches\tloss 0.8443 (0.8638)\taccu 100.000 (99.442)\n",
      "Epoch-106 440 batches\tloss 0.8698 (0.8639)\taccu 100.000 (99.450)\n",
      "Epoch-106 460 batches\tloss 0.8385 (0.8641)\taccu 100.000 (99.453)\n",
      "Epoch-106 480 batches\tloss 0.8923 (0.8640)\taccu 96.875 (99.450)\n",
      "Epoch-106 89.2s\tTrain: loss 0.8644\taccu 99.4445\tValid: loss 1.2185\taccu 85.8933\n",
      "Epoch 106: val_acc did not improve\n",
      "106 1e-05\n",
      "Epoch-107  20 batches\tloss 0.8452 (0.8636)\taccu 100.000 (99.609)\n",
      "Epoch-107  40 batches\tloss 0.8569 (0.8650)\taccu 100.000 (99.570)\n",
      "Epoch-107  60 batches\tloss 0.8539 (0.8659)\taccu 100.000 (99.479)\n",
      "Epoch-107  80 batches\tloss 0.8872 (0.8666)\taccu 100.000 (99.512)\n",
      "Epoch-107 100 batches\tloss 0.8625 (0.8656)\taccu 100.000 (99.516)\n",
      "Epoch-107 120 batches\tloss 0.8681 (0.8653)\taccu 100.000 (99.544)\n",
      "Epoch-107 140 batches\tloss 0.8758 (0.8652)\taccu 100.000 (99.576)\n",
      "Epoch-107 160 batches\tloss 0.8549 (0.8652)\taccu 100.000 (99.590)\n",
      "Epoch-107 180 batches\tloss 0.8506 (0.8655)\taccu 100.000 (99.540)\n",
      "Epoch-107 200 batches\tloss 0.9016 (0.8662)\taccu 98.438 (99.492)\n",
      "Epoch-107 220 batches\tloss 0.8906 (0.8661)\taccu 100.000 (99.482)\n",
      "Epoch-107 240 batches\tloss 0.8472 (0.8655)\taccu 100.000 (99.492)\n",
      "Epoch-107 260 batches\tloss 0.8708 (0.8651)\taccu 98.438 (99.483)\n",
      "Epoch-107 280 batches\tloss 0.8445 (0.8651)\taccu 100.000 (99.481)\n",
      "Epoch-107 300 batches\tloss 0.9234 (0.8650)\taccu 95.312 (99.490)\n",
      "Epoch-107 320 batches\tloss 0.8687 (0.8650)\taccu 98.438 (99.502)\n",
      "Epoch-107 340 batches\tloss 0.8836 (0.8648)\taccu 96.875 (99.508)\n",
      "Epoch-107 360 batches\tloss 0.8549 (0.8650)\taccu 100.000 (99.501)\n",
      "Epoch-107 380 batches\tloss 0.8538 (0.8651)\taccu 100.000 (99.498)\n",
      "Epoch-107 400 batches\tloss 0.9006 (0.8652)\taccu 98.438 (99.504)\n",
      "Epoch-107 420 batches\tloss 0.8824 (0.8656)\taccu 98.438 (99.494)\n",
      "Epoch-107 440 batches\tloss 0.8657 (0.8658)\taccu 100.000 (99.489)\n",
      "Epoch-107 460 batches\tloss 0.8483 (0.8657)\taccu 100.000 (99.480)\n",
      "Epoch-107 480 batches\tloss 0.8554 (0.8653)\taccu 100.000 (99.495)\n",
      "Epoch-107 89.7s\tTrain: loss 0.8651\taccu 99.5013\tValid: loss 1.2180\taccu 86.2176\n",
      "Epoch 107: val_acc did not improve\n",
      "107 1e-05\n",
      "Epoch-108  20 batches\tloss 0.8392 (0.8643)\taccu 100.000 (99.453)\n",
      "Epoch-108  40 batches\tloss 0.8703 (0.8668)\taccu 98.438 (99.492)\n",
      "Epoch-108  60 batches\tloss 0.8556 (0.8642)\taccu 100.000 (99.557)\n",
      "Epoch-108  80 batches\tloss 0.8288 (0.8644)\taccu 100.000 (99.551)\n",
      "Epoch-108 100 batches\tloss 0.8494 (0.8634)\taccu 100.000 (99.531)\n",
      "Epoch-108 120 batches\tloss 0.9093 (0.8663)\taccu 95.312 (99.375)\n",
      "Epoch-108 140 batches\tloss 0.8830 (0.8655)\taccu 100.000 (99.386)\n",
      "Epoch-108 160 batches\tloss 0.8762 (0.8669)\taccu 100.000 (99.355)\n",
      "Epoch-108 180 batches\tloss 0.8499 (0.8661)\taccu 100.000 (99.375)\n",
      "Epoch-108 200 batches\tloss 0.8480 (0.8663)\taccu 100.000 (99.367)\n",
      "Epoch-108 220 batches\tloss 0.8333 (0.8666)\taccu 100.000 (99.354)\n",
      "Epoch-108 240 batches\tloss 0.8544 (0.8660)\taccu 98.438 (99.382)\n",
      "Epoch-108 260 batches\tloss 0.8773 (0.8657)\taccu 98.438 (99.417)\n",
      "Epoch-108 280 batches\tloss 0.8493 (0.8652)\taccu 100.000 (99.436)\n",
      "Epoch-108 300 batches\tloss 0.8394 (0.8649)\taccu 100.000 (99.458)\n",
      "Epoch-108 320 batches\tloss 0.9382 (0.8649)\taccu 96.875 (99.443)\n",
      "Epoch-108 340 batches\tloss 0.8424 (0.8646)\taccu 100.000 (99.439)\n",
      "Epoch-108 360 batches\tloss 0.8628 (0.8647)\taccu 100.000 (99.440)\n",
      "Epoch-108 380 batches\tloss 0.8540 (0.8645)\taccu 100.000 (99.470)\n",
      "Epoch-108 400 batches\tloss 0.8416 (0.8646)\taccu 100.000 (99.457)\n",
      "Epoch-108 420 batches\tloss 0.8746 (0.8650)\taccu 100.000 (99.446)\n",
      "Epoch-108 440 batches\tloss 0.8558 (0.8649)\taccu 98.438 (99.442)\n",
      "Epoch-108 460 batches\tloss 0.8424 (0.8648)\taccu 100.000 (99.433)\n",
      "Epoch-108 480 batches\tloss 0.8556 (0.8648)\taccu 98.438 (99.443)\n",
      "Epoch-108 89.6s\tTrain: loss 0.8650\taccu 99.4445\tValid: loss 1.2202\taccu 86.3502\n",
      "Epoch 108: val_acc did not improve\n",
      "108 1e-05\n",
      "Epoch-109  20 batches\tloss 0.8980 (0.8696)\taccu 98.438 (99.609)\n",
      "Epoch-109  40 batches\tloss 0.8449 (0.8691)\taccu 100.000 (99.492)\n",
      "Epoch-109  60 batches\tloss 0.8690 (0.8674)\taccu 98.438 (99.479)\n",
      "Epoch-109  80 batches\tloss 0.8488 (0.8678)\taccu 100.000 (99.512)\n",
      "Epoch-109 100 batches\tloss 0.8874 (0.8675)\taccu 98.438 (99.500)\n",
      "Epoch-109 120 batches\tloss 0.8814 (0.8665)\taccu 98.438 (99.531)\n",
      "Epoch-109 140 batches\tloss 0.8613 (0.8672)\taccu 100.000 (99.498)\n",
      "Epoch-109 160 batches\tloss 0.8502 (0.8662)\taccu 100.000 (99.521)\n",
      "Epoch-109 180 batches\tloss 0.8898 (0.8666)\taccu 100.000 (99.531)\n",
      "Epoch-109 200 batches\tloss 0.8403 (0.8664)\taccu 100.000 (99.531)\n",
      "Epoch-109 220 batches\tloss 0.8767 (0.8662)\taccu 100.000 (99.531)\n",
      "Epoch-109 240 batches\tloss 0.8560 (0.8650)\taccu 100.000 (99.557)\n",
      "Epoch-109 260 batches\tloss 0.8565 (0.8652)\taccu 100.000 (99.579)\n",
      "Epoch-109 280 batches\tloss 0.8868 (0.8648)\taccu 100.000 (99.576)\n",
      "Epoch-109 300 batches\tloss 0.8474 (0.8650)\taccu 100.000 (99.583)\n",
      "Epoch-109 320 batches\tloss 0.9065 (0.8651)\taccu 100.000 (99.570)\n",
      "Epoch-109 340 batches\tloss 0.8494 (0.8650)\taccu 100.000 (99.573)\n",
      "Epoch-109 360 batches\tloss 0.8963 (0.8650)\taccu 96.875 (99.562)\n",
      "Epoch-109 380 batches\tloss 0.8684 (0.8643)\taccu 100.000 (99.564)\n",
      "Epoch-109 400 batches\tloss 0.8710 (0.8644)\taccu 98.438 (99.547)\n",
      "Epoch-109 420 batches\tloss 0.8278 (0.8643)\taccu 100.000 (99.550)\n",
      "Epoch-109 440 batches\tloss 0.8635 (0.8643)\taccu 100.000 (99.538)\n",
      "Epoch-109 460 batches\tloss 0.8249 (0.8638)\taccu 100.000 (99.545)\n",
      "Epoch-109 480 batches\tloss 0.8846 (0.8639)\taccu 100.000 (99.538)\n",
      "Epoch-109 89.4s\tTrain: loss 0.8639\taccu 99.5328\tValid: loss 1.2144\taccu 86.3355\n",
      "Epoch 109: val_acc did not improve\n",
      "109 1e-05\n",
      "Epoch-110  20 batches\tloss 0.8367 (0.8530)\taccu 100.000 (99.922)\n",
      "Epoch-110  40 batches\tloss 0.8476 (0.8585)\taccu 100.000 (99.727)\n",
      "Epoch-110  60 batches\tloss 0.8505 (0.8609)\taccu 100.000 (99.661)\n",
      "Epoch-110  80 batches\tloss 0.8591 (0.8609)\taccu 100.000 (99.648)\n",
      "Epoch-110 100 batches\tloss 0.8723 (0.8612)\taccu 100.000 (99.641)\n",
      "Epoch-110 120 batches\tloss 0.8387 (0.8622)\taccu 100.000 (99.596)\n",
      "Epoch-110 140 batches\tloss 0.8510 (0.8625)\taccu 100.000 (99.621)\n",
      "Epoch-110 160 batches\tloss 0.8466 (0.8625)\taccu 100.000 (99.609)\n",
      "Epoch-110 180 batches\tloss 0.8765 (0.8630)\taccu 100.000 (99.601)\n",
      "Epoch-110 200 batches\tloss 0.8352 (0.8632)\taccu 100.000 (99.617)\n",
      "Epoch-110 220 batches\tloss 0.8332 (0.8639)\taccu 100.000 (99.595)\n",
      "Epoch-110 240 batches\tloss 0.8613 (0.8637)\taccu 100.000 (99.609)\n",
      "Epoch-110 260 batches\tloss 0.8738 (0.8647)\taccu 100.000 (99.573)\n",
      "Epoch-110 280 batches\tloss 0.8546 (0.8644)\taccu 100.000 (99.559)\n",
      "Epoch-110 300 batches\tloss 0.8684 (0.8642)\taccu 100.000 (99.562)\n",
      "Epoch-110 320 batches\tloss 0.9037 (0.8644)\taccu 98.438 (99.551)\n",
      "Epoch-110 340 batches\tloss 0.8587 (0.8645)\taccu 100.000 (99.536)\n",
      "Epoch-110 360 batches\tloss 0.8372 (0.8644)\taccu 100.000 (99.527)\n",
      "Epoch-110 380 batches\tloss 0.8467 (0.8647)\taccu 100.000 (99.515)\n",
      "Epoch-110 400 batches\tloss 0.8881 (0.8644)\taccu 98.438 (99.523)\n",
      "Epoch-110 420 batches\tloss 0.8548 (0.8642)\taccu 100.000 (99.520)\n",
      "Epoch-110 440 batches\tloss 0.8735 (0.8644)\taccu 100.000 (99.517)\n",
      "Epoch-110 460 batches\tloss 0.8347 (0.8643)\taccu 100.000 (99.514)\n",
      "Epoch-110 480 batches\tloss 0.8889 (0.8641)\taccu 98.438 (99.515)\n",
      "Epoch-110 89.5s\tTrain: loss 0.8640\taccu 99.5171\tValid: loss 1.2200\taccu 86.0554\n",
      "Epoch 110: val_acc did not improve\n",
      "110 1.0000000000000002e-06\n",
      "Epoch-111  20 batches\tloss 0.8515 (0.8653)\taccu 100.000 (99.609)\n",
      "Epoch-111  40 batches\tloss 0.8673 (0.8686)\taccu 100.000 (99.414)\n",
      "Epoch-111  60 batches\tloss 0.8815 (0.8679)\taccu 100.000 (99.453)\n",
      "Epoch-111  80 batches\tloss 0.8786 (0.8662)\taccu 98.438 (99.492)\n",
      "Epoch-111 100 batches\tloss 0.8918 (0.8657)\taccu 98.438 (99.500)\n",
      "Epoch-111 120 batches\tloss 0.8661 (0.8647)\taccu 98.438 (99.492)\n",
      "Epoch-111 140 batches\tloss 0.8507 (0.8635)\taccu 100.000 (99.554)\n",
      "Epoch-111 160 batches\tloss 0.8588 (0.8630)\taccu 100.000 (99.541)\n",
      "Epoch-111 180 batches\tloss 0.8494 (0.8630)\taccu 100.000 (99.549)\n",
      "Epoch-111 200 batches\tloss 0.8698 (0.8632)\taccu 96.875 (99.516)\n",
      "Epoch-111 220 batches\tloss 0.8537 (0.8629)\taccu 100.000 (99.503)\n",
      "Epoch-111 240 batches\tloss 0.8342 (0.8628)\taccu 100.000 (99.512)\n",
      "Epoch-111 260 batches\tloss 0.8380 (0.8628)\taccu 100.000 (99.501)\n",
      "Epoch-111 280 batches\tloss 0.8652 (0.8629)\taccu 100.000 (99.487)\n",
      "Epoch-111 300 batches\tloss 0.9063 (0.8628)\taccu 96.875 (99.500)\n",
      "Epoch-111 320 batches\tloss 0.8390 (0.8624)\taccu 100.000 (99.507)\n",
      "Epoch-111 340 batches\tloss 0.8328 (0.8623)\taccu 100.000 (99.508)\n",
      "Epoch-111 360 batches\tloss 0.8663 (0.8629)\taccu 100.000 (99.484)\n",
      "Epoch-111 380 batches\tloss 1.0052 (0.8631)\taccu 96.875 (99.486)\n",
      "Epoch-111 400 batches\tloss 0.8999 (0.8631)\taccu 96.875 (99.488)\n",
      "Epoch-111 420 batches\tloss 0.8435 (0.8628)\taccu 100.000 (99.498)\n",
      "Epoch-111 440 batches\tloss 0.8444 (0.8628)\taccu 100.000 (99.503)\n",
      "Epoch-111 460 batches\tloss 0.8773 (0.8630)\taccu 100.000 (99.497)\n",
      "Epoch-111 480 batches\tloss 0.8505 (0.8631)\taccu 100.000 (99.489)\n",
      "Epoch-111 89.7s\tTrain: loss 0.8633\taccu 99.4760\tValid: loss 1.2203\taccu 86.0702\n",
      "Epoch 111: val_acc did not improve\n",
      "111 1.0000000000000002e-06\n",
      "Epoch-112  20 batches\tloss 0.8693 (0.8574)\taccu 100.000 (99.766)\n",
      "Epoch-112  40 batches\tloss 0.8558 (0.8616)\taccu 100.000 (99.609)\n",
      "Epoch-112  60 batches\tloss 0.8551 (0.8630)\taccu 100.000 (99.635)\n",
      "Epoch-112  80 batches\tloss 0.8515 (0.8618)\taccu 100.000 (99.629)\n",
      "Epoch-112 100 batches\tloss 0.8322 (0.8629)\taccu 100.000 (99.609)\n",
      "Epoch-112 120 batches\tloss 0.8751 (0.8625)\taccu 100.000 (99.609)\n",
      "Epoch-112 140 batches\tloss 0.8537 (0.8623)\taccu 100.000 (99.621)\n",
      "Epoch-112 160 batches\tloss 0.8509 (0.8620)\taccu 100.000 (99.609)\n",
      "Epoch-112 180 batches\tloss 0.8443 (0.8623)\taccu 98.438 (99.583)\n",
      "Epoch-112 200 batches\tloss 0.8600 (0.8618)\taccu 100.000 (99.594)\n",
      "Epoch-112 220 batches\tloss 0.8488 (0.8617)\taccu 100.000 (99.602)\n",
      "Epoch-112 240 batches\tloss 0.8308 (0.8621)\taccu 100.000 (99.590)\n",
      "Epoch-112 260 batches\tloss 0.8752 (0.8617)\taccu 98.438 (99.591)\n",
      "Epoch-112 280 batches\tloss 0.8438 (0.8621)\taccu 98.438 (99.565)\n",
      "Epoch-112 300 batches\tloss 0.8493 (0.8619)\taccu 100.000 (99.552)\n",
      "Epoch-112 320 batches\tloss 0.8624 (0.8625)\taccu 100.000 (99.526)\n",
      "Epoch-112 340 batches\tloss 0.8633 (0.8629)\taccu 100.000 (99.517)\n",
      "Epoch-112 360 batches\tloss 0.8458 (0.8633)\taccu 100.000 (99.510)\n",
      "Epoch-112 380 batches\tloss 0.8687 (0.8633)\taccu 98.438 (99.507)\n",
      "Epoch-112 400 batches\tloss 0.8514 (0.8632)\taccu 100.000 (99.492)\n",
      "Epoch-112 420 batches\tloss 0.9277 (0.8633)\taccu 96.875 (99.487)\n",
      "Epoch-112 440 batches\tloss 0.8654 (0.8632)\taccu 100.000 (99.485)\n",
      "Epoch-112 460 batches\tloss 0.8912 (0.8632)\taccu 98.438 (99.494)\n",
      "Epoch-112 480 batches\tloss 0.8527 (0.8629)\taccu 100.000 (99.505)\n",
      "Epoch-112 89.5s\tTrain: loss 0.8631\taccu 99.4981\tValid: loss 1.2231\taccu 86.0554\n",
      "Epoch 112: val_acc did not improve\n",
      "112 1.0000000000000002e-06\n",
      "Epoch-113  20 batches\tloss 0.8403 (0.8631)\taccu 100.000 (99.375)\n",
      "Epoch-113  40 batches\tloss 0.8683 (0.8641)\taccu 100.000 (99.492)\n",
      "Epoch-113  60 batches\tloss 0.8608 (0.8633)\taccu 100.000 (99.531)\n",
      "Epoch-113  80 batches\tloss 0.8540 (0.8640)\taccu 98.438 (99.551)\n",
      "Epoch-113 100 batches\tloss 0.8607 (0.8639)\taccu 98.438 (99.547)\n",
      "Epoch-113 120 batches\tloss 0.8615 (0.8636)\taccu 100.000 (99.570)\n",
      "Epoch-113 140 batches\tloss 0.8717 (0.8630)\taccu 98.438 (99.565)\n",
      "Epoch-113 160 batches\tloss 0.8934 (0.8636)\taccu 98.438 (99.541)\n",
      "Epoch-113 180 batches\tloss 0.8486 (0.8637)\taccu 98.438 (99.531)\n",
      "Epoch-113 200 batches\tloss 0.8731 (0.8634)\taccu 100.000 (99.516)\n",
      "Epoch-113 220 batches\tloss 0.8521 (0.8631)\taccu 100.000 (99.531)\n",
      "Epoch-113 240 batches\tloss 0.8502 (0.8630)\taccu 100.000 (99.531)\n",
      "Epoch-113 260 batches\tloss 0.8434 (0.8629)\taccu 100.000 (99.549)\n",
      "Epoch-113 280 batches\tloss 0.8426 (0.8630)\taccu 100.000 (99.531)\n",
      "Epoch-113 300 batches\tloss 0.8727 (0.8632)\taccu 100.000 (99.516)\n",
      "Epoch-113 320 batches\tloss 0.8267 (0.8632)\taccu 100.000 (99.521)\n",
      "Epoch-113 340 batches\tloss 0.8735 (0.8634)\taccu 100.000 (99.527)\n",
      "Epoch-113 360 batches\tloss 0.8623 (0.8635)\taccu 100.000 (99.514)\n",
      "Epoch-113 380 batches\tloss 0.8677 (0.8635)\taccu 100.000 (99.519)\n",
      "Epoch-113 400 batches\tloss 0.8650 (0.8635)\taccu 100.000 (99.523)\n",
      "Epoch-113 420 batches\tloss 0.8359 (0.8633)\taccu 100.000 (99.505)\n",
      "Epoch-113 440 batches\tloss 0.8386 (0.8630)\taccu 100.000 (99.524)\n",
      "Epoch-113 460 batches\tloss 0.8750 (0.8629)\taccu 100.000 (99.528)\n",
      "Epoch-113 480 batches\tloss 0.9283 (0.8634)\taccu 96.875 (99.508)\n",
      "Epoch-113 89.7s\tTrain: loss 0.8633\taccu 99.5139\tValid: loss 1.2237\taccu 86.2913\n",
      "Epoch 113: val_acc did not improve\n",
      "113 1.0000000000000002e-06\n",
      "Epoch-114  20 batches\tloss 0.8924 (0.8637)\taccu 96.875 (99.219)\n",
      "Epoch-114  40 batches\tloss 0.8627 (0.8649)\taccu 100.000 (99.297)\n",
      "Epoch-114  60 batches\tloss 0.9029 (0.8646)\taccu 98.438 (99.349)\n",
      "Epoch-114  80 batches\tloss 0.8887 (0.8642)\taccu 98.438 (99.414)\n",
      "Epoch-114 100 batches\tloss 0.8627 (0.8637)\taccu 100.000 (99.453)\n",
      "Epoch-114 120 batches\tloss 0.8810 (0.8636)\taccu 98.438 (99.414)\n",
      "Epoch-114 140 batches\tloss 0.8471 (0.8638)\taccu 100.000 (99.420)\n",
      "Epoch-114 160 batches\tloss 0.8368 (0.8625)\taccu 100.000 (99.453)\n",
      "Epoch-114 180 batches\tloss 0.8409 (0.8621)\taccu 100.000 (99.470)\n",
      "Epoch-114 200 batches\tloss 0.8909 (0.8624)\taccu 98.438 (99.438)\n",
      "Epoch-114 220 batches\tloss 0.8573 (0.8622)\taccu 100.000 (99.446)\n",
      "Epoch-114 240 batches\tloss 0.8314 (0.8617)\taccu 100.000 (99.473)\n",
      "Epoch-114 260 batches\tloss 0.8695 (0.8614)\taccu 98.438 (99.453)\n",
      "Epoch-114 280 batches\tloss 0.8492 (0.8622)\taccu 100.000 (99.436)\n",
      "Epoch-114 300 batches\tloss 0.9074 (0.8623)\taccu 98.438 (99.453)\n",
      "Epoch-114 320 batches\tloss 0.8539 (0.8624)\taccu 100.000 (99.468)\n",
      "Epoch-114 340 batches\tloss 0.8639 (0.8625)\taccu 100.000 (99.481)\n",
      "Epoch-114 360 batches\tloss 0.8602 (0.8626)\taccu 100.000 (99.497)\n",
      "Epoch-114 380 batches\tloss 0.8691 (0.8627)\taccu 100.000 (99.502)\n",
      "Epoch-114 400 batches\tloss 0.9033 (0.8630)\taccu 100.000 (99.504)\n",
      "Epoch-114 420 batches\tloss 0.8489 (0.8632)\taccu 100.000 (99.501)\n",
      "Epoch-114 440 batches\tloss 0.8824 (0.8632)\taccu 98.438 (99.492)\n",
      "Epoch-114 460 batches\tloss 0.8738 (0.8636)\taccu 98.438 (99.494)\n",
      "Epoch-114 480 batches\tloss 0.9056 (0.8634)\taccu 98.438 (99.489)\n",
      "Epoch-114 89.3s\tTrain: loss 0.8631\taccu 99.4981\tValid: loss 1.2222\taccu 85.9817\n",
      "Epoch 114: val_acc did not improve\n",
      "114 1.0000000000000002e-06\n",
      "Epoch-115  20 batches\tloss 0.8922 (0.8575)\taccu 100.000 (99.766)\n",
      "Epoch-115  40 batches\tloss 0.8586 (0.8630)\taccu 100.000 (99.570)\n",
      "Epoch-115  60 batches\tloss 0.8791 (0.8637)\taccu 98.438 (99.583)\n",
      "Epoch-115  80 batches\tloss 0.8761 (0.8644)\taccu 98.438 (99.551)\n",
      "Epoch-115 100 batches\tloss 0.8903 (0.8643)\taccu 100.000 (99.484)\n",
      "Epoch-115 120 batches\tloss 0.8812 (0.8653)\taccu 98.438 (99.427)\n",
      "Epoch-115 140 batches\tloss 0.8775 (0.8654)\taccu 100.000 (99.408)\n",
      "Epoch-115 160 batches\tloss 0.8770 (0.8648)\taccu 100.000 (99.434)\n",
      "Epoch-115 180 batches\tloss 0.8768 (0.8649)\taccu 100.000 (99.436)\n",
      "Epoch-115 200 batches\tloss 0.8280 (0.8644)\taccu 100.000 (99.422)\n",
      "Epoch-115 220 batches\tloss 0.8532 (0.8639)\taccu 100.000 (99.439)\n",
      "Epoch-115 240 batches\tloss 0.8502 (0.8632)\taccu 100.000 (99.473)\n",
      "Epoch-115 260 batches\tloss 0.8849 (0.8629)\taccu 100.000 (99.477)\n",
      "Epoch-115 280 batches\tloss 0.8683 (0.8624)\taccu 100.000 (99.492)\n",
      "Epoch-115 300 batches\tloss 0.8659 (0.8624)\taccu 100.000 (99.500)\n",
      "Epoch-115 320 batches\tloss 0.8524 (0.8616)\taccu 100.000 (99.517)\n",
      "Epoch-115 340 batches\tloss 0.8529 (0.8618)\taccu 100.000 (99.522)\n",
      "Epoch-115 360 batches\tloss 0.8411 (0.8624)\taccu 100.000 (99.510)\n",
      "Epoch-115 380 batches\tloss 0.8690 (0.8622)\taccu 100.000 (99.519)\n",
      "Epoch-115 400 batches\tloss 0.8409 (0.8620)\taccu 100.000 (99.520)\n",
      "Epoch-115 420 batches\tloss 0.8513 (0.8622)\taccu 100.000 (99.505)\n",
      "Epoch-115 440 batches\tloss 0.8466 (0.8623)\taccu 100.000 (99.510)\n",
      "Epoch-115 460 batches\tloss 0.8638 (0.8622)\taccu 100.000 (99.511)\n",
      "Epoch-115 480 batches\tloss 0.8389 (0.8622)\taccu 100.000 (99.512)\n",
      "Epoch-115 89.7s\tTrain: loss 0.8621\taccu 99.5202\tValid: loss 1.2119\taccu 86.1439\n",
      "Epoch 115: val_acc did not improve\n",
      "115 1.0000000000000002e-06\n",
      "Epoch-116  20 batches\tloss 0.8486 (0.8536)\taccu 100.000 (99.766)\n",
      "Epoch-116  40 batches\tloss 0.8655 (0.8582)\taccu 100.000 (99.688)\n",
      "Epoch-116  60 batches\tloss 0.8854 (0.8639)\taccu 100.000 (99.479)\n",
      "Epoch-116  80 batches\tloss 0.8275 (0.8641)\taccu 100.000 (99.395)\n",
      "Epoch-116 100 batches\tloss 0.8530 (0.8635)\taccu 100.000 (99.484)\n",
      "Epoch-116 120 batches\tloss 0.8288 (0.8632)\taccu 100.000 (99.492)\n",
      "Epoch-116 140 batches\tloss 0.8881 (0.8627)\taccu 100.000 (99.464)\n",
      "Epoch-116 160 batches\tloss 0.8552 (0.8620)\taccu 100.000 (99.482)\n",
      "Epoch-116 180 batches\tloss 0.8810 (0.8622)\taccu 100.000 (99.514)\n",
      "Epoch-116 200 batches\tloss 0.8473 (0.8629)\taccu 100.000 (99.484)\n",
      "Epoch-116 220 batches\tloss 0.8767 (0.8633)\taccu 98.438 (99.453)\n",
      "Epoch-116 240 batches\tloss 0.8849 (0.8640)\taccu 100.000 (99.440)\n",
      "Epoch-116 260 batches\tloss 0.8507 (0.8642)\taccu 100.000 (99.405)\n",
      "Epoch-116 280 batches\tloss 0.8507 (0.8639)\taccu 100.000 (99.420)\n",
      "Epoch-116 300 batches\tloss 0.8892 (0.8637)\taccu 100.000 (99.427)\n",
      "Epoch-116 320 batches\tloss 0.8829 (0.8632)\taccu 96.875 (99.429)\n",
      "Epoch-116 340 batches\tloss 0.8759 (0.8630)\taccu 98.438 (99.439)\n",
      "Epoch-116 360 batches\tloss 0.8682 (0.8629)\taccu 98.438 (99.444)\n",
      "Epoch-116 380 batches\tloss 0.8795 (0.8634)\taccu 98.438 (99.433)\n",
      "Epoch-116 400 batches\tloss 0.8806 (0.8633)\taccu 100.000 (99.434)\n",
      "Epoch-116 420 batches\tloss 0.8633 (0.8632)\taccu 98.438 (99.438)\n",
      "Epoch-116 440 batches\tloss 0.8560 (0.8630)\taccu 100.000 (99.453)\n",
      "Epoch-116 460 batches\tloss 0.8405 (0.8631)\taccu 100.000 (99.453)\n",
      "Epoch-116 480 batches\tloss 0.8753 (0.8628)\taccu 100.000 (99.460)\n",
      "Epoch-116 89.3s\tTrain: loss 0.8631\taccu 99.4634\tValid: loss 1.2328\taccu 85.5690\n",
      "Epoch 116: val_acc did not improve\n",
      "116 1.0000000000000002e-06\n",
      "Epoch-117  20 batches\tloss 0.8526 (0.8628)\taccu 100.000 (99.609)\n",
      "Epoch-117  40 batches\tloss 0.8574 (0.8616)\taccu 100.000 (99.648)\n",
      "Epoch-117  60 batches\tloss 0.8528 (0.8611)\taccu 100.000 (99.661)\n",
      "Epoch-117  80 batches\tloss 0.8728 (0.8640)\taccu 100.000 (99.492)\n",
      "Epoch-117 100 batches\tloss 0.8425 (0.8635)\taccu 100.000 (99.547)\n",
      "Epoch-117 120 batches\tloss 0.8606 (0.8627)\taccu 100.000 (99.609)\n",
      "Epoch-117 140 batches\tloss 0.8526 (0.8627)\taccu 98.438 (99.598)\n",
      "Epoch-117 160 batches\tloss 0.8767 (0.8623)\taccu 100.000 (99.600)\n",
      "Epoch-117 180 batches\tloss 0.9311 (0.8632)\taccu 96.875 (99.557)\n",
      "Epoch-117 200 batches\tloss 0.8454 (0.8633)\taccu 100.000 (99.555)\n",
      "Epoch-117 220 batches\tloss 0.8429 (0.8628)\taccu 100.000 (99.538)\n",
      "Epoch-117 240 batches\tloss 0.8720 (0.8629)\taccu 98.438 (99.538)\n",
      "Epoch-117 260 batches\tloss 0.8463 (0.8626)\taccu 100.000 (99.537)\n",
      "Epoch-117 280 batches\tloss 0.8831 (0.8629)\taccu 98.438 (99.537)\n",
      "Epoch-117 300 batches\tloss 0.8666 (0.8626)\taccu 100.000 (99.562)\n",
      "Epoch-117 320 batches\tloss 0.8658 (0.8624)\taccu 100.000 (99.551)\n",
      "Epoch-117 340 batches\tloss 0.8435 (0.8624)\taccu 98.438 (99.559)\n",
      "Epoch-117 360 batches\tloss 0.8311 (0.8620)\taccu 100.000 (99.570)\n",
      "Epoch-117 380 batches\tloss 0.8402 (0.8621)\taccu 100.000 (99.564)\n",
      "Epoch-117 400 batches\tloss 0.8456 (0.8619)\taccu 100.000 (99.574)\n",
      "Epoch-117 420 batches\tloss 0.8452 (0.8620)\taccu 100.000 (99.583)\n",
      "Epoch-117 440 batches\tloss 0.8836 (0.8618)\taccu 100.000 (99.585)\n",
      "Epoch-117 460 batches\tloss 0.8518 (0.8618)\taccu 100.000 (99.582)\n",
      "Epoch-117 480 batches\tloss 0.8487 (0.8619)\taccu 100.000 (99.580)\n",
      "Epoch-117 89.7s\tTrain: loss 0.8621\taccu 99.5707\tValid: loss 1.2199\taccu 86.3650\n",
      "Epoch 117: val_acc did not improve\n",
      "117 1.0000000000000002e-06\n",
      "Epoch-118  20 batches\tloss 0.8601 (0.8662)\taccu 100.000 (99.375)\n",
      "Epoch-118  40 batches\tloss 0.8654 (0.8652)\taccu 100.000 (99.414)\n",
      "Epoch-118  60 batches\tloss 0.8641 (0.8645)\taccu 98.438 (99.401)\n",
      "Epoch-118  80 batches\tloss 0.8829 (0.8635)\taccu 98.438 (99.434)\n",
      "Epoch-118 100 batches\tloss 0.8617 (0.8648)\taccu 98.438 (99.422)\n",
      "Epoch-118 120 batches\tloss 0.8455 (0.8648)\taccu 100.000 (99.414)\n",
      "Epoch-118 140 batches\tloss 0.8782 (0.8645)\taccu 100.000 (99.386)\n",
      "Epoch-118 160 batches\tloss 0.8515 (0.8632)\taccu 100.000 (99.434)\n",
      "Epoch-118 180 batches\tloss 0.8723 (0.8633)\taccu 98.438 (99.462)\n",
      "Epoch-118 200 batches\tloss 0.8861 (0.8628)\taccu 100.000 (99.477)\n",
      "Epoch-118 220 batches\tloss 0.8662 (0.8636)\taccu 100.000 (99.482)\n",
      "Epoch-118 240 batches\tloss 0.8453 (0.8638)\taccu 100.000 (99.499)\n",
      "Epoch-118 260 batches\tloss 0.8485 (0.8632)\taccu 100.000 (99.507)\n",
      "Epoch-118 280 batches\tloss 0.8387 (0.8631)\taccu 100.000 (99.492)\n",
      "Epoch-118 300 batches\tloss 0.8587 (0.8634)\taccu 100.000 (99.505)\n",
      "Epoch-118 320 batches\tloss 0.8739 (0.8645)\taccu 98.438 (99.487)\n",
      "Epoch-118 340 batches\tloss 0.8505 (0.8643)\taccu 100.000 (99.481)\n",
      "Epoch-118 360 batches\tloss 0.8930 (0.8642)\taccu 100.000 (99.505)\n",
      "Epoch-118 380 batches\tloss 0.8715 (0.8641)\taccu 100.000 (99.507)\n",
      "Epoch-118 400 batches\tloss 0.8575 (0.8637)\taccu 100.000 (99.523)\n",
      "Epoch-118 420 batches\tloss 0.8693 (0.8640)\taccu 100.000 (99.513)\n",
      "Epoch-118 440 batches\tloss 0.8562 (0.8642)\taccu 100.000 (99.499)\n",
      "Epoch-118 460 batches\tloss 0.8621 (0.8641)\taccu 100.000 (99.511)\n",
      "Epoch-118 480 batches\tloss 0.8502 (0.8643)\taccu 100.000 (99.508)\n",
      "Epoch-118 89.9s\tTrain: loss 0.8641\taccu 99.5107\tValid: loss 1.2182\taccu 86.0849\n",
      "Epoch 118: val_acc did not improve\n",
      "118 1.0000000000000002e-06\n",
      "Epoch-119  20 batches\tloss 0.8745 (0.8708)\taccu 100.000 (99.688)\n",
      "Epoch-119  40 batches\tloss 0.8519 (0.8670)\taccu 100.000 (99.766)\n",
      "Epoch-119  60 batches\tloss 0.8657 (0.8653)\taccu 98.438 (99.609)\n",
      "Epoch-119  80 batches\tloss 0.8356 (0.8641)\taccu 100.000 (99.570)\n",
      "Epoch-119 100 batches\tloss 0.8434 (0.8635)\taccu 100.000 (99.531)\n",
      "Epoch-119 120 batches\tloss 0.8697 (0.8627)\taccu 98.438 (99.557)\n",
      "Epoch-119 140 batches\tloss 0.8524 (0.8627)\taccu 100.000 (99.520)\n",
      "Epoch-119 160 batches\tloss 0.8529 (0.8632)\taccu 100.000 (99.492)\n",
      "Epoch-119 180 batches\tloss 0.8576 (0.8634)\taccu 100.000 (99.505)\n",
      "Epoch-119 200 batches\tloss 0.8593 (0.8631)\taccu 100.000 (99.516)\n",
      "Epoch-119 220 batches\tloss 0.8629 (0.8629)\taccu 98.438 (99.538)\n",
      "Epoch-119 240 batches\tloss 0.8675 (0.8623)\taccu 98.438 (99.564)\n",
      "Epoch-119 260 batches\tloss 0.8973 (0.8622)\taccu 100.000 (99.537)\n",
      "Epoch-119 280 batches\tloss 0.8920 (0.8626)\taccu 98.438 (99.531)\n",
      "Epoch-119 300 batches\tloss 0.8484 (0.8621)\taccu 100.000 (99.536)\n",
      "Epoch-119 320 batches\tloss 0.8258 (0.8618)\taccu 100.000 (99.541)\n",
      "Epoch-119 340 batches\tloss 0.8432 (0.8614)\taccu 100.000 (99.550)\n",
      "Epoch-119 360 batches\tloss 0.9174 (0.8616)\taccu 98.438 (99.536)\n",
      "Epoch-119 380 batches\tloss 0.8691 (0.8615)\taccu 100.000 (99.539)\n",
      "Epoch-119 400 batches\tloss 0.8474 (0.8616)\taccu 100.000 (99.551)\n",
      "Epoch-119 420 batches\tloss 0.8805 (0.8616)\taccu 96.875 (99.542)\n",
      "Epoch-119 440 batches\tloss 0.8830 (0.8620)\taccu 98.438 (99.542)\n",
      "Epoch-119 460 batches\tloss 0.8482 (0.8621)\taccu 100.000 (99.535)\n",
      "Epoch-119 480 batches\tloss 0.8678 (0.8621)\taccu 98.438 (99.531)\n",
      "Epoch-119 89.4s\tTrain: loss 0.8622\taccu 99.5265\tValid: loss 1.2168\taccu 86.3208\n",
      "Epoch 119: val_acc did not improve\n",
      "119 1.0000000000000002e-06\n",
      "Epoch-120  20 batches\tloss 0.8293 (0.8629)\taccu 100.000 (99.375)\n",
      "Epoch-120  40 batches\tloss 0.8515 (0.8622)\taccu 100.000 (99.492)\n",
      "Epoch-120  60 batches\tloss 0.8371 (0.8609)\taccu 100.000 (99.479)\n",
      "Epoch-120  80 batches\tloss 0.8518 (0.8589)\taccu 100.000 (99.551)\n",
      "Epoch-120 100 batches\tloss 0.9027 (0.8603)\taccu 100.000 (99.562)\n",
      "Epoch-120 120 batches\tloss 0.8601 (0.8598)\taccu 100.000 (99.531)\n",
      "Epoch-120 140 batches\tloss 0.8616 (0.8598)\taccu 100.000 (99.531)\n",
      "Epoch-120 160 batches\tloss 0.8709 (0.8595)\taccu 96.875 (99.531)\n",
      "Epoch-120 180 batches\tloss 0.8604 (0.8589)\taccu 100.000 (99.540)\n",
      "Epoch-120 200 batches\tloss 0.8824 (0.8595)\taccu 98.438 (99.492)\n",
      "Epoch-120 220 batches\tloss 0.8708 (0.8601)\taccu 98.438 (99.439)\n",
      "Epoch-120 240 batches\tloss 0.8315 (0.8603)\taccu 100.000 (99.440)\n",
      "Epoch-120 260 batches\tloss 0.8426 (0.8610)\taccu 98.438 (99.405)\n",
      "Epoch-120 280 batches\tloss 0.8481 (0.8613)\taccu 100.000 (99.392)\n",
      "Epoch-120 300 batches\tloss 0.8625 (0.8611)\taccu 100.000 (99.406)\n",
      "Epoch-120 320 batches\tloss 0.8766 (0.8612)\taccu 98.438 (99.404)\n",
      "Epoch-120 340 batches\tloss 0.8542 (0.8608)\taccu 100.000 (99.421)\n",
      "Epoch-120 360 batches\tloss 0.8403 (0.8611)\taccu 100.000 (99.427)\n",
      "Epoch-120 380 batches\tloss 0.8488 (0.8611)\taccu 100.000 (99.453)\n",
      "Epoch-120 400 batches\tloss 0.8322 (0.8611)\taccu 100.000 (99.469)\n",
      "Epoch-120 420 batches\tloss 0.8425 (0.8615)\taccu 100.000 (99.468)\n",
      "Epoch-120 440 batches\tloss 0.8671 (0.8613)\taccu 98.438 (99.464)\n",
      "Epoch-120 460 batches\tloss 0.8640 (0.8614)\taccu 98.438 (99.463)\n",
      "Epoch-120 480 batches\tloss 0.8627 (0.8616)\taccu 100.000 (99.473)\n",
      "Epoch-120 89.3s\tTrain: loss 0.8615\taccu 99.4729\tValid: loss 1.2188\taccu 86.1586\n",
      "Epoch 120: val_acc did not improve\n",
      "Best val_acc: 86.5713 from epoch-92\n",
      "Save train and validation log into into ./SGN/pretrained/SGN\\1_log.csv\n",
      "Test: accuracy 87.854, time: 20.24s\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, do_train)\n",
    "\n",
    "    total = get_n_params(model)\n",
    "    # print(model)\n",
    "    print('The number of parameters: ', total)\n",
    "    print('The modes is:', network)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('It is using GPU!')\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = LabelSmoothingLoss(num_classes, smoothing=0.1).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "\n",
    "    if monitor == 'val_acc':\n",
    "        mode = 'max'\n",
    "        monitor_op = np.greater\n",
    "        best = -np.Inf\n",
    "        str_op = 'improve'\n",
    "    elif monitor == 'val_loss':\n",
    "        mode = 'min'\n",
    "        monitor_op = np.less\n",
    "        best = np.Inf\n",
    "        str_op = 'reduce'\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[60, 90, 110], gamma=0.1)\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, case, seg=seg, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    train_loader = ntu_loaders.get_train_loader(batch_size, workers)\n",
    "    val_loader = ntu_loaders.get_val_loader(batch_size, workers)\n",
    "    train_size = ntu_loaders.get_train_size()\n",
    "    val_size = ntu_loaders.get_val_size()\n",
    "\n",
    "    test_loader = ntu_loaders.get_test_loader(32, workers)\n",
    "\n",
    "    print('Train on %d samples, validate on %d samples' %\n",
    "          (train_size, val_size))\n",
    "\n",
    "    best_epoch = 0\n",
    "    output_dir = make_dir(dataset)\n",
    "\n",
    "    save_path = os.path.join(output_dir, network)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    checkpoint = osp.join(save_path, '%s_best.pth' % case)\n",
    "    earlystop_cnt = 0\n",
    "    csv_file = osp.join(save_path, '%s_log.csv' % case)\n",
    "    log_res = list()\n",
    "\n",
    "    lable_path = osp.join(save_path, '%s_lable.txt' % case)\n",
    "    pred_path = osp.join(save_path, '%s_pred.txt' % case)\n",
    "\n",
    "    # Training\n",
    "    if do_train == 1:\n",
    "        for epoch in range(start_epoch, max_epochs):\n",
    "\n",
    "            print(epoch, optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            t_start = time.time()\n",
    "            train_loss, train_acc = train(\n",
    "                train_loader, model, criterion, optimizer, epoch)\n",
    "            val_loss, val_acc = validate(val_loader, model, criterion)\n",
    "            log_res += [[train_loss, train_acc.cpu().numpy(),\n",
    "                         val_loss, val_acc.cpu().numpy()]]\n",
    "\n",
    "            print('Epoch-{:<3d} {:.1f}s\\t'\n",
    "                  'Train: loss {:.4f}\\taccu {:.4f}\\tValid: loss {:.4f}\\taccu {:.4f}'\n",
    "                  .format(epoch + 1, time.time() - t_start, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "            current = val_loss if mode == 'min' else val_acc\n",
    "\n",
    "            # store tensor in cpu\n",
    "            current = current.cpu()\n",
    "\n",
    "            if monitor_op(current, best):\n",
    "                print('Epoch %d: %s %sd from %.4f to %.4f, '\n",
    "                      'saving model to %s'\n",
    "                      % (epoch + 1, monitor, str_op, best, current, checkpoint))\n",
    "                best = current\n",
    "                best_epoch = epoch + 1\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best': best,\n",
    "                    'monitor': monitor,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "                earlystop_cnt = 0\n",
    "            else:\n",
    "                print('Epoch %d: %s did not %s' % (epoch + 1, monitor, str_op))\n",
    "                earlystop_cnt += 1\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        print('Best %s: %.4f from epoch-%d' % (monitor, best, best_epoch))\n",
    "        with open(csv_file, 'w') as fw:\n",
    "            cw = csv.writer(fw)\n",
    "            cw.writerow(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "            cw.writerows(log_res)\n",
    "        print('Save train and validation log into into %s' % csv_file)\n",
    "\n",
    "    # Test\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, 0)\n",
    "    model = model.cuda()\n",
    "    test(test_loader, model, checkpoint, lable_path, pred_path)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
