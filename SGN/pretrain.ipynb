{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/thomas/Downloads/nturgb+d_skeletons'\n",
    "path = 'D:\\\\Datasets\\\\Motion Privacy\\\\NTU RGB+D 120\\\\Skeleton Data'\n",
    "ntu_120 = False\n",
    "is_privacy = True\n",
    "if is_privacy: num_classes = 106 if ntu_120 else 40\n",
    "else: num_classes = 120 if ntu_120 else 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../NTU/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ntu_120:\n",
    "    to_del = [] \n",
    "    for key in X:\n",
    "        if int(key[17:20]) > 60:\n",
    "            to_del.append(key)\n",
    "    for key in to_del:\n",
    "        del X[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_optimized():\n",
    "    # Preallocate memory for x and y, assuming we know the length of X\n",
    "    x = np.zeros((len(X), 300, 150), dtype=np.float32)  # 300 timesteps, 150 features (75 * 2 skeletons)\n",
    "    y = np.zeros((len(X), num_classes), dtype=np.float32)\n",
    "\n",
    "    for i, (file, data) in enumerate(X.items()):\n",
    "        # Get the y index\n",
    "        if is_privacy: y_idx = int(file[9:12]) - 1\n",
    "        else: y_idx = int(file[17:20]) - 1\n",
    "        \n",
    "        # Set the correct class index\n",
    "        y[i, y_idx] = 1\n",
    "        \n",
    "        # Process the data\n",
    "        data = data[:, :, :3]  # Assuming data is a 3D array\n",
    "        num_timesteps = min(data.shape[0], 300)\n",
    "        \n",
    "        # Assign the data to the preallocated array (flattening last dimensions)\n",
    "        x[i, :num_timesteps, :75] = data[:num_timesteps].reshape(num_timesteps, -1)\n",
    "        # The second skeleton remains zero due to preallocation\n",
    "\n",
    "    return x, y\n",
    "\n",
    "X_, Y_ = process_data_optimized()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_, Y_, test_size=0.3, random_state=42)\n",
    "# Split into validation and test\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=0.5, random_state=42)\n",
    "\n",
    "del X_, Y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGN\n",
    "\n",
    "All code in this section is adapted from Microsoft's SGN. [Github](https://github.com/microsoft/SGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from model import SGN\n",
    "from data import NTUDataLoaders, AverageMeter\n",
    "from util import make_dir, get_num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters/Tuning Parameters\n",
    "network='SGN'\n",
    "dataset='NTU'\n",
    "start_epoch=0\n",
    "case= 0 if is_privacy else 1 # 0 = privacy, 1 = Action\n",
    "batch_size=64\n",
    "max_epochs=120\n",
    "monitor='val_acc'\n",
    "lr=0.001\n",
    "weight_decay=0.0001\n",
    "lr_factor=0.1\n",
    "workers=16\n",
    "print_freq = 20\n",
    "do_train=1\n",
    "seg=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "\n",
    "        output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # clear gradients out before each mini-batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_freq == 0:\n",
    "            print('Epoch-{:<3d} {:3d} batches\\t'\n",
    "                  'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'accu {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                      epoch + 1, i + 1, loss=losses, acc=acces))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def test(test_loader, model, checkpoint, lable_path, pred_path):\n",
    "    acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.load_state_dict(torch.load(checkpoint)['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    np.savetxt(lable_path, label_output, fmt='%d')\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "    np.savetxt(pred_path, pred_output, fmt='%f')\n",
    "\n",
    "    print('Test: accuracy {:.3f}, time: {:.2f}s'\n",
    "          .format(acces.avg, time.time() - t_start))\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters:  680788\n",
      "The modes is: SGN\n",
      "It is using GPU!\n",
      "Train on 31742 samples, validate on 6802 samples\n",
      "0 0.001\n",
      "Epoch-1    20 batches\tloss 3.5786 (3.5962)\taccu 6.250 (9.766)\n",
      "Epoch-1    40 batches\tloss 3.2880 (3.4491)\taccu 15.625 (11.836)\n",
      "Epoch-1    60 batches\tloss 3.0740 (3.3757)\taccu 20.312 (13.281)\n",
      "Epoch-1    80 batches\tloss 2.8059 (3.2932)\taccu 25.000 (15.117)\n",
      "Epoch-1   100 batches\tloss 2.9519 (3.2154)\taccu 21.875 (16.703)\n",
      "Epoch-1   120 batches\tloss 2.8211 (3.1464)\taccu 20.312 (18.242)\n",
      "Epoch-1   140 batches\tloss 2.6868 (3.0857)\taccu 26.562 (19.665)\n",
      "Epoch-1   160 batches\tloss 2.5968 (3.0356)\taccu 35.938 (20.791)\n",
      "Epoch-1   180 batches\tloss 2.5880 (2.9955)\taccu 28.125 (21.493)\n",
      "Epoch-1   200 batches\tloss 2.8265 (2.9586)\taccu 23.438 (22.242)\n",
      "Epoch-1   220 batches\tloss 2.5366 (2.9239)\taccu 37.500 (23.153)\n",
      "Epoch-1   240 batches\tloss 2.5179 (2.8905)\taccu 37.500 (24.134)\n",
      "Epoch-1   260 batches\tloss 2.4291 (2.8580)\taccu 39.062 (24.982)\n",
      "Epoch-1   280 batches\tloss 2.4989 (2.8299)\taccu 29.688 (25.642)\n",
      "Epoch-1   300 batches\tloss 2.4096 (2.8052)\taccu 43.750 (26.281)\n",
      "Epoch-1   320 batches\tloss 2.4368 (2.7814)\taccu 40.625 (27.017)\n",
      "Epoch-1   340 batches\tloss 2.7276 (2.7573)\taccu 28.125 (27.619)\n",
      "Epoch-1   360 batches\tloss 2.4232 (2.7339)\taccu 32.812 (28.290)\n",
      "Epoch-1   380 batches\tloss 2.1778 (2.7127)\taccu 43.750 (28.984)\n",
      "Epoch-1   400 batches\tloss 2.3686 (2.6915)\taccu 43.750 (29.578)\n",
      "Epoch-1   420 batches\tloss 2.2928 (2.6743)\taccu 43.750 (29.970)\n",
      "Epoch-1   440 batches\tloss 2.1057 (2.6571)\taccu 50.000 (30.504)\n",
      "Epoch-1   460 batches\tloss 2.2771 (2.6407)\taccu 39.062 (30.975)\n",
      "Epoch-1   480 batches\tloss 2.3348 (2.6264)\taccu 39.062 (31.374)\n",
      "Epoch-1   112.2s\tTrain: loss 2.6154\taccu 31.7014\tValid: loss 2.0941\taccu 49.5136\n",
      "Epoch 1: val_acc improved from -inf to 49.5136, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "1 0.001\n",
      "Epoch-2    20 batches\tloss 2.0500 (2.2067)\taccu 54.688 (44.766)\n",
      "Epoch-2    40 batches\tloss 2.3892 (2.1766)\taccu 34.375 (45.664)\n",
      "Epoch-2    60 batches\tloss 2.2172 (2.1678)\taccu 45.312 (46.615)\n",
      "Epoch-2    80 batches\tloss 2.2538 (2.1709)\taccu 35.938 (45.996)\n",
      "Epoch-2   100 batches\tloss 2.0273 (2.1562)\taccu 50.000 (46.656)\n",
      "Epoch-2   120 batches\tloss 2.2231 (2.1546)\taccu 40.625 (46.732)\n",
      "Epoch-2   140 batches\tloss 1.9696 (2.1449)\taccu 50.000 (46.696)\n",
      "Epoch-2   160 batches\tloss 1.8359 (2.1454)\taccu 56.250 (46.631)\n",
      "Epoch-2   180 batches\tloss 2.2995 (2.1423)\taccu 40.625 (46.849)\n",
      "Epoch-2   200 batches\tloss 1.8056 (2.1362)\taccu 53.125 (47.000)\n",
      "Epoch-2   220 batches\tloss 2.0961 (2.1293)\taccu 50.000 (47.173)\n",
      "Epoch-2   240 batches\tloss 2.0828 (2.1265)\taccu 46.875 (47.285)\n",
      "Epoch-2   260 batches\tloss 2.0695 (2.1228)\taccu 53.125 (47.452)\n",
      "Epoch-2   280 batches\tloss 2.0793 (2.1155)\taccu 46.875 (47.656)\n",
      "Epoch-2   300 batches\tloss 2.2222 (2.1069)\taccu 42.188 (47.854)\n",
      "Epoch-2   320 batches\tloss 1.8651 (2.0938)\taccu 60.938 (48.477)\n",
      "Epoch-2   340 batches\tloss 2.0453 (2.0880)\taccu 56.250 (48.653)\n",
      "Epoch-2   360 batches\tloss 1.8290 (2.0804)\taccu 62.500 (48.915)\n",
      "Epoch-2   380 batches\tloss 1.8191 (2.0742)\taccu 60.938 (49.095)\n",
      "Epoch-2   400 batches\tloss 1.7477 (2.0670)\taccu 56.250 (49.422)\n",
      "Epoch-2   420 batches\tloss 1.9247 (2.0612)\taccu 54.688 (49.650)\n",
      "Epoch-2   440 batches\tloss 2.2193 (2.0532)\taccu 46.875 (49.975)\n",
      "Epoch-2   460 batches\tloss 2.0640 (2.0484)\taccu 40.625 (50.095)\n",
      "Epoch-2   480 batches\tloss 1.8255 (2.0429)\taccu 56.250 (50.264)\n",
      "Epoch-2   110.0s\tTrain: loss 2.0376\taccu 50.4703\tValid: loss 1.7787\taccu 60.4658\n",
      "Epoch 2: val_acc improved from 49.5136 to 60.4658, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "2 0.001\n",
      "Epoch-3    20 batches\tloss 1.8422 (1.8726)\taccu 56.250 (56.562)\n",
      "Epoch-3    40 batches\tloss 1.8600 (1.8601)\taccu 53.125 (57.031)\n",
      "Epoch-3    60 batches\tloss 1.8512 (1.8602)\taccu 57.812 (57.005)\n",
      "Epoch-3    80 batches\tloss 1.9730 (1.8565)\taccu 54.688 (57.324)\n",
      "Epoch-3   100 batches\tloss 1.8165 (1.8539)\taccu 64.062 (57.797)\n",
      "Epoch-3   120 batches\tloss 1.9551 (1.8503)\taccu 53.125 (57.747)\n",
      "Epoch-3   140 batches\tloss 2.1722 (1.8512)\taccu 48.438 (57.734)\n",
      "Epoch-3   160 batches\tloss 1.8904 (1.8458)\taccu 53.125 (57.969)\n",
      "Epoch-3   180 batches\tloss 1.6967 (1.8420)\taccu 68.750 (58.273)\n",
      "Epoch-3   200 batches\tloss 1.9801 (1.8362)\taccu 53.125 (58.523)\n",
      "Epoch-3   220 batches\tloss 1.8654 (1.8313)\taccu 57.812 (58.714)\n",
      "Epoch-3   240 batches\tloss 1.6839 (1.8263)\taccu 64.062 (59.043)\n",
      "Epoch-3   260 batches\tloss 1.7119 (1.8221)\taccu 73.438 (59.297)\n",
      "Epoch-3   280 batches\tloss 1.8226 (1.8219)\taccu 54.688 (59.342)\n",
      "Epoch-3   300 batches\tloss 1.5480 (1.8155)\taccu 67.188 (59.562)\n",
      "Epoch-3   320 batches\tloss 1.6760 (1.8134)\taccu 65.625 (59.575)\n",
      "Epoch-3   340 batches\tloss 1.8185 (1.8141)\taccu 59.375 (59.513)\n",
      "Epoch-3   360 batches\tloss 1.7926 (1.8122)\taccu 59.375 (59.583)\n",
      "Epoch-3   380 batches\tloss 1.5877 (1.8092)\taccu 71.875 (59.741)\n",
      "Epoch-3   400 batches\tloss 1.6395 (1.8088)\taccu 62.500 (59.730)\n",
      "Epoch-3   420 batches\tloss 1.7467 (1.8035)\taccu 68.750 (59.985)\n",
      "Epoch-3   440 batches\tloss 1.7867 (1.7987)\taccu 54.688 (60.167)\n",
      "Epoch-3   460 batches\tloss 1.7447 (1.7976)\taccu 67.188 (60.248)\n",
      "Epoch-3   480 batches\tloss 1.5675 (1.7952)\taccu 64.062 (60.339)\n",
      "Epoch-3   149.0s\tTrain: loss 1.7931\taccu 60.4325\tValid: loss 1.6020\taccu 68.8827\n",
      "Epoch 3: val_acc improved from 60.4658 to 68.8827, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "3 0.001\n",
      "Epoch-4    20 batches\tloss 1.7715 (1.6350)\taccu 60.938 (65.547)\n",
      "Epoch-4    40 batches\tloss 1.6221 (1.6690)\taccu 57.812 (64.570)\n",
      "Epoch-4    60 batches\tloss 1.4109 (1.6591)\taccu 73.438 (65.625)\n",
      "Epoch-4    80 batches\tloss 1.7047 (1.6495)\taccu 60.938 (66.406)\n",
      "Epoch-4   100 batches\tloss 1.6627 (1.6576)\taccu 67.188 (66.078)\n",
      "Epoch-4   120 batches\tloss 1.7466 (1.6647)\taccu 59.375 (65.716)\n",
      "Epoch-4   140 batches\tloss 1.4129 (1.6590)\taccu 71.875 (65.882)\n",
      "Epoch-4   160 batches\tloss 1.4536 (1.6580)\taccu 73.438 (65.947)\n",
      "Epoch-4   180 batches\tloss 1.6119 (1.6630)\taccu 67.188 (65.764)\n",
      "Epoch-4   200 batches\tloss 1.6113 (1.6611)\taccu 68.750 (65.703)\n",
      "Epoch-4   220 batches\tloss 1.8766 (1.6673)\taccu 51.562 (65.447)\n",
      "Epoch-4   240 batches\tloss 1.8411 (1.6699)\taccu 56.250 (65.352)\n",
      "Epoch-4   260 batches\tloss 1.4991 (1.6712)\taccu 65.625 (65.240)\n",
      "Epoch-4   280 batches\tloss 1.9102 (1.6714)\taccu 64.062 (65.312)\n",
      "Epoch-4   300 batches\tloss 1.5167 (1.6713)\taccu 71.875 (65.333)\n",
      "Epoch-4   320 batches\tloss 1.7010 (1.6695)\taccu 65.625 (65.381)\n",
      "Epoch-4   340 batches\tloss 1.6677 (1.6674)\taccu 62.500 (65.358)\n",
      "Epoch-4   360 batches\tloss 1.4324 (1.6659)\taccu 76.562 (65.378)\n",
      "Epoch-4   380 batches\tloss 1.7645 (1.6654)\taccu 65.625 (65.424)\n",
      "Epoch-4   400 batches\tloss 1.3900 (1.6613)\taccu 78.125 (65.586)\n",
      "Epoch-4   420 batches\tloss 1.3870 (1.6583)\taccu 81.250 (65.722)\n",
      "Epoch-4   440 batches\tloss 1.5712 (1.6560)\taccu 70.312 (65.810)\n",
      "Epoch-4   460 batches\tloss 1.4078 (1.6535)\taccu 73.438 (65.887)\n",
      "Epoch-4   480 batches\tloss 1.4835 (1.6517)\taccu 67.188 (65.938)\n",
      "Epoch-4   126.9s\tTrain: loss 1.6489\taccu 66.0480\tValid: loss 1.5140\taccu 71.9487\n",
      "Epoch 4: val_acc improved from 68.8827 to 71.9487, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "4 0.001\n",
      "Epoch-5    20 batches\tloss 1.3595 (1.5346)\taccu 78.125 (69.688)\n",
      "Epoch-5    40 batches\tloss 1.3428 (1.5402)\taccu 75.000 (70.195)\n",
      "Epoch-5    60 batches\tloss 1.4885 (1.5565)\taccu 65.625 (69.974)\n",
      "Epoch-5    80 batches\tloss 1.2979 (1.5557)\taccu 81.250 (69.824)\n",
      "Epoch-5   100 batches\tloss 1.4941 (1.5660)\taccu 71.875 (69.266)\n",
      "Epoch-5   120 batches\tloss 1.3562 (1.5661)\taccu 79.688 (69.284)\n",
      "Epoch-5   140 batches\tloss 1.3438 (1.5678)\taccu 81.250 (69.241)\n",
      "Epoch-5   160 batches\tloss 1.4973 (1.5690)\taccu 78.125 (69.023)\n",
      "Epoch-5   180 batches\tloss 1.6966 (1.5724)\taccu 60.938 (68.872)\n",
      "Epoch-5   200 batches\tloss 1.4364 (1.5723)\taccu 76.562 (68.859)\n",
      "Epoch-5   220 batches\tloss 1.4291 (1.5655)\taccu 73.438 (69.112)\n",
      "Epoch-5   240 batches\tloss 1.4191 (1.5620)\taccu 71.875 (69.219)\n",
      "Epoch-5   260 batches\tloss 1.3890 (1.5605)\taccu 71.875 (69.387)\n",
      "Epoch-5   280 batches\tloss 1.6887 (1.5575)\taccu 64.062 (69.526)\n",
      "Epoch-5   300 batches\tloss 1.5656 (1.5547)\taccu 68.750 (69.531)\n",
      "Epoch-5   320 batches\tloss 1.2484 (1.5556)\taccu 85.938 (69.473)\n",
      "Epoch-5   340 batches\tloss 1.6541 (1.5558)\taccu 64.062 (69.490)\n",
      "Epoch-5   360 batches\tloss 1.5165 (1.5567)\taccu 68.750 (69.497)\n",
      "Epoch-5   380 batches\tloss 1.8355 (1.5559)\taccu 54.688 (69.515)\n",
      "Epoch-5   400 batches\tloss 1.4456 (1.5572)\taccu 75.000 (69.484)\n",
      "Epoch-5   420 batches\tloss 1.7378 (1.5613)\taccu 64.062 (69.364)\n",
      "Epoch-5   440 batches\tloss 1.4620 (1.5595)\taccu 68.750 (69.400)\n",
      "Epoch-5   460 batches\tloss 1.4126 (1.5582)\taccu 76.562 (69.436)\n",
      "Epoch-5   480 batches\tloss 1.8358 (1.5569)\taccu 57.812 (69.508)\n",
      "Epoch-5   120.2s\tTrain: loss 1.5553\taccu 69.5202\tValid: loss 1.4458\taccu 75.0737\n",
      "Epoch 5: val_acc improved from 71.9487 to 75.0737, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "5 0.001\n",
      "Epoch-6    20 batches\tloss 1.2893 (1.3676)\taccu 82.812 (76.719)\n",
      "Epoch-6    40 batches\tloss 1.3290 (1.3966)\taccu 79.688 (76.133)\n",
      "Epoch-6    60 batches\tloss 1.6306 (1.4343)\taccu 70.312 (74.401)\n",
      "Epoch-6    80 batches\tloss 1.4329 (1.4493)\taccu 75.000 (74.160)\n",
      "Epoch-6   100 batches\tloss 1.4035 (1.4583)\taccu 78.125 (73.562)\n",
      "Epoch-6   120 batches\tloss 1.4670 (1.4634)\taccu 70.312 (73.411)\n",
      "Epoch-6   140 batches\tloss 1.4264 (1.4671)\taccu 73.438 (73.192)\n",
      "Epoch-6   160 batches\tloss 1.3873 (1.4668)\taccu 81.250 (73.262)\n",
      "Epoch-6   180 batches\tloss 1.5124 (1.4750)\taccu 76.562 (72.812)\n",
      "Epoch-6   200 batches\tloss 1.4454 (1.4725)\taccu 75.000 (72.953)\n",
      "Epoch-6   220 batches\tloss 1.6356 (1.4753)\taccu 71.875 (72.770)\n",
      "Epoch-6   240 batches\tloss 1.3557 (1.4783)\taccu 73.438 (72.734)\n",
      "Epoch-6   260 batches\tloss 1.6704 (1.4825)\taccu 70.312 (72.614)\n",
      "Epoch-6   280 batches\tloss 1.7133 (1.4842)\taccu 65.625 (72.478)\n",
      "Epoch-6   300 batches\tloss 1.3436 (1.4853)\taccu 76.562 (72.391)\n",
      "Epoch-6   320 batches\tloss 1.3518 (1.4870)\taccu 73.438 (72.363)\n",
      "Epoch-6   340 batches\tloss 1.6174 (1.4896)\taccu 65.625 (72.233)\n",
      "Epoch-6   360 batches\tloss 1.3588 (1.4882)\taccu 78.125 (72.279)\n",
      "Epoch-6   380 batches\tloss 1.5670 (1.4865)\taccu 76.562 (72.360)\n",
      "Epoch-6   400 batches\tloss 1.5430 (1.4849)\taccu 70.312 (72.461)\n",
      "Epoch-6   420 batches\tloss 1.3973 (1.4857)\taccu 78.125 (72.452)\n",
      "Epoch-6   440 batches\tloss 1.2571 (1.4853)\taccu 84.375 (72.454)\n",
      "Epoch-6   460 batches\tloss 1.3118 (1.4833)\taccu 75.000 (72.514)\n",
      "Epoch-6   480 batches\tloss 1.6397 (1.4803)\taccu 70.312 (72.650)\n",
      "Epoch-6   126.6s\tTrain: loss 1.4788\taccu 72.7178\tValid: loss 1.4252\taccu 75.3243\n",
      "Epoch 6: val_acc improved from 75.0737 to 75.3243, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "6 0.001\n",
      "Epoch-7    20 batches\tloss 1.3925 (1.4484)\taccu 78.125 (73.125)\n",
      "Epoch-7    40 batches\tloss 1.3546 (1.4519)\taccu 75.000 (72.852)\n",
      "Epoch-7    60 batches\tloss 1.6002 (1.4309)\taccu 70.312 (74.245)\n",
      "Epoch-7    80 batches\tloss 1.4259 (1.4322)\taccu 68.750 (74.375)\n",
      "Epoch-7   100 batches\tloss 1.3692 (1.4339)\taccu 78.125 (74.266)\n",
      "Epoch-7   120 batches\tloss 1.3318 (1.4289)\taccu 78.125 (74.622)\n",
      "Epoch-7   140 batches\tloss 1.4951 (1.4291)\taccu 68.750 (74.453)\n",
      "Epoch-7   160 batches\tloss 1.3769 (1.4281)\taccu 75.000 (74.512)\n",
      "Epoch-7   180 batches\tloss 1.2983 (1.4296)\taccu 81.250 (74.583)\n",
      "Epoch-7   200 batches\tloss 1.3889 (1.4266)\taccu 78.125 (74.766)\n",
      "Epoch-7   220 batches\tloss 1.2248 (1.4231)\taccu 81.250 (74.936)\n",
      "Epoch-7   240 batches\tloss 1.4673 (1.4217)\taccu 76.562 (75.085)\n",
      "Epoch-7   260 batches\tloss 1.3859 (1.4234)\taccu 79.688 (75.036)\n",
      "Epoch-7   280 batches\tloss 1.3705 (1.4231)\taccu 70.312 (75.056)\n",
      "Epoch-7   300 batches\tloss 1.3587 (1.4248)\taccu 78.125 (75.010)\n",
      "Epoch-7   320 batches\tloss 1.5870 (1.4232)\taccu 73.438 (75.107)\n",
      "Epoch-7   340 batches\tloss 1.3639 (1.4228)\taccu 78.125 (75.129)\n",
      "Epoch-7   360 batches\tloss 1.4091 (1.4227)\taccu 73.438 (75.178)\n",
      "Epoch-7   380 batches\tloss 1.3113 (1.4209)\taccu 76.562 (75.201)\n",
      "Epoch-7   400 batches\tloss 1.2887 (1.4198)\taccu 82.812 (75.266)\n",
      "Epoch-7   420 batches\tloss 1.4522 (1.4189)\taccu 70.312 (75.227)\n",
      "Epoch-7   440 batches\tloss 1.3295 (1.4193)\taccu 82.812 (75.234)\n",
      "Epoch-7   460 batches\tloss 1.4840 (1.4176)\taccu 76.562 (75.279)\n",
      "Epoch-7   480 batches\tloss 1.3095 (1.4176)\taccu 82.812 (75.290)\n",
      "Epoch-7   134.3s\tTrain: loss 1.4184\taccu 75.2273\tValid: loss 1.3369\taccu 79.2453\n",
      "Epoch 7: val_acc improved from 75.3243 to 79.2453, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "7 0.001\n",
      "Epoch-8    20 batches\tloss 1.3369 (1.2964)\taccu 84.375 (80.312)\n",
      "Epoch-8    40 batches\tloss 1.2606 (1.3358)\taccu 79.688 (78.906)\n",
      "Epoch-8    60 batches\tloss 1.2236 (1.3291)\taccu 84.375 (78.828)\n",
      "Epoch-8    80 batches\tloss 1.5239 (1.3316)\taccu 65.625 (78.652)\n",
      "Epoch-8   100 batches\tloss 1.4320 (1.3389)\taccu 78.125 (78.469)\n",
      "Epoch-8   120 batches\tloss 1.5188 (1.3560)\taccu 75.000 (77.682)\n",
      "Epoch-8   140 batches\tloss 1.3093 (1.3605)\taccu 81.250 (77.545)\n",
      "Epoch-8   160 batches\tloss 1.4093 (1.3629)\taccu 68.750 (77.285)\n",
      "Epoch-8   180 batches\tloss 1.3751 (1.3612)\taccu 73.438 (77.309)\n",
      "Epoch-8   200 batches\tloss 1.1899 (1.3638)\taccu 81.250 (77.188)\n",
      "Epoch-8   220 batches\tloss 1.4558 (1.3693)\taccu 73.438 (76.996)\n",
      "Epoch-8   240 batches\tloss 1.6493 (1.3699)\taccu 64.062 (76.979)\n",
      "Epoch-8   260 batches\tloss 1.3079 (1.3696)\taccu 79.688 (76.935)\n",
      "Epoch-8   280 batches\tloss 1.5331 (1.3685)\taccu 73.438 (76.953)\n",
      "Epoch-8   300 batches\tloss 1.3031 (1.3705)\taccu 82.812 (76.917)\n",
      "Epoch-8   320 batches\tloss 1.6612 (1.3732)\taccu 65.625 (76.748)\n",
      "Epoch-8   340 batches\tloss 1.2586 (1.3720)\taccu 79.688 (76.778)\n",
      "Epoch-8   360 batches\tloss 1.4532 (1.3733)\taccu 76.562 (76.740)\n",
      "Epoch-8   380 batches\tloss 1.4957 (1.3749)\taccu 65.625 (76.727)\n",
      "Epoch-8   400 batches\tloss 1.3358 (1.3742)\taccu 75.000 (76.754)\n",
      "Epoch-8   420 batches\tloss 1.3519 (1.3762)\taccu 79.688 (76.678)\n",
      "Epoch-8   440 batches\tloss 1.4788 (1.3781)\taccu 76.562 (76.605)\n",
      "Epoch-8   460 batches\tloss 1.2672 (1.3763)\taccu 82.812 (76.675)\n",
      "Epoch-8   480 batches\tloss 1.4209 (1.3756)\taccu 78.125 (76.676)\n",
      "Epoch-8   135.1s\tTrain: loss 1.3761\taccu 76.6320\tValid: loss 1.3586\taccu 77.6238\n",
      "Epoch 8: val_acc did not improve\n",
      "8 0.001\n",
      "Epoch-9    20 batches\tloss 1.2078 (1.3343)\taccu 85.938 (77.969)\n",
      "Epoch-9    40 batches\tloss 1.2102 (1.3432)\taccu 89.062 (78.203)\n",
      "Epoch-9    60 batches\tloss 1.2297 (1.3405)\taccu 84.375 (78.125)\n",
      "Epoch-9    80 batches\tloss 1.3629 (1.3405)\taccu 76.562 (77.812)\n",
      "Epoch-9   100 batches\tloss 1.2436 (1.3386)\taccu 85.938 (77.984)\n",
      "Epoch-9   120 batches\tloss 1.2197 (1.3346)\taccu 81.250 (78.151)\n",
      "Epoch-9   140 batches\tloss 1.3807 (1.3378)\taccu 78.125 (78.103)\n",
      "Epoch-9   160 batches\tloss 1.2969 (1.3368)\taccu 78.125 (78.184)\n",
      "Epoch-9   180 batches\tloss 1.2329 (1.3376)\taccu 79.688 (78.116)\n",
      "Epoch-9   200 batches\tloss 1.5014 (1.3372)\taccu 75.000 (78.172)\n",
      "Epoch-9   220 batches\tloss 1.5621 (1.3375)\taccu 65.625 (78.175)\n",
      "Epoch-9   240 batches\tloss 1.2788 (1.3382)\taccu 82.812 (78.197)\n",
      "Epoch-9   260 batches\tloss 1.4727 (1.3365)\taccu 75.000 (78.287)\n",
      "Epoch-9   280 batches\tloss 1.1737 (1.3366)\taccu 89.062 (78.265)\n",
      "Epoch-9   300 batches\tloss 1.3025 (1.3356)\taccu 79.688 (78.281)\n",
      "Epoch-9   320 batches\tloss 1.4592 (1.3375)\taccu 75.000 (78.140)\n",
      "Epoch-9   340 batches\tloss 1.2802 (1.3397)\taccu 81.250 (78.038)\n",
      "Epoch-9   360 batches\tloss 1.1673 (1.3374)\taccu 85.938 (78.155)\n",
      "Epoch-9   380 batches\tloss 1.4624 (1.3386)\taccu 73.438 (78.133)\n",
      "Epoch-9   400 batches\tloss 1.2191 (1.3386)\taccu 85.938 (78.125)\n",
      "Epoch-9   420 batches\tloss 1.5223 (1.3381)\taccu 73.438 (78.188)\n",
      "Epoch-9   440 batches\tloss 1.2978 (1.3404)\taccu 76.562 (78.114)\n",
      "Epoch-9   460 batches\tloss 1.4494 (1.3399)\taccu 78.125 (78.179)\n",
      "Epoch-9   480 batches\tloss 1.2306 (1.3376)\taccu 84.375 (78.255)\n",
      "Epoch-9   130.7s\tTrain: loss 1.3367\taccu 78.2955\tValid: loss 1.2665\taccu 81.9723\n",
      "Epoch 9: val_acc improved from 79.2453 to 81.9723, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "9 0.001\n",
      "Epoch-10   20 batches\tloss 1.1797 (1.2716)\taccu 81.250 (80.781)\n",
      "Epoch-10   40 batches\tloss 1.4098 (1.2771)\taccu 75.000 (81.406)\n",
      "Epoch-10   60 batches\tloss 1.2884 (1.2916)\taccu 79.688 (80.677)\n",
      "Epoch-10   80 batches\tloss 1.3092 (1.2991)\taccu 75.000 (80.430)\n",
      "Epoch-10  100 batches\tloss 1.2821 (1.2961)\taccu 76.562 (80.438)\n",
      "Epoch-10  120 batches\tloss 1.1845 (1.2891)\taccu 87.500 (80.781)\n",
      "Epoch-10  140 batches\tloss 1.2870 (1.2914)\taccu 82.812 (80.547)\n",
      "Epoch-10  160 batches\tloss 1.2179 (1.2859)\taccu 82.812 (80.684)\n",
      "Epoch-10  180 batches\tloss 1.4251 (1.2865)\taccu 79.688 (80.668)\n",
      "Epoch-10  200 batches\tloss 1.2003 (1.2868)\taccu 79.688 (80.586)\n",
      "Epoch-10  220 batches\tloss 1.2722 (1.2939)\taccu 79.688 (80.312)\n",
      "Epoch-10  240 batches\tloss 1.1573 (1.2946)\taccu 79.688 (80.215)\n",
      "Epoch-10  260 batches\tloss 1.4032 (1.2975)\taccu 70.312 (80.048)\n",
      "Epoch-10  280 batches\tloss 1.1693 (1.2997)\taccu 84.375 (79.944)\n",
      "Epoch-10  300 batches\tloss 1.3371 (1.3004)\taccu 75.000 (79.844)\n",
      "Epoch-10  320 batches\tloss 1.2539 (1.2994)\taccu 81.250 (79.912)\n",
      "Epoch-10  340 batches\tloss 1.3374 (1.3007)\taccu 81.250 (79.885)\n",
      "Epoch-10  360 batches\tloss 1.3131 (1.3011)\taccu 81.250 (79.861)\n",
      "Epoch-10  380 batches\tloss 1.3542 (1.3026)\taccu 71.875 (79.745)\n",
      "Epoch-10  400 batches\tloss 1.3469 (1.3035)\taccu 75.000 (79.660)\n",
      "Epoch-10  420 batches\tloss 1.1268 (1.3041)\taccu 87.500 (79.647)\n",
      "Epoch-10  440 batches\tloss 1.2807 (1.3046)\taccu 82.812 (79.616)\n",
      "Epoch-10  460 batches\tloss 1.1936 (1.3031)\taccu 85.938 (79.704)\n",
      "Epoch-10  480 batches\tloss 1.3873 (1.3017)\taccu 76.562 (79.779)\n",
      "Epoch-10  131.0s\tTrain: loss 1.3001\taccu 79.8674\tValid: loss 1.2605\taccu 82.3113\n",
      "Epoch 10: val_acc improved from 81.9723 to 82.3113, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "10 0.001\n",
      "Epoch-11   20 batches\tloss 1.3365 (1.2717)\taccu 81.250 (79.922)\n",
      "Epoch-11   40 batches\tloss 1.4674 (1.2758)\taccu 68.750 (79.766)\n",
      "Epoch-11   60 batches\tloss 1.3449 (1.2761)\taccu 81.250 (80.078)\n",
      "Epoch-11   80 batches\tloss 1.4451 (1.2761)\taccu 70.312 (80.137)\n",
      "Epoch-11  100 batches\tloss 1.1654 (1.2773)\taccu 89.062 (80.344)\n",
      "Epoch-11  120 batches\tloss 1.2237 (1.2703)\taccu 79.688 (80.534)\n",
      "Epoch-11  140 batches\tloss 1.2762 (1.2674)\taccu 78.125 (80.781)\n",
      "Epoch-11  160 batches\tloss 1.3705 (1.2665)\taccu 81.250 (81.016)\n",
      "Epoch-11  180 batches\tloss 1.1603 (1.2690)\taccu 84.375 (80.842)\n",
      "Epoch-11  200 batches\tloss 1.1611 (1.2695)\taccu 82.812 (80.969)\n",
      "Epoch-11  220 batches\tloss 1.1443 (1.2679)\taccu 87.500 (81.051)\n",
      "Epoch-11  240 batches\tloss 1.2381 (1.2666)\taccu 81.250 (81.146)\n",
      "Epoch-11  260 batches\tloss 1.1783 (1.2675)\taccu 81.250 (81.172)\n",
      "Epoch-11  280 batches\tloss 1.2065 (1.2699)\taccu 82.812 (81.071)\n",
      "Epoch-11  300 batches\tloss 1.2760 (1.2724)\taccu 78.125 (80.995)\n",
      "Epoch-11  320 batches\tloss 1.3193 (1.2743)\taccu 78.125 (80.938)\n",
      "Epoch-11  340 batches\tloss 1.2434 (1.2748)\taccu 84.375 (80.896)\n",
      "Epoch-11  360 batches\tloss 1.2141 (1.2777)\taccu 87.500 (80.820)\n",
      "Epoch-11  380 batches\tloss 1.4296 (1.2759)\taccu 75.000 (80.872)\n",
      "Epoch-11  400 batches\tloss 1.1752 (1.2774)\taccu 82.812 (80.797)\n",
      "Epoch-11  420 batches\tloss 1.3868 (1.2782)\taccu 73.438 (80.703)\n",
      "Epoch-11  440 batches\tloss 1.3259 (1.2787)\taccu 81.250 (80.749)\n",
      "Epoch-11  460 batches\tloss 1.2659 (1.2785)\taccu 76.562 (80.751)\n",
      "Epoch-11  480 batches\tloss 1.4098 (1.2789)\taccu 73.438 (80.706)\n",
      "Epoch-11  130.4s\tTrain: loss 1.2799\taccu 80.6313\tValid: loss 1.2683\taccu 81.8396\n",
      "Epoch 11: val_acc did not improve\n",
      "11 0.001\n",
      "Epoch-12   20 batches\tloss 1.2311 (1.2237)\taccu 82.812 (83.281)\n",
      "Epoch-12   40 batches\tloss 1.1755 (1.2221)\taccu 82.812 (82.695)\n",
      "Epoch-12   60 batches\tloss 1.2365 (1.2181)\taccu 82.812 (82.969)\n",
      "Epoch-12   80 batches\tloss 1.2245 (1.2202)\taccu 82.812 (82.734)\n",
      "Epoch-12  100 batches\tloss 1.1489 (1.2222)\taccu 84.375 (82.562)\n",
      "Epoch-12  120 batches\tloss 1.0471 (1.2247)\taccu 90.625 (82.435)\n",
      "Epoch-12  140 batches\tloss 1.4199 (1.2267)\taccu 78.125 (82.422)\n",
      "Epoch-12  160 batches\tloss 1.1467 (1.2322)\taccu 85.938 (82.207)\n",
      "Epoch-12  180 batches\tloss 1.1381 (1.2307)\taccu 85.938 (82.378)\n",
      "Epoch-12  200 batches\tloss 1.2094 (1.2326)\taccu 85.938 (82.344)\n",
      "Epoch-12  220 batches\tloss 1.2764 (1.2348)\taccu 81.250 (82.188)\n",
      "Epoch-12  240 batches\tloss 1.3669 (1.2360)\taccu 71.875 (82.135)\n",
      "Epoch-12  260 batches\tloss 1.2064 (1.2358)\taccu 84.375 (82.115)\n",
      "Epoch-12  280 batches\tloss 1.2743 (1.2355)\taccu 81.250 (82.148)\n",
      "Epoch-12  300 batches\tloss 1.2445 (1.2363)\taccu 79.688 (82.146)\n",
      "Epoch-12  320 batches\tloss 1.1569 (1.2349)\taccu 84.375 (82.231)\n",
      "Epoch-12  340 batches\tloss 1.0976 (1.2355)\taccu 85.938 (82.252)\n",
      "Epoch-12  360 batches\tloss 1.6134 (1.2375)\taccu 67.188 (82.088)\n",
      "Epoch-12  380 batches\tloss 1.4356 (1.2373)\taccu 79.688 (82.159)\n",
      "Epoch-12  400 batches\tloss 1.1970 (1.2363)\taccu 82.812 (82.230)\n",
      "Epoch-12  420 batches\tloss 1.3669 (1.2362)\taccu 70.312 (82.247)\n",
      "Epoch-12  440 batches\tloss 1.4370 (1.2377)\taccu 71.875 (82.223)\n",
      "Epoch-12  460 batches\tloss 1.1702 (1.2385)\taccu 82.812 (82.157)\n",
      "Epoch-12  480 batches\tloss 1.3053 (1.2389)\taccu 78.125 (82.139)\n",
      "Epoch-12  130.2s\tTrain: loss 1.2397\taccu 82.1307\tValid: loss 1.2418\taccu 83.2547\n",
      "Epoch 12: val_acc improved from 82.3113 to 83.2547, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "12 0.001\n",
      "Epoch-13   20 batches\tloss 1.1600 (1.1991)\taccu 87.500 (84.297)\n",
      "Epoch-13   40 batches\tloss 1.3473 (1.2026)\taccu 79.688 (84.102)\n",
      "Epoch-13   60 batches\tloss 1.1599 (1.2114)\taccu 87.500 (83.880)\n",
      "Epoch-13   80 batches\tloss 1.2978 (1.2122)\taccu 85.938 (84.023)\n",
      "Epoch-13  100 batches\tloss 1.2276 (1.2165)\taccu 82.812 (83.594)\n",
      "Epoch-13  120 batches\tloss 1.3276 (1.2091)\taccu 79.688 (83.763)\n",
      "Epoch-13  140 batches\tloss 1.3849 (1.2113)\taccu 71.875 (83.504)\n",
      "Epoch-13  160 batches\tloss 1.0045 (1.2102)\taccu 95.312 (83.535)\n",
      "Epoch-13  180 batches\tloss 1.1957 (1.2053)\taccu 85.938 (83.915)\n",
      "Epoch-13  200 batches\tloss 1.5442 (1.2092)\taccu 70.312 (83.648)\n",
      "Epoch-13  220 batches\tloss 1.0133 (1.2078)\taccu 93.750 (83.757)\n",
      "Epoch-13  240 batches\tloss 1.3855 (1.2110)\taccu 70.312 (83.633)\n",
      "Epoch-13  260 batches\tloss 1.0829 (1.2105)\taccu 89.062 (83.654)\n",
      "Epoch-13  280 batches\tloss 1.2126 (1.2090)\taccu 82.812 (83.756)\n",
      "Epoch-13  300 batches\tloss 1.2542 (1.2122)\taccu 79.688 (83.583)\n",
      "Epoch-13  320 batches\tloss 1.1319 (1.2143)\taccu 82.812 (83.521)\n",
      "Epoch-13  340 batches\tloss 1.1341 (1.2143)\taccu 87.500 (83.502)\n",
      "Epoch-13  360 batches\tloss 1.3191 (1.2176)\taccu 82.812 (83.403)\n",
      "Epoch-13  380 batches\tloss 1.2502 (1.2163)\taccu 85.938 (83.446)\n",
      "Epoch-13  400 batches\tloss 1.4278 (1.2181)\taccu 67.188 (83.371)\n",
      "Epoch-13  420 batches\tloss 1.2050 (1.2191)\taccu 84.375 (83.330)\n",
      "Epoch-13  440 batches\tloss 1.3733 (1.2214)\taccu 75.000 (83.232)\n",
      "Epoch-13  460 batches\tloss 1.5184 (1.2228)\taccu 71.875 (83.122)\n",
      "Epoch-13  480 batches\tloss 1.1999 (1.2233)\taccu 81.250 (83.060)\n",
      "Epoch-13  131.2s\tTrain: loss 1.2237\taccu 83.0114\tValid: loss 1.2408\taccu 82.5619\n",
      "Epoch 13: val_acc did not improve\n",
      "13 0.001\n",
      "Epoch-14   20 batches\tloss 1.1519 (1.1843)\taccu 89.062 (84.922)\n",
      "Epoch-14   40 batches\tloss 1.1100 (1.2053)\taccu 84.375 (83.555)\n",
      "Epoch-14   60 batches\tloss 1.1785 (1.2016)\taccu 90.625 (83.672)\n",
      "Epoch-14   80 batches\tloss 1.2445 (1.2151)\taccu 84.375 (83.105)\n",
      "Epoch-14  100 batches\tloss 1.0658 (1.2111)\taccu 87.500 (83.141)\n",
      "Epoch-14  120 batches\tloss 1.2274 (1.2099)\taccu 87.500 (83.451)\n",
      "Epoch-14  140 batches\tloss 1.1782 (1.2080)\taccu 89.062 (83.638)\n",
      "Epoch-14  160 batches\tloss 1.3773 (1.2129)\taccu 78.125 (83.496)\n",
      "Epoch-14  180 batches\tloss 1.0827 (1.2096)\taccu 90.625 (83.672)\n",
      "Epoch-14  200 batches\tloss 1.0946 (1.2057)\taccu 85.938 (83.773)\n",
      "Epoch-14  220 batches\tloss 1.0676 (1.2029)\taccu 93.750 (83.892)\n",
      "Epoch-14  240 batches\tloss 1.0827 (1.2043)\taccu 89.062 (83.835)\n",
      "Epoch-14  260 batches\tloss 1.1224 (1.2034)\taccu 87.500 (83.924)\n",
      "Epoch-14  280 batches\tloss 1.2602 (1.2038)\taccu 82.812 (83.895)\n",
      "Epoch-14  300 batches\tloss 1.3687 (1.2029)\taccu 85.938 (83.906)\n",
      "Epoch-14  320 batches\tloss 1.1135 (1.2041)\taccu 87.500 (83.906)\n",
      "Epoch-14  340 batches\tloss 1.3072 (1.2046)\taccu 78.125 (83.856)\n",
      "Epoch-14  360 batches\tloss 1.0625 (1.2028)\taccu 92.188 (83.893)\n",
      "Epoch-14  380 batches\tloss 1.1435 (1.2019)\taccu 81.250 (83.914)\n",
      "Epoch-14  400 batches\tloss 1.0550 (1.2002)\taccu 92.188 (83.969)\n",
      "Epoch-14  420 batches\tloss 1.3407 (1.2001)\taccu 79.688 (83.943)\n",
      "Epoch-14  440 batches\tloss 1.2074 (1.2008)\taccu 87.500 (83.920)\n",
      "Epoch-14  460 batches\tloss 1.2242 (1.2017)\taccu 81.250 (83.886)\n",
      "Epoch-14  480 batches\tloss 1.3749 (1.2032)\taccu 78.125 (83.818)\n",
      "Epoch-14  130.1s\tTrain: loss 1.2042\taccu 83.7879\tValid: loss 1.2607\taccu 82.3555\n",
      "Epoch 14: val_acc did not improve\n",
      "14 0.001\n",
      "Epoch-15   20 batches\tloss 1.1299 (1.1775)\taccu 87.500 (83.906)\n",
      "Epoch-15   40 batches\tloss 1.3037 (1.1815)\taccu 79.688 (83.945)\n",
      "Epoch-15   60 batches\tloss 1.2078 (1.1707)\taccu 82.812 (84.583)\n",
      "Epoch-15   80 batches\tloss 1.3789 (1.1725)\taccu 81.250 (84.863)\n",
      "Epoch-15  100 batches\tloss 1.2188 (1.1744)\taccu 84.375 (84.797)\n",
      "Epoch-15  120 batches\tloss 1.2699 (1.1753)\taccu 82.812 (84.583)\n",
      "Epoch-15  140 batches\tloss 1.2962 (1.1728)\taccu 75.000 (84.743)\n",
      "Epoch-15  160 batches\tloss 1.1258 (1.1748)\taccu 85.938 (84.727)\n",
      "Epoch-15  180 batches\tloss 1.1542 (1.1782)\taccu 87.500 (84.757)\n",
      "Epoch-15  200 batches\tloss 1.2171 (1.1774)\taccu 81.250 (84.805)\n",
      "Epoch-15  220 batches\tloss 1.2843 (1.1802)\taccu 82.812 (84.723)\n",
      "Epoch-15  240 batches\tloss 1.3150 (1.1815)\taccu 75.000 (84.629)\n",
      "Epoch-15  260 batches\tloss 1.2225 (1.1814)\taccu 79.688 (84.555)\n",
      "Epoch-15  280 batches\tloss 1.1284 (1.1802)\taccu 85.938 (84.587)\n",
      "Epoch-15  300 batches\tloss 1.2807 (1.1816)\taccu 81.250 (84.479)\n",
      "Epoch-15  320 batches\tloss 1.1595 (1.1825)\taccu 85.938 (84.443)\n",
      "Epoch-15  340 batches\tloss 1.1112 (1.1870)\taccu 87.500 (84.274)\n",
      "Epoch-15  360 batches\tloss 1.2200 (1.1883)\taccu 78.125 (84.214)\n",
      "Epoch-15  380 batches\tloss 1.1085 (1.1878)\taccu 89.062 (84.227)\n",
      "Epoch-15  400 batches\tloss 1.2962 (1.1877)\taccu 79.688 (84.223)\n",
      "Epoch-15  420 batches\tloss 1.0515 (1.1882)\taccu 87.500 (84.189)\n",
      "Epoch-15  440 batches\tloss 1.1062 (1.1878)\taccu 89.062 (84.151)\n",
      "Epoch-15  460 batches\tloss 1.2668 (1.1860)\taccu 81.250 (84.212)\n",
      "Epoch-15  480 batches\tloss 1.3228 (1.1870)\taccu 71.875 (84.147)\n",
      "Epoch-15  130.4s\tTrain: loss 1.1875\taccu 84.1635\tValid: loss 1.1973\taccu 84.1097\n",
      "Epoch 15: val_acc improved from 83.2547 to 84.1097, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "15 0.001\n",
      "Epoch-16   20 batches\tloss 1.0488 (1.1274)\taccu 90.625 (86.016)\n",
      "Epoch-16   40 batches\tloss 1.0768 (1.1297)\taccu 89.062 (86.289)\n",
      "Epoch-16   60 batches\tloss 1.0537 (1.1253)\taccu 89.062 (86.563)\n",
      "Epoch-16   80 batches\tloss 1.0962 (1.1377)\taccu 92.188 (86.230)\n",
      "Epoch-16  100 batches\tloss 1.1327 (1.1396)\taccu 89.062 (86.281)\n",
      "Epoch-16  120 batches\tloss 1.0351 (1.1372)\taccu 95.312 (86.432)\n",
      "Epoch-16  140 batches\tloss 1.1294 (1.1377)\taccu 92.188 (86.339)\n",
      "Epoch-16  160 batches\tloss 1.1113 (1.1422)\taccu 87.500 (86.191)\n",
      "Epoch-16  180 batches\tloss 1.0869 (1.1457)\taccu 87.500 (86.033)\n",
      "Epoch-16  200 batches\tloss 1.1545 (1.1475)\taccu 82.812 (85.945)\n",
      "Epoch-16  220 batches\tloss 1.4792 (1.1511)\taccu 73.438 (85.817)\n",
      "Epoch-16  240 batches\tloss 1.2889 (1.1521)\taccu 79.688 (85.807)\n",
      "Epoch-16  260 batches\tloss 1.1613 (1.1558)\taccu 89.062 (85.679)\n",
      "Epoch-16  280 batches\tloss 1.2184 (1.1554)\taccu 85.938 (85.709)\n",
      "Epoch-16  300 batches\tloss 1.4147 (1.1583)\taccu 73.438 (85.479)\n",
      "Epoch-16  320 batches\tloss 1.1151 (1.1592)\taccu 89.062 (85.527)\n",
      "Epoch-16  340 batches\tloss 1.1773 (1.1598)\taccu 81.250 (85.483)\n",
      "Epoch-16  360 batches\tloss 1.1808 (1.1608)\taccu 84.375 (85.434)\n",
      "Epoch-16  380 batches\tloss 1.1564 (1.1620)\taccu 84.375 (85.407)\n",
      "Epoch-16  400 batches\tloss 1.1841 (1.1613)\taccu 85.938 (85.441)\n",
      "Epoch-16  420 batches\tloss 1.2984 (1.1631)\taccu 78.125 (85.353)\n",
      "Epoch-16  440 batches\tloss 1.1437 (1.1632)\taccu 84.375 (85.327)\n",
      "Epoch-16  460 batches\tloss 1.2193 (1.1652)\taccu 76.562 (85.234)\n",
      "Epoch-16  480 batches\tloss 1.3523 (1.1676)\taccu 78.125 (85.075)\n",
      "Epoch-16  101.8s\tTrain: loss 1.1677\taccu 85.0505\tValid: loss 1.1913\taccu 85.1710\n",
      "Epoch 16: val_acc improved from 84.1097 to 85.1710, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "16 0.001\n",
      "Epoch-17   20 batches\tloss 1.2808 (1.1240)\taccu 78.125 (87.422)\n",
      "Epoch-17   40 batches\tloss 1.1732 (1.1306)\taccu 89.062 (87.031)\n",
      "Epoch-17   60 batches\tloss 1.0155 (1.1312)\taccu 93.750 (87.109)\n",
      "Epoch-17   80 batches\tloss 1.1391 (1.1297)\taccu 87.500 (86.973)\n",
      "Epoch-17  100 batches\tloss 1.2014 (1.1312)\taccu 81.250 (86.688)\n",
      "Epoch-17  120 batches\tloss 1.1025 (1.1371)\taccu 87.500 (86.380)\n",
      "Epoch-17  140 batches\tloss 1.1128 (1.1361)\taccu 89.062 (86.384)\n",
      "Epoch-17  160 batches\tloss 1.1357 (1.1429)\taccu 84.375 (85.967)\n",
      "Epoch-17  180 batches\tloss 1.1694 (1.1426)\taccu 84.375 (86.033)\n",
      "Epoch-17  200 batches\tloss 1.0529 (1.1418)\taccu 90.625 (86.062)\n",
      "Epoch-17  220 batches\tloss 1.3433 (1.1427)\taccu 75.000 (86.001)\n",
      "Epoch-17  240 batches\tloss 1.2257 (1.1414)\taccu 78.125 (86.042)\n",
      "Epoch-17  260 batches\tloss 1.0265 (1.1420)\taccu 90.625 (86.022)\n",
      "Epoch-17  280 batches\tloss 1.0521 (1.1431)\taccu 90.625 (85.971)\n",
      "Epoch-17  300 batches\tloss 1.1766 (1.1421)\taccu 84.375 (86.031)\n",
      "Epoch-17  320 batches\tloss 1.0293 (1.1443)\taccu 90.625 (85.913)\n",
      "Epoch-17  340 batches\tloss 1.1534 (1.1470)\taccu 87.500 (85.772)\n",
      "Epoch-17  360 batches\tloss 1.1772 (1.1470)\taccu 84.375 (85.816)\n",
      "Epoch-17  380 batches\tloss 1.1655 (1.1471)\taccu 85.938 (85.810)\n",
      "Epoch-17  400 batches\tloss 1.1146 (1.1485)\taccu 90.625 (85.754)\n",
      "Epoch-17  420 batches\tloss 1.0779 (1.1504)\taccu 90.625 (85.636)\n",
      "Epoch-17  440 batches\tloss 1.2340 (1.1505)\taccu 82.812 (85.668)\n",
      "Epoch-17  460 batches\tloss 1.2504 (1.1509)\taccu 81.250 (85.659)\n",
      "Epoch-17  480 batches\tloss 1.1011 (1.1517)\taccu 84.375 (85.622)\n",
      "Epoch-17  99.2s\tTrain: loss 1.1524\taccu 85.6155\tValid: loss 1.1875\taccu 84.3897\n",
      "Epoch 17: val_acc did not improve\n",
      "17 0.001\n",
      "Epoch-18   20 batches\tloss 1.1789 (1.1290)\taccu 85.938 (85.781)\n",
      "Epoch-18   40 batches\tloss 0.9905 (1.1201)\taccu 93.750 (86.523)\n",
      "Epoch-18   60 batches\tloss 1.2038 (1.1266)\taccu 79.688 (86.042)\n",
      "Epoch-18   80 batches\tloss 1.0265 (1.1163)\taccu 92.188 (86.738)\n",
      "Epoch-18  100 batches\tloss 1.2286 (1.1176)\taccu 85.938 (86.891)\n",
      "Epoch-18  120 batches\tloss 1.1930 (1.1176)\taccu 84.375 (86.953)\n",
      "Epoch-18  140 batches\tloss 1.1837 (1.1266)\taccu 81.250 (86.652)\n",
      "Epoch-18  160 batches\tloss 1.0017 (1.1277)\taccu 93.750 (86.602)\n",
      "Epoch-18  180 batches\tloss 1.0364 (1.1301)\taccu 90.625 (86.493)\n",
      "Epoch-18  200 batches\tloss 1.1381 (1.1332)\taccu 90.625 (86.453)\n",
      "Epoch-18  220 batches\tloss 1.2464 (1.1379)\taccu 78.125 (86.271)\n",
      "Epoch-18  240 batches\tloss 1.0754 (1.1418)\taccu 90.625 (86.159)\n",
      "Epoch-18  260 batches\tloss 1.1066 (1.1441)\taccu 85.938 (85.986)\n",
      "Epoch-18  280 batches\tloss 1.2291 (1.1437)\taccu 81.250 (86.077)\n",
      "Epoch-18  300 batches\tloss 1.0051 (1.1443)\taccu 92.188 (86.068)\n",
      "Epoch-18  320 batches\tloss 1.1208 (1.1451)\taccu 82.812 (86.045)\n",
      "Epoch-18  340 batches\tloss 1.1865 (1.1450)\taccu 85.938 (86.071)\n",
      "Epoch-18  360 batches\tloss 1.2530 (1.1453)\taccu 79.688 (86.081)\n",
      "Epoch-18  380 batches\tloss 1.2300 (1.1461)\taccu 78.125 (85.999)\n",
      "Epoch-18  400 batches\tloss 1.2240 (1.1473)\taccu 87.500 (85.973)\n",
      "Epoch-18  420 batches\tloss 1.1408 (1.1469)\taccu 84.375 (85.993)\n",
      "Epoch-18  440 batches\tloss 1.1811 (1.1469)\taccu 84.375 (85.991)\n",
      "Epoch-18  460 batches\tloss 1.1456 (1.1471)\taccu 87.500 (85.975)\n",
      "Epoch-18  480 batches\tloss 1.1518 (1.1481)\taccu 92.188 (85.951)\n",
      "Epoch-18  117.6s\tTrain: loss 1.1481\taccu 85.9407\tValid: loss 1.1753\taccu 85.0678\n",
      "Epoch 18: val_acc did not improve\n",
      "18 0.001\n",
      "Epoch-19   20 batches\tloss 1.0833 (1.1350)\taccu 90.625 (86.094)\n",
      "Epoch-19   40 batches\tloss 1.0709 (1.1326)\taccu 90.625 (86.133)\n",
      "Epoch-19   60 batches\tloss 1.0973 (1.1191)\taccu 89.062 (86.771)\n",
      "Epoch-19   80 batches\tloss 0.9965 (1.1232)\taccu 95.312 (86.465)\n",
      "Epoch-19  100 batches\tloss 1.0960 (1.1161)\taccu 85.938 (86.828)\n",
      "Epoch-19  120 batches\tloss 1.1627 (1.1181)\taccu 79.688 (86.797)\n",
      "Epoch-19  140 batches\tloss 1.1466 (1.1164)\taccu 84.375 (86.730)\n",
      "Epoch-19  160 batches\tloss 1.2367 (1.1202)\taccu 81.250 (86.504)\n",
      "Epoch-19  180 batches\tloss 1.2546 (1.1242)\taccu 84.375 (86.389)\n",
      "Epoch-19  200 batches\tloss 1.1722 (1.1280)\taccu 85.938 (86.148)\n",
      "Epoch-19  220 batches\tloss 1.1352 (1.1269)\taccu 90.625 (86.257)\n",
      "Epoch-19  240 batches\tloss 1.0822 (1.1267)\taccu 85.938 (86.296)\n",
      "Epoch-19  260 batches\tloss 1.0219 (1.1287)\taccu 90.625 (86.256)\n",
      "Epoch-19  280 batches\tloss 1.2416 (1.1302)\taccu 79.688 (86.189)\n",
      "Epoch-19  300 batches\tloss 1.0772 (1.1303)\taccu 87.500 (86.161)\n",
      "Epoch-19  320 batches\tloss 1.2028 (1.1302)\taccu 81.250 (86.221)\n",
      "Epoch-19  340 batches\tloss 1.0290 (1.1301)\taccu 90.625 (86.241)\n",
      "Epoch-19  360 batches\tloss 1.1837 (1.1306)\taccu 87.500 (86.211)\n",
      "Epoch-19  380 batches\tloss 1.1884 (1.1310)\taccu 81.250 (86.147)\n",
      "Epoch-19  400 batches\tloss 1.2463 (1.1337)\taccu 84.375 (86.039)\n",
      "Epoch-19  420 batches\tloss 1.0960 (1.1350)\taccu 89.062 (85.993)\n",
      "Epoch-19  440 batches\tloss 1.0269 (1.1355)\taccu 90.625 (85.948)\n",
      "Epoch-19  460 batches\tloss 1.1397 (1.1367)\taccu 82.812 (85.907)\n",
      "Epoch-19  480 batches\tloss 1.1898 (1.1378)\taccu 82.812 (85.879)\n",
      "Epoch-19  131.3s\tTrain: loss 1.1384\taccu 85.8681\tValid: loss 1.1725\taccu 84.8172\n",
      "Epoch 19: val_acc did not improve\n",
      "19 0.001\n",
      "Epoch-20   20 batches\tloss 1.0369 (1.1114)\taccu 93.750 (87.578)\n",
      "Epoch-20   40 batches\tloss 1.0610 (1.0970)\taccu 93.750 (88.594)\n",
      "Epoch-20   60 batches\tloss 1.0562 (1.0922)\taccu 89.062 (88.255)\n",
      "Epoch-20   80 batches\tloss 1.0738 (1.0960)\taccu 87.500 (88.066)\n",
      "Epoch-20  100 batches\tloss 1.0150 (1.0984)\taccu 90.625 (88.047)\n",
      "Epoch-20  120 batches\tloss 1.2510 (1.1057)\taccu 75.000 (87.695)\n",
      "Epoch-20  140 batches\tloss 1.1141 (1.1076)\taccu 85.938 (87.467)\n",
      "Epoch-20  160 batches\tloss 1.3867 (1.1138)\taccu 73.438 (87.051)\n",
      "Epoch-20  180 batches\tloss 1.1301 (1.1128)\taccu 82.812 (87.092)\n",
      "Epoch-20  200 batches\tloss 1.0030 (1.1111)\taccu 92.188 (87.250)\n",
      "Epoch-20  220 batches\tloss 1.2008 (1.1150)\taccu 81.250 (87.031)\n",
      "Epoch-20  240 batches\tloss 1.3042 (1.1183)\taccu 76.562 (86.875)\n",
      "Epoch-20  260 batches\tloss 1.1433 (1.1196)\taccu 87.500 (86.863)\n",
      "Epoch-20  280 batches\tloss 1.0427 (1.1188)\taccu 90.625 (86.780)\n",
      "Epoch-20  300 batches\tloss 1.0202 (1.1179)\taccu 92.188 (86.844)\n",
      "Epoch-20  320 batches\tloss 1.1262 (1.1184)\taccu 87.500 (86.855)\n",
      "Epoch-20  340 batches\tloss 1.1668 (1.1195)\taccu 84.375 (86.838)\n",
      "Epoch-20  360 batches\tloss 1.2662 (1.1207)\taccu 76.562 (86.819)\n",
      "Epoch-20  380 batches\tloss 1.1546 (1.1207)\taccu 89.062 (86.883)\n",
      "Epoch-20  400 batches\tloss 1.1636 (1.1205)\taccu 85.938 (86.906)\n",
      "Epoch-20  420 batches\tloss 1.0864 (1.1203)\taccu 92.188 (86.964)\n",
      "Epoch-20  440 batches\tloss 1.1479 (1.1208)\taccu 87.500 (86.889)\n",
      "Epoch-20  460 batches\tloss 0.9768 (1.1223)\taccu 96.875 (86.868)\n",
      "Epoch-20  480 batches\tloss 1.3103 (1.1233)\taccu 75.000 (86.829)\n",
      "Epoch-20  131.2s\tTrain: loss 1.1241\taccu 86.7772\tValid: loss 1.1657\taccu 85.7459\n",
      "Epoch 20: val_acc improved from 85.1710 to 85.7459, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "20 0.001\n",
      "Epoch-21   20 batches\tloss 1.0990 (1.0821)\taccu 90.625 (89.297)\n",
      "Epoch-21   40 batches\tloss 1.1474 (1.0881)\taccu 81.250 (88.906)\n",
      "Epoch-21   60 batches\tloss 1.0857 (1.0957)\taccu 89.062 (88.021)\n",
      "Epoch-21   80 batches\tloss 1.0181 (1.0924)\taccu 92.188 (88.125)\n",
      "Epoch-21  100 batches\tloss 0.9415 (1.0904)\taccu 95.312 (88.297)\n",
      "Epoch-21  120 batches\tloss 1.0819 (1.0916)\taccu 90.625 (88.268)\n",
      "Epoch-21  140 batches\tloss 1.1011 (1.0931)\taccu 89.062 (88.170)\n",
      "Epoch-21  160 batches\tloss 1.1390 (1.0928)\taccu 82.812 (88.213)\n",
      "Epoch-21  180 batches\tloss 1.0396 (1.0920)\taccu 89.062 (88.177)\n",
      "Epoch-21  200 batches\tloss 1.2255 (1.0930)\taccu 79.688 (88.109)\n",
      "Epoch-21  220 batches\tloss 1.0975 (1.0938)\taccu 85.938 (88.104)\n",
      "Epoch-21  240 batches\tloss 1.1900 (1.0989)\taccu 85.938 (87.910)\n",
      "Epoch-21  260 batches\tloss 1.2491 (1.1021)\taccu 81.250 (87.806)\n",
      "Epoch-21  280 batches\tloss 1.0752 (1.1026)\taccu 92.188 (87.762)\n",
      "Epoch-21  300 batches\tloss 1.1226 (1.1031)\taccu 84.375 (87.719)\n",
      "Epoch-21  320 batches\tloss 1.1317 (1.1027)\taccu 85.938 (87.710)\n",
      "Epoch-21  340 batches\tloss 1.1722 (1.1031)\taccu 81.250 (87.739)\n",
      "Epoch-21  360 batches\tloss 1.0576 (1.1055)\taccu 90.625 (87.630)\n",
      "Epoch-21  380 batches\tloss 1.0577 (1.1063)\taccu 90.625 (87.632)\n",
      "Epoch-21  400 batches\tloss 1.1621 (1.1081)\taccu 84.375 (87.547)\n",
      "Epoch-21  420 batches\tloss 1.0773 (1.1082)\taccu 89.062 (87.522)\n",
      "Epoch-21  440 batches\tloss 1.0658 (1.1086)\taccu 87.500 (87.496)\n",
      "Epoch-21  460 batches\tloss 1.0793 (1.1102)\taccu 87.500 (87.432)\n",
      "Epoch-21  480 batches\tloss 1.1588 (1.1114)\taccu 85.938 (87.386)\n",
      "Epoch-21  130.7s\tTrain: loss 1.1117\taccu 87.3453\tValid: loss 1.1498\taccu 86.4387\n",
      "Epoch 21: val_acc improved from 85.7459 to 86.4387, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "21 0.001\n",
      "Epoch-22   20 batches\tloss 1.0840 (1.0882)\taccu 85.938 (87.422)\n",
      "Epoch-22   40 batches\tloss 1.0919 (1.0777)\taccu 90.625 (88.164)\n",
      "Epoch-22   60 batches\tloss 1.1498 (1.0799)\taccu 84.375 (88.438)\n",
      "Epoch-22   80 batches\tloss 0.9978 (1.0750)\taccu 93.750 (88.945)\n",
      "Epoch-22  100 batches\tloss 1.0749 (1.0786)\taccu 87.500 (88.688)\n",
      "Epoch-22  120 batches\tloss 1.0893 (1.0885)\taccu 87.500 (88.177)\n",
      "Epoch-22  140 batches\tloss 1.0121 (1.0898)\taccu 95.312 (87.991)\n",
      "Epoch-22  160 batches\tloss 1.0827 (1.0925)\taccu 92.188 (87.842)\n",
      "Epoch-22  180 batches\tloss 0.9692 (1.0984)\taccu 95.312 (87.682)\n",
      "Epoch-22  200 batches\tloss 1.0589 (1.1016)\taccu 87.500 (87.570)\n",
      "Epoch-22  220 batches\tloss 1.0064 (1.1019)\taccu 90.625 (87.663)\n",
      "Epoch-22  240 batches\tloss 1.1311 (1.1031)\taccu 87.500 (87.630)\n",
      "Epoch-22  260 batches\tloss 1.0882 (1.1015)\taccu 89.062 (87.710)\n",
      "Epoch-22  280 batches\tloss 0.9442 (1.0995)\taccu 95.312 (87.773)\n",
      "Epoch-22  300 batches\tloss 0.9875 (1.0993)\taccu 92.188 (87.734)\n",
      "Epoch-22  320 batches\tloss 1.0214 (1.0994)\taccu 89.062 (87.671)\n",
      "Epoch-22  340 batches\tloss 1.1690 (1.1011)\taccu 85.938 (87.583)\n",
      "Epoch-22  360 batches\tloss 1.1975 (1.1015)\taccu 81.250 (87.565)\n",
      "Epoch-22  380 batches\tloss 1.1658 (1.1024)\taccu 89.062 (87.553)\n",
      "Epoch-22  400 batches\tloss 1.1860 (1.1029)\taccu 90.625 (87.512)\n",
      "Epoch-22  420 batches\tloss 1.0770 (1.1040)\taccu 87.500 (87.496)\n",
      "Epoch-22  440 batches\tloss 1.1352 (1.1046)\taccu 87.500 (87.500)\n",
      "Epoch-22  460 batches\tloss 1.1709 (1.1046)\taccu 84.375 (87.490)\n",
      "Epoch-22  480 batches\tloss 1.0042 (1.1060)\taccu 95.312 (87.461)\n",
      "Epoch-22  131.2s\tTrain: loss 1.1050\taccu 87.5158\tValid: loss 1.1513\taccu 86.3355\n",
      "Epoch 22: val_acc did not improve\n",
      "22 0.001\n",
      "Epoch-23   20 batches\tloss 1.0654 (1.0467)\taccu 89.062 (90.312)\n",
      "Epoch-23   40 batches\tloss 1.1396 (1.0564)\taccu 84.375 (89.609)\n",
      "Epoch-23   60 batches\tloss 1.1909 (1.0593)\taccu 85.938 (89.505)\n",
      "Epoch-23   80 batches\tloss 1.2081 (1.0671)\taccu 82.812 (89.219)\n",
      "Epoch-23  100 batches\tloss 1.0821 (1.0701)\taccu 87.500 (88.953)\n",
      "Epoch-23  120 batches\tloss 1.2739 (1.0719)\taccu 81.250 (88.789)\n",
      "Epoch-23  140 batches\tloss 1.2433 (1.0731)\taccu 84.375 (88.728)\n",
      "Epoch-23  160 batches\tloss 1.0670 (1.0746)\taccu 90.625 (88.711)\n",
      "Epoch-23  180 batches\tloss 1.2007 (1.0758)\taccu 85.938 (88.681)\n",
      "Epoch-23  200 batches\tloss 0.9736 (1.0761)\taccu 90.625 (88.680)\n",
      "Epoch-23  220 batches\tloss 1.0766 (1.0771)\taccu 89.062 (88.544)\n",
      "Epoch-23  240 batches\tloss 1.1056 (1.0800)\taccu 87.500 (88.359)\n",
      "Epoch-23  260 batches\tloss 1.1795 (1.0802)\taccu 84.375 (88.323)\n",
      "Epoch-23  280 batches\tloss 1.0614 (1.0844)\taccu 90.625 (88.147)\n",
      "Epoch-23  300 batches\tloss 1.0049 (1.0869)\taccu 92.188 (87.979)\n",
      "Epoch-23  320 batches\tloss 1.1964 (1.0886)\taccu 82.812 (87.920)\n",
      "Epoch-23  340 batches\tloss 1.1654 (1.0898)\taccu 87.500 (87.941)\n",
      "Epoch-23  360 batches\tloss 1.1024 (1.0884)\taccu 84.375 (88.051)\n",
      "Epoch-23  380 batches\tloss 1.1855 (1.0887)\taccu 87.500 (88.072)\n",
      "Epoch-23  400 batches\tloss 1.0982 (1.0883)\taccu 84.375 (88.066)\n",
      "Epoch-23  420 batches\tloss 1.1270 (1.0894)\taccu 85.938 (88.051)\n",
      "Epoch-23  440 batches\tloss 1.3355 (1.0902)\taccu 81.250 (87.994)\n",
      "Epoch-23  460 batches\tloss 1.0000 (1.0920)\taccu 90.625 (87.921)\n",
      "Epoch-23  480 batches\tloss 1.1934 (1.0924)\taccu 78.125 (87.904)\n",
      "Epoch-23  130.5s\tTrain: loss 1.0936\taccu 87.8662\tValid: loss 1.1419\taccu 86.9251\n",
      "Epoch 23: val_acc improved from 86.4387 to 86.9251, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "23 0.001\n",
      "Epoch-24   20 batches\tloss 0.9722 (1.0432)\taccu 92.188 (90.391)\n",
      "Epoch-24   40 batches\tloss 1.0910 (1.0674)\taccu 84.375 (89.297)\n",
      "Epoch-24   60 batches\tloss 1.1516 (1.0626)\taccu 84.375 (89.323)\n",
      "Epoch-24   80 batches\tloss 1.1278 (1.0656)\taccu 93.750 (89.121)\n",
      "Epoch-24  100 batches\tloss 1.1260 (1.0662)\taccu 84.375 (89.109)\n",
      "Epoch-24  120 batches\tloss 1.0076 (1.0722)\taccu 93.750 (88.828)\n",
      "Epoch-24  140 batches\tloss 1.1577 (1.0692)\taccu 89.062 (89.129)\n",
      "Epoch-24  160 batches\tloss 1.1585 (1.0701)\taccu 81.250 (89.111)\n",
      "Epoch-24  180 batches\tloss 1.0951 (1.0711)\taccu 87.500 (89.071)\n",
      "Epoch-24  200 batches\tloss 1.0104 (1.0700)\taccu 92.188 (89.102)\n",
      "Epoch-24  220 batches\tloss 0.9950 (1.0709)\taccu 92.188 (88.999)\n",
      "Epoch-24  240 batches\tloss 0.9964 (1.0742)\taccu 90.625 (88.828)\n",
      "Epoch-24  260 batches\tloss 0.9394 (1.0724)\taccu 92.188 (88.810)\n",
      "Epoch-24  280 batches\tloss 1.0850 (1.0740)\taccu 87.500 (88.789)\n",
      "Epoch-24  300 batches\tloss 1.1110 (1.0750)\taccu 85.938 (88.714)\n",
      "Epoch-24  320 batches\tloss 1.0606 (1.0759)\taccu 89.062 (88.721)\n",
      "Epoch-24  340 batches\tloss 1.0752 (1.0771)\taccu 90.625 (88.704)\n",
      "Epoch-24  360 batches\tloss 1.0667 (1.0773)\taccu 87.500 (88.711)\n",
      "Epoch-24  380 batches\tloss 1.0259 (1.0776)\taccu 90.625 (88.672)\n",
      "Epoch-24  400 batches\tloss 1.0174 (1.0789)\taccu 92.188 (88.629)\n",
      "Epoch-24  420 batches\tloss 1.0353 (1.0795)\taccu 95.312 (88.642)\n",
      "Epoch-24  440 batches\tloss 1.0493 (1.0808)\taccu 87.500 (88.530)\n",
      "Epoch-24  460 batches\tloss 1.1993 (1.0824)\taccu 81.250 (88.431)\n",
      "Epoch-24  480 batches\tloss 1.1267 (1.0833)\taccu 87.500 (88.369)\n",
      "Epoch-24  131.0s\tTrain: loss 1.0842\taccu 88.3081\tValid: loss 1.1570\taccu 85.3774\n",
      "Epoch 24: val_acc did not improve\n",
      "24 0.001\n",
      "Epoch-25   20 batches\tloss 1.0847 (1.0567)\taccu 90.625 (89.531)\n",
      "Epoch-25   40 batches\tloss 0.9866 (1.0539)\taccu 95.312 (90.273)\n",
      "Epoch-25   60 batches\tloss 1.0041 (1.0619)\taccu 85.938 (89.792)\n",
      "Epoch-25   80 batches\tloss 1.0673 (1.0621)\taccu 89.062 (89.688)\n",
      "Epoch-25  100 batches\tloss 1.0892 (1.0607)\taccu 92.188 (89.672)\n",
      "Epoch-25  120 batches\tloss 1.0133 (1.0585)\taccu 90.625 (89.583)\n",
      "Epoch-25  140 batches\tloss 1.1470 (1.0593)\taccu 89.062 (89.587)\n",
      "Epoch-25  160 batches\tloss 1.0786 (1.0595)\taccu 84.375 (89.541)\n",
      "Epoch-25  180 batches\tloss 1.0731 (1.0606)\taccu 90.625 (89.523)\n",
      "Epoch-25  200 batches\tloss 1.1312 (1.0629)\taccu 84.375 (89.453)\n",
      "Epoch-25  220 batches\tloss 1.0800 (1.0639)\taccu 87.500 (89.418)\n",
      "Epoch-25  240 batches\tloss 1.0518 (1.0650)\taccu 90.625 (89.323)\n",
      "Epoch-25  260 batches\tloss 1.0942 (1.0647)\taccu 90.625 (89.249)\n",
      "Epoch-25  280 batches\tloss 0.9287 (1.0638)\taccu 95.312 (89.286)\n",
      "Epoch-25  300 batches\tloss 1.0435 (1.0642)\taccu 95.312 (89.292)\n",
      "Epoch-25  320 batches\tloss 1.0139 (1.0663)\taccu 93.750 (89.150)\n",
      "Epoch-25  340 batches\tloss 1.0856 (1.0671)\taccu 89.062 (89.164)\n",
      "Epoch-25  360 batches\tloss 1.1617 (1.0685)\taccu 82.812 (89.045)\n",
      "Epoch-25  380 batches\tloss 1.1934 (1.0696)\taccu 82.812 (88.993)\n",
      "Epoch-25  400 batches\tloss 1.0072 (1.0719)\taccu 93.750 (88.891)\n",
      "Epoch-25  420 batches\tloss 1.0033 (1.0751)\taccu 92.188 (88.761)\n",
      "Epoch-25  440 batches\tloss 1.1710 (1.0773)\taccu 85.938 (88.704)\n",
      "Epoch-25  460 batches\tloss 1.1717 (1.0768)\taccu 89.062 (88.753)\n",
      "Epoch-25  480 batches\tloss 1.1281 (1.0776)\taccu 84.375 (88.711)\n",
      "Epoch-25  130.9s\tTrain: loss 1.0776\taccu 88.7090\tValid: loss 1.1509\taccu 85.8343\n",
      "Epoch 25: val_acc did not improve\n",
      "25 0.001\n",
      "Epoch-26   20 batches\tloss 0.9597 (1.0573)\taccu 92.188 (90.000)\n",
      "Epoch-26   40 batches\tloss 1.0886 (1.0610)\taccu 90.625 (89.414)\n",
      "Epoch-26   60 batches\tloss 1.0351 (1.0632)\taccu 89.062 (89.115)\n",
      "Epoch-26   80 batches\tloss 0.9723 (1.0608)\taccu 93.750 (89.375)\n",
      "Epoch-26  100 batches\tloss 0.9861 (1.0630)\taccu 95.312 (89.234)\n",
      "Epoch-26  120 batches\tloss 1.0558 (1.0592)\taccu 92.188 (89.505)\n",
      "Epoch-26  140 batches\tloss 1.0579 (1.0592)\taccu 87.500 (89.487)\n",
      "Epoch-26  160 batches\tloss 1.0647 (1.0593)\taccu 93.750 (89.521)\n",
      "Epoch-26  180 batches\tloss 1.0205 (1.0614)\taccu 93.750 (89.462)\n",
      "Epoch-26  200 batches\tloss 1.0844 (1.0607)\taccu 81.250 (89.391)\n",
      "Epoch-26  220 batches\tloss 1.0842 (1.0592)\taccu 87.500 (89.467)\n",
      "Epoch-26  240 batches\tloss 0.9992 (1.0617)\taccu 95.312 (89.375)\n",
      "Epoch-26  260 batches\tloss 1.1174 (1.0656)\taccu 87.500 (89.243)\n",
      "Epoch-26  280 batches\tloss 0.9177 (1.0639)\taccu 95.312 (89.269)\n",
      "Epoch-26  300 batches\tloss 1.1427 (1.0623)\taccu 87.500 (89.380)\n",
      "Epoch-26  320 batches\tloss 0.9875 (1.0653)\taccu 89.062 (89.199)\n",
      "Epoch-26  340 batches\tloss 1.0287 (1.0668)\taccu 85.938 (89.072)\n",
      "Epoch-26  360 batches\tloss 1.2887 (1.0692)\taccu 84.375 (88.950)\n",
      "Epoch-26  380 batches\tloss 1.0855 (1.0728)\taccu 89.062 (88.820)\n",
      "Epoch-26  400 batches\tloss 0.9564 (1.0739)\taccu 95.312 (88.812)\n",
      "Epoch-26  420 batches\tloss 1.0174 (1.0749)\taccu 90.625 (88.728)\n",
      "Epoch-26  440 batches\tloss 1.0204 (1.0754)\taccu 92.188 (88.718)\n",
      "Epoch-26  460 batches\tloss 1.1152 (1.0759)\taccu 87.500 (88.651)\n",
      "Epoch-26  480 batches\tloss 1.0837 (1.0768)\taccu 92.188 (88.636)\n",
      "Epoch-26  130.3s\tTrain: loss 1.0767\taccu 88.6332\tValid: loss 1.1271\taccu 87.1905\n",
      "Epoch 26: val_acc improved from 86.9251 to 87.1905, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "26 0.001\n",
      "Epoch-27   20 batches\tloss 1.1441 (1.0273)\taccu 85.938 (91.406)\n",
      "Epoch-27   40 batches\tloss 0.9896 (1.0339)\taccu 90.625 (90.898)\n",
      "Epoch-27   60 batches\tloss 0.9971 (1.0333)\taccu 87.500 (90.651)\n",
      "Epoch-27   80 batches\tloss 0.9739 (1.0405)\taccu 93.750 (90.273)\n",
      "Epoch-27  100 batches\tloss 0.9860 (1.0378)\taccu 92.188 (90.469)\n",
      "Epoch-27  120 batches\tloss 0.9612 (1.0359)\taccu 93.750 (90.469)\n",
      "Epoch-27  140 batches\tloss 1.1679 (1.0325)\taccu 82.812 (90.547)\n",
      "Epoch-27  160 batches\tloss 1.0292 (1.0309)\taccu 92.188 (90.586)\n",
      "Epoch-27  180 batches\tloss 1.1270 (1.0381)\taccu 84.375 (90.208)\n",
      "Epoch-27  200 batches\tloss 1.0647 (1.0415)\taccu 90.625 (90.008)\n",
      "Epoch-27  220 batches\tloss 0.9621 (1.0419)\taccu 95.312 (90.064)\n",
      "Epoch-27  240 batches\tloss 1.3105 (1.0434)\taccu 73.438 (89.993)\n",
      "Epoch-27  260 batches\tloss 1.1363 (1.0462)\taccu 81.250 (89.826)\n",
      "Epoch-27  280 batches\tloss 0.9562 (1.0457)\taccu 93.750 (89.860)\n",
      "Epoch-27  300 batches\tloss 1.0513 (1.0456)\taccu 90.625 (89.922)\n",
      "Epoch-27  320 batches\tloss 1.4811 (1.0492)\taccu 71.875 (89.790)\n",
      "Epoch-27  340 batches\tloss 1.0727 (1.0493)\taccu 87.500 (89.766)\n",
      "Epoch-27  360 batches\tloss 1.0450 (1.0520)\taccu 87.500 (89.670)\n",
      "Epoch-27  380 batches\tloss 1.0452 (1.0525)\taccu 92.188 (89.638)\n",
      "Epoch-27  400 batches\tloss 1.0600 (1.0521)\taccu 93.750 (89.699)\n",
      "Epoch-27  420 batches\tloss 0.9905 (1.0540)\taccu 92.188 (89.613)\n",
      "Epoch-27  440 batches\tloss 1.1647 (1.0564)\taccu 84.375 (89.474)\n",
      "Epoch-27  460 batches\tloss 1.0182 (1.0574)\taccu 93.750 (89.453)\n",
      "Epoch-27  480 batches\tloss 0.9903 (1.0581)\taccu 92.188 (89.421)\n",
      "Epoch-27  130.5s\tTrain: loss 1.0588\taccu 89.3908\tValid: loss 1.1209\taccu 87.5147\n",
      "Epoch 27: val_acc improved from 87.1905 to 87.5147, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "27 0.001\n",
      "Epoch-28   20 batches\tloss 1.0629 (1.0370)\taccu 92.188 (90.859)\n",
      "Epoch-28   40 batches\tloss 1.0009 (1.0365)\taccu 92.188 (90.586)\n",
      "Epoch-28   60 batches\tloss 0.9948 (1.0309)\taccu 93.750 (90.833)\n",
      "Epoch-28   80 batches\tloss 1.0587 (1.0322)\taccu 89.062 (90.625)\n",
      "Epoch-28  100 batches\tloss 1.0089 (1.0349)\taccu 92.188 (90.516)\n",
      "Epoch-28  120 batches\tloss 1.0828 (1.0352)\taccu 90.625 (90.521)\n",
      "Epoch-28  140 batches\tloss 1.0135 (1.0350)\taccu 92.188 (90.435)\n",
      "Epoch-28  160 batches\tloss 0.9600 (1.0332)\taccu 93.750 (90.566)\n",
      "Epoch-28  180 batches\tloss 1.0055 (1.0357)\taccu 90.625 (90.451)\n",
      "Epoch-28  200 batches\tloss 1.0432 (1.0366)\taccu 90.625 (90.422)\n",
      "Epoch-28  220 batches\tloss 1.0579 (1.0415)\taccu 90.625 (90.206)\n",
      "Epoch-28  240 batches\tloss 1.0934 (1.0437)\taccu 84.375 (90.026)\n",
      "Epoch-28  260 batches\tloss 1.0915 (1.0441)\taccu 87.500 (90.024)\n",
      "Epoch-28  280 batches\tloss 1.2432 (1.0441)\taccu 76.562 (89.967)\n",
      "Epoch-28  300 batches\tloss 1.1371 (1.0453)\taccu 87.500 (89.943)\n",
      "Epoch-28  320 batches\tloss 1.0831 (1.0489)\taccu 87.500 (89.731)\n",
      "Epoch-28  340 batches\tloss 0.9732 (1.0499)\taccu 89.062 (89.720)\n",
      "Epoch-28  360 batches\tloss 1.2019 (1.0517)\taccu 85.938 (89.674)\n",
      "Epoch-28  380 batches\tloss 1.3140 (1.0535)\taccu 78.125 (89.589)\n",
      "Epoch-28  400 batches\tloss 0.8834 (1.0531)\taccu 98.438 (89.629)\n",
      "Epoch-28  420 batches\tloss 1.1049 (1.0526)\taccu 87.500 (89.613)\n",
      "Epoch-28  440 batches\tloss 1.0921 (1.0536)\taccu 82.812 (89.581)\n",
      "Epoch-28  460 batches\tloss 1.0543 (1.0540)\taccu 90.625 (89.548)\n",
      "Epoch-28  480 batches\tloss 1.1239 (1.0556)\taccu 82.812 (89.479)\n",
      "Epoch-28  103.2s\tTrain: loss 1.0564\taccu 89.4224\tValid: loss 1.1311\taccu 87.2052\n",
      "Epoch 28: val_acc did not improve\n",
      "28 0.001\n",
      "Epoch-29   20 batches\tloss 1.1586 (1.0456)\taccu 81.250 (90.391)\n",
      "Epoch-29   40 batches\tloss 0.9273 (1.0434)\taccu 98.438 (90.703)\n",
      "Epoch-29   60 batches\tloss 0.9859 (1.0433)\taccu 90.625 (90.729)\n",
      "Epoch-29   80 batches\tloss 0.9966 (1.0486)\taccu 92.188 (90.273)\n",
      "Epoch-29  100 batches\tloss 1.0287 (1.0547)\taccu 90.625 (89.953)\n",
      "Epoch-29  120 batches\tloss 1.0683 (1.0579)\taccu 89.062 (89.766)\n",
      "Epoch-29  140 batches\tloss 0.9625 (1.0595)\taccu 96.875 (89.676)\n",
      "Epoch-29  160 batches\tloss 1.1560 (1.0623)\taccu 85.938 (89.531)\n",
      "Epoch-29  180 batches\tloss 1.0618 (1.0617)\taccu 85.938 (89.497)\n",
      "Epoch-29  200 batches\tloss 1.0984 (1.0594)\taccu 84.375 (89.469)\n",
      "Epoch-29  220 batches\tloss 1.1489 (1.0582)\taccu 82.812 (89.567)\n",
      "Epoch-29  240 batches\tloss 1.0353 (1.0570)\taccu 92.188 (89.603)\n",
      "Epoch-29  260 batches\tloss 1.0349 (1.0546)\taccu 90.625 (89.669)\n",
      "Epoch-29  280 batches\tloss 1.0589 (1.0536)\taccu 92.188 (89.671)\n",
      "Epoch-29  300 batches\tloss 1.0559 (1.0527)\taccu 90.625 (89.677)\n",
      "Epoch-29  320 batches\tloss 1.0803 (1.0543)\taccu 90.625 (89.604)\n",
      "Epoch-29  340 batches\tloss 1.0241 (1.0546)\taccu 90.625 (89.623)\n",
      "Epoch-29  360 batches\tloss 0.9566 (1.0546)\taccu 93.750 (89.596)\n",
      "Epoch-29  380 batches\tloss 1.0565 (1.0551)\taccu 87.500 (89.585)\n",
      "Epoch-29  400 batches\tloss 1.0815 (1.0566)\taccu 89.062 (89.500)\n",
      "Epoch-29  420 batches\tloss 1.0069 (1.0572)\taccu 92.188 (89.483)\n",
      "Epoch-29  440 batches\tloss 1.0100 (1.0572)\taccu 92.188 (89.457)\n",
      "Epoch-29  460 batches\tloss 1.0204 (1.0569)\taccu 93.750 (89.446)\n",
      "Epoch-29  480 batches\tloss 1.0882 (1.0576)\taccu 84.375 (89.375)\n",
      "Epoch-29  102.5s\tTrain: loss 1.0576\taccu 89.3687\tValid: loss 1.1422\taccu 86.5713\n",
      "Epoch 29: val_acc did not improve\n",
      "29 0.001\n",
      "Epoch-30   20 batches\tloss 1.0307 (1.0107)\taccu 90.625 (91.172)\n",
      "Epoch-30   40 batches\tloss 0.9747 (1.0071)\taccu 89.062 (91.445)\n",
      "Epoch-30   60 batches\tloss 0.9330 (1.0143)\taccu 93.750 (91.224)\n",
      "Epoch-30   80 batches\tloss 0.9222 (1.0113)\taccu 95.312 (91.270)\n",
      "Epoch-30  100 batches\tloss 0.9676 (1.0142)\taccu 93.750 (91.172)\n",
      "Epoch-30  120 batches\tloss 1.0508 (1.0137)\taccu 90.625 (91.302)\n",
      "Epoch-30  140 batches\tloss 0.9135 (1.0127)\taccu 96.875 (91.339)\n",
      "Epoch-30  160 batches\tloss 1.0622 (1.0151)\taccu 92.188 (91.318)\n",
      "Epoch-30  180 batches\tloss 1.0008 (1.0180)\taccu 90.625 (91.155)\n",
      "Epoch-30  200 batches\tloss 0.9059 (1.0216)\taccu 92.188 (91.070)\n",
      "Epoch-30  220 batches\tloss 0.9684 (1.0244)\taccu 92.188 (90.824)\n",
      "Epoch-30  240 batches\tloss 1.0549 (1.0235)\taccu 90.625 (90.866)\n",
      "Epoch-30  260 batches\tloss 1.1094 (1.0249)\taccu 89.062 (90.757)\n",
      "Epoch-30  280 batches\tloss 0.9923 (1.0257)\taccu 90.625 (90.787)\n",
      "Epoch-30  300 batches\tloss 1.0081 (1.0279)\taccu 89.062 (90.703)\n",
      "Epoch-30  320 batches\tloss 1.0315 (1.0310)\taccu 92.188 (90.586)\n",
      "Epoch-30  340 batches\tloss 1.1307 (1.0338)\taccu 87.500 (90.450)\n",
      "Epoch-30  360 batches\tloss 1.2595 (1.0361)\taccu 82.812 (90.312)\n",
      "Epoch-30  380 batches\tloss 1.0022 (1.0362)\taccu 93.750 (90.308)\n",
      "Epoch-30  400 batches\tloss 1.0406 (1.0370)\taccu 87.500 (90.262)\n",
      "Epoch-30  420 batches\tloss 1.0729 (1.0392)\taccu 84.375 (90.126)\n",
      "Epoch-30  440 batches\tloss 0.9336 (1.0415)\taccu 93.750 (90.014)\n",
      "Epoch-30  460 batches\tloss 1.1519 (1.0436)\taccu 85.938 (89.952)\n",
      "Epoch-30  480 batches\tloss 0.9946 (1.0460)\taccu 92.188 (89.834)\n",
      "Epoch-30  122.6s\tTrain: loss 1.0462\taccu 89.8390\tValid: loss 1.1280\taccu 86.9693\n",
      "Epoch 30: val_acc did not improve\n",
      "30 0.001\n",
      "Epoch-31   20 batches\tloss 0.9564 (1.0144)\taccu 95.312 (91.641)\n",
      "Epoch-31   40 batches\tloss 0.9846 (1.0244)\taccu 93.750 (90.820)\n",
      "Epoch-31   60 batches\tloss 1.0488 (1.0267)\taccu 90.625 (90.885)\n",
      "Epoch-31   80 batches\tloss 0.9584 (1.0222)\taccu 93.750 (90.918)\n",
      "Epoch-31  100 batches\tloss 0.9446 (1.0163)\taccu 95.312 (91.250)\n",
      "Epoch-31  120 batches\tloss 0.9911 (1.0121)\taccu 90.625 (91.471)\n",
      "Epoch-31  140 batches\tloss 1.0283 (1.0156)\taccu 90.625 (91.339)\n",
      "Epoch-31  160 batches\tloss 0.9611 (1.0171)\taccu 92.188 (91.172)\n",
      "Epoch-31  180 batches\tloss 1.0059 (1.0209)\taccu 92.188 (91.024)\n",
      "Epoch-31  200 batches\tloss 0.9612 (1.0212)\taccu 93.750 (91.008)\n",
      "Epoch-31  220 batches\tloss 0.9555 (1.0221)\taccu 92.188 (91.009)\n",
      "Epoch-31  240 batches\tloss 0.9079 (1.0250)\taccu 98.438 (90.938)\n",
      "Epoch-31  260 batches\tloss 1.1632 (1.0258)\taccu 78.125 (90.901)\n",
      "Epoch-31  280 batches\tloss 1.0639 (1.0287)\taccu 90.625 (90.781)\n",
      "Epoch-31  300 batches\tloss 1.0712 (1.0295)\taccu 89.062 (90.734)\n",
      "Epoch-31  320 batches\tloss 0.9881 (1.0304)\taccu 95.312 (90.688)\n",
      "Epoch-31  340 batches\tloss 1.0584 (1.0325)\taccu 89.062 (90.597)\n",
      "Epoch-31  360 batches\tloss 1.0317 (1.0345)\taccu 90.625 (90.503)\n",
      "Epoch-31  380 batches\tloss 1.0366 (1.0356)\taccu 95.312 (90.461)\n",
      "Epoch-31  400 batches\tloss 1.0267 (1.0359)\taccu 89.062 (90.445)\n",
      "Epoch-31  420 batches\tloss 1.0094 (1.0360)\taccu 92.188 (90.406)\n",
      "Epoch-31  440 batches\tloss 1.3401 (1.0375)\taccu 76.562 (90.334)\n",
      "Epoch-31  460 batches\tloss 0.9934 (1.0382)\taccu 89.062 (90.262)\n",
      "Epoch-31  480 batches\tloss 1.0646 (1.0389)\taccu 90.625 (90.231)\n",
      "Epoch-31  128.0s\tTrain: loss 1.0400\taccu 90.1673\tValid: loss 1.1347\taccu 86.6893\n",
      "Epoch 31: val_acc did not improve\n",
      "31 0.001\n",
      "Epoch-32   20 batches\tloss 0.9945 (1.0136)\taccu 92.188 (91.172)\n",
      "Epoch-32   40 batches\tloss 0.8610 (1.0089)\taccu 100.000 (91.250)\n",
      "Epoch-32   60 batches\tloss 0.9778 (1.0071)\taccu 93.750 (91.458)\n",
      "Epoch-32   80 batches\tloss 0.9568 (1.0109)\taccu 95.312 (91.113)\n",
      "Epoch-32  100 batches\tloss 0.9776 (1.0182)\taccu 93.750 (90.938)\n",
      "Epoch-32  120 batches\tloss 1.0933 (1.0241)\taccu 85.938 (90.664)\n",
      "Epoch-32  140 batches\tloss 1.0773 (1.0237)\taccu 85.938 (90.748)\n",
      "Epoch-32  160 batches\tloss 1.0182 (1.0237)\taccu 89.062 (90.723)\n",
      "Epoch-32  180 batches\tloss 1.1388 (1.0263)\taccu 89.062 (90.642)\n",
      "Epoch-32  200 batches\tloss 0.9651 (1.0302)\taccu 95.312 (90.438)\n",
      "Epoch-32  220 batches\tloss 1.1916 (1.0284)\taccu 82.812 (90.547)\n",
      "Epoch-32  240 batches\tloss 0.9867 (1.0281)\taccu 92.188 (90.573)\n",
      "Epoch-32  260 batches\tloss 1.2092 (1.0269)\taccu 76.562 (90.577)\n",
      "Epoch-32  280 batches\tloss 0.9095 (1.0263)\taccu 96.875 (90.619)\n",
      "Epoch-32  300 batches\tloss 1.1840 (1.0266)\taccu 84.375 (90.609)\n",
      "Epoch-32  320 batches\tloss 0.9112 (1.0292)\taccu 95.312 (90.493)\n",
      "Epoch-32  340 batches\tloss 0.9212 (1.0302)\taccu 93.750 (90.446)\n",
      "Epoch-32  360 batches\tloss 0.9565 (1.0329)\taccu 95.312 (90.295)\n",
      "Epoch-32  380 batches\tloss 1.0632 (1.0334)\taccu 85.938 (90.284)\n",
      "Epoch-32  400 batches\tloss 0.9069 (1.0352)\taccu 96.875 (90.227)\n",
      "Epoch-32  420 batches\tloss 0.9686 (1.0362)\taccu 95.312 (90.179)\n",
      "Epoch-32  440 batches\tloss 1.0352 (1.0365)\taccu 92.188 (90.174)\n",
      "Epoch-32  460 batches\tloss 1.1434 (1.0378)\taccu 87.500 (90.139)\n",
      "Epoch-32  480 batches\tloss 1.0822 (1.0397)\taccu 90.625 (90.075)\n",
      "Epoch-32  128.6s\tTrain: loss 1.0402\taccu 90.0442\tValid: loss 1.1458\taccu 85.8933\n",
      "Epoch 32: val_acc did not improve\n",
      "32 0.001\n",
      "Epoch-33   20 batches\tloss 1.0418 (1.0064)\taccu 87.500 (91.406)\n",
      "Epoch-33   40 batches\tloss 0.9407 (0.9927)\taccu 93.750 (92.031)\n",
      "Epoch-33   60 batches\tloss 0.9574 (0.9922)\taccu 92.188 (92.266)\n",
      "Epoch-33   80 batches\tloss 1.0362 (1.0029)\taccu 90.625 (91.758)\n",
      "Epoch-33  100 batches\tloss 1.1023 (1.0038)\taccu 85.938 (91.672)\n",
      "Epoch-33  120 batches\tloss 0.9885 (1.0033)\taccu 90.625 (91.680)\n",
      "Epoch-33  140 batches\tloss 0.9949 (1.0097)\taccu 92.188 (91.417)\n",
      "Epoch-33  160 batches\tloss 1.1367 (1.0181)\taccu 81.250 (91.016)\n",
      "Epoch-33  180 batches\tloss 0.9524 (1.0219)\taccu 93.750 (90.747)\n",
      "Epoch-33  200 batches\tloss 0.9858 (1.0241)\taccu 93.750 (90.734)\n",
      "Epoch-33  220 batches\tloss 0.9949 (1.0236)\taccu 89.062 (90.696)\n",
      "Epoch-33  240 batches\tloss 1.0826 (1.0217)\taccu 87.500 (90.794)\n",
      "Epoch-33  260 batches\tloss 0.9702 (1.0228)\taccu 90.625 (90.739)\n",
      "Epoch-33  280 batches\tloss 0.9228 (1.0240)\taccu 93.750 (90.742)\n",
      "Epoch-33  300 batches\tloss 0.9767 (1.0244)\taccu 92.188 (90.755)\n",
      "Epoch-33  320 batches\tloss 1.1645 (1.0256)\taccu 84.375 (90.664)\n",
      "Epoch-33  340 batches\tloss 1.0643 (1.0269)\taccu 92.188 (90.643)\n",
      "Epoch-33  360 batches\tloss 1.1279 (1.0270)\taccu 82.812 (90.625)\n",
      "Epoch-33  380 batches\tloss 1.0876 (1.0270)\taccu 87.500 (90.609)\n",
      "Epoch-33  400 batches\tloss 0.9558 (1.0280)\taccu 90.625 (90.535)\n",
      "Epoch-33  420 batches\tloss 1.1775 (1.0273)\taccu 85.938 (90.599)\n",
      "Epoch-33  440 batches\tloss 1.1708 (1.0279)\taccu 84.375 (90.554)\n",
      "Epoch-33  460 batches\tloss 1.0994 (1.0285)\taccu 85.938 (90.537)\n",
      "Epoch-33  480 batches\tloss 0.9286 (1.0292)\taccu 92.188 (90.498)\n",
      "Epoch-33  127.8s\tTrain: loss 1.0296\taccu 90.4861\tValid: loss 1.1278\taccu 87.1020\n",
      "Epoch 33: val_acc did not improve\n",
      "33 0.001\n",
      "Epoch-34   20 batches\tloss 0.9200 (1.0061)\taccu 93.750 (90.859)\n",
      "Epoch-34   40 batches\tloss 1.0219 (0.9973)\taccu 95.312 (91.641)\n",
      "Epoch-34   60 batches\tloss 0.9109 (0.9970)\taccu 95.312 (91.615)\n",
      "Epoch-34   80 batches\tloss 1.1697 (0.9965)\taccu 84.375 (91.875)\n",
      "Epoch-34  100 batches\tloss 1.0819 (0.9982)\taccu 89.062 (91.750)\n",
      "Epoch-34  120 batches\tloss 1.0083 (1.0026)\taccu 95.312 (91.380)\n",
      "Epoch-34  140 batches\tloss 1.1210 (1.0022)\taccu 90.625 (91.484)\n",
      "Epoch-34  160 batches\tloss 1.0608 (1.0081)\taccu 85.938 (91.182)\n",
      "Epoch-34  180 batches\tloss 0.9734 (1.0103)\taccu 96.875 (91.111)\n",
      "Epoch-34  200 batches\tloss 1.0157 (1.0125)\taccu 92.188 (91.047)\n",
      "Epoch-34  220 batches\tloss 1.0643 (1.0129)\taccu 90.625 (90.994)\n",
      "Epoch-34  240 batches\tloss 1.0194 (1.0149)\taccu 90.625 (90.898)\n",
      "Epoch-34  260 batches\tloss 1.2321 (1.0146)\taccu 81.250 (90.925)\n",
      "Epoch-34  280 batches\tloss 1.0239 (1.0146)\taccu 87.500 (90.887)\n",
      "Epoch-34  300 batches\tloss 1.1802 (1.0157)\taccu 87.500 (90.922)\n",
      "Epoch-34  320 batches\tloss 1.1578 (1.0170)\taccu 85.938 (90.874)\n",
      "Epoch-34  340 batches\tloss 1.1194 (1.0200)\taccu 87.500 (90.790)\n",
      "Epoch-34  360 batches\tloss 1.0854 (1.0210)\taccu 85.938 (90.760)\n",
      "Epoch-34  380 batches\tloss 1.0440 (1.0217)\taccu 90.625 (90.699)\n",
      "Epoch-34  400 batches\tloss 1.2295 (1.0237)\taccu 82.812 (90.656)\n",
      "Epoch-34  420 batches\tloss 0.9977 (1.0243)\taccu 90.625 (90.621)\n",
      "Epoch-34  440 batches\tloss 1.0995 (1.0247)\taccu 85.938 (90.611)\n",
      "Epoch-34  460 batches\tloss 1.1470 (1.0268)\taccu 87.500 (90.520)\n",
      "Epoch-34  480 batches\tloss 1.0688 (1.0264)\taccu 87.500 (90.540)\n",
      "Epoch-34  127.6s\tTrain: loss 1.0276\taccu 90.4987\tValid: loss 1.1193\taccu 87.5147\n",
      "Epoch 34: val_acc did not improve\n",
      "34 0.001\n",
      "Epoch-35   20 batches\tloss 1.0315 (1.0209)\taccu 89.062 (90.703)\n",
      "Epoch-35   40 batches\tloss 0.9415 (1.0166)\taccu 95.312 (90.898)\n",
      "Epoch-35   60 batches\tloss 0.9315 (1.0122)\taccu 90.625 (91.172)\n",
      "Epoch-35   80 batches\tloss 0.9227 (1.0180)\taccu 95.312 (90.977)\n",
      "Epoch-35  100 batches\tloss 1.0487 (1.0248)\taccu 90.625 (90.766)\n",
      "Epoch-35  120 batches\tloss 1.0758 (1.0261)\taccu 89.062 (90.599)\n",
      "Epoch-35  140 batches\tloss 0.9818 (1.0273)\taccu 95.312 (90.458)\n",
      "Epoch-35  160 batches\tloss 1.0252 (1.0290)\taccu 89.062 (90.420)\n",
      "Epoch-35  180 batches\tloss 0.9428 (1.0260)\taccu 93.750 (90.582)\n",
      "Epoch-35  200 batches\tloss 1.0208 (1.0251)\taccu 90.625 (90.570)\n",
      "Epoch-35  220 batches\tloss 1.0496 (1.0253)\taccu 90.625 (90.533)\n",
      "Epoch-35  240 batches\tloss 0.8992 (1.0222)\taccu 98.438 (90.684)\n",
      "Epoch-35  260 batches\tloss 1.0589 (1.0225)\taccu 90.625 (90.673)\n",
      "Epoch-35  280 batches\tloss 1.0307 (1.0233)\taccu 90.625 (90.614)\n",
      "Epoch-35  300 batches\tloss 0.9290 (1.0237)\taccu 95.312 (90.583)\n",
      "Epoch-35  320 batches\tloss 0.9572 (1.0243)\taccu 93.750 (90.566)\n",
      "Epoch-35  340 batches\tloss 1.0090 (1.0234)\taccu 90.625 (90.588)\n",
      "Epoch-35  360 batches\tloss 1.0247 (1.0235)\taccu 90.625 (90.560)\n",
      "Epoch-35  380 batches\tloss 0.9315 (1.0235)\taccu 92.188 (90.576)\n",
      "Epoch-35  400 batches\tloss 0.9259 (1.0231)\taccu 93.750 (90.570)\n",
      "Epoch-35  420 batches\tloss 0.9221 (1.0244)\taccu 93.750 (90.491)\n",
      "Epoch-35  440 batches\tloss 1.0930 (1.0265)\taccu 85.938 (90.387)\n",
      "Epoch-35  460 batches\tloss 0.9379 (1.0277)\taccu 95.312 (90.370)\n",
      "Epoch-35  480 batches\tloss 1.1093 (1.0293)\taccu 84.375 (90.283)\n",
      "Epoch-35  127.3s\tTrain: loss 1.0288\taccu 90.2873\tValid: loss 1.1336\taccu 86.4829\n",
      "Epoch 35: val_acc did not improve\n",
      "35 0.001\n",
      "Epoch-36   20 batches\tloss 1.0266 (0.9960)\taccu 92.188 (91.797)\n",
      "Epoch-36   40 batches\tloss 0.9419 (1.0159)\taccu 95.312 (90.781)\n",
      "Epoch-36   60 batches\tloss 1.1138 (1.0179)\taccu 87.500 (90.885)\n",
      "Epoch-36   80 batches\tloss 0.9353 (1.0136)\taccu 92.188 (91.055)\n",
      "Epoch-36  100 batches\tloss 0.9282 (1.0089)\taccu 95.312 (91.469)\n",
      "Epoch-36  120 batches\tloss 0.9689 (1.0078)\taccu 95.312 (91.523)\n",
      "Epoch-36  140 batches\tloss 0.9472 (1.0042)\taccu 96.875 (91.663)\n",
      "Epoch-36  160 batches\tloss 1.0241 (1.0062)\taccu 87.500 (91.602)\n",
      "Epoch-36  180 batches\tloss 1.1260 (1.0071)\taccu 84.375 (91.562)\n",
      "Epoch-36  200 batches\tloss 0.9244 (1.0041)\taccu 96.875 (91.625)\n",
      "Epoch-36  220 batches\tloss 0.9975 (1.0040)\taccu 90.625 (91.570)\n",
      "Epoch-36  240 batches\tloss 1.0651 (1.0047)\taccu 85.938 (91.569)\n",
      "Epoch-36  260 batches\tloss 1.1175 (1.0060)\taccu 90.625 (91.472)\n",
      "Epoch-36  280 batches\tloss 0.9411 (1.0055)\taccu 96.875 (91.484)\n",
      "Epoch-36  300 batches\tloss 1.0175 (1.0063)\taccu 90.625 (91.427)\n",
      "Epoch-36  320 batches\tloss 1.0402 (1.0096)\taccu 92.188 (91.279)\n",
      "Epoch-36  340 batches\tloss 0.9075 (1.0106)\taccu 96.875 (91.287)\n",
      "Epoch-36  360 batches\tloss 1.0540 (1.0104)\taccu 92.188 (91.311)\n",
      "Epoch-36  380 batches\tloss 1.1526 (1.0112)\taccu 79.688 (91.238)\n",
      "Epoch-36  400 batches\tloss 1.0813 (1.0120)\taccu 87.500 (91.195)\n",
      "Epoch-36  420 batches\tloss 0.9534 (1.0138)\taccu 96.875 (91.124)\n",
      "Epoch-36  440 batches\tloss 1.0569 (1.0138)\taccu 89.062 (91.147)\n",
      "Epoch-36  460 batches\tloss 0.9764 (1.0145)\taccu 93.750 (91.124)\n",
      "Epoch-36  480 batches\tloss 0.9841 (1.0149)\taccu 93.750 (91.100)\n",
      "Epoch-36  127.3s\tTrain: loss 1.0147\taccu 91.0922\tValid: loss 1.1090\taccu 87.7211\n",
      "Epoch 36: val_acc improved from 87.5147 to 87.7211, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "36 0.001\n",
      "Epoch-37   20 batches\tloss 0.9301 (0.9748)\taccu 95.312 (92.891)\n",
      "Epoch-37   40 batches\tloss 0.9992 (0.9854)\taccu 93.750 (91.992)\n",
      "Epoch-37   60 batches\tloss 0.8479 (0.9892)\taccu 98.438 (91.693)\n",
      "Epoch-37   80 batches\tloss 0.9958 (0.9903)\taccu 93.750 (91.836)\n",
      "Epoch-37  100 batches\tloss 0.8819 (0.9877)\taccu 96.875 (92.125)\n",
      "Epoch-37  120 batches\tloss 1.0296 (0.9935)\taccu 90.625 (91.953)\n",
      "Epoch-37  140 batches\tloss 1.0428 (0.9942)\taccu 87.500 (91.819)\n",
      "Epoch-37  160 batches\tloss 0.9428 (0.9963)\taccu 92.188 (91.680)\n",
      "Epoch-37  180 batches\tloss 0.9097 (0.9967)\taccu 98.438 (91.710)\n",
      "Epoch-37  200 batches\tloss 1.1745 (0.9980)\taccu 85.938 (91.766)\n",
      "Epoch-37  220 batches\tloss 1.0304 (0.9991)\taccu 87.500 (91.690)\n",
      "Epoch-37  240 batches\tloss 1.1415 (1.0003)\taccu 84.375 (91.634)\n",
      "Epoch-37  260 batches\tloss 1.0112 (1.0023)\taccu 89.062 (91.532)\n",
      "Epoch-37  280 batches\tloss 1.0060 (1.0011)\taccu 90.625 (91.590)\n",
      "Epoch-37  300 batches\tloss 1.1016 (1.0019)\taccu 92.188 (91.557)\n",
      "Epoch-37  320 batches\tloss 0.9921 (1.0026)\taccu 90.625 (91.504)\n",
      "Epoch-37  340 batches\tloss 1.0069 (1.0044)\taccu 89.062 (91.429)\n",
      "Epoch-37  360 batches\tloss 1.1785 (1.0078)\taccu 82.812 (91.254)\n",
      "Epoch-37  380 batches\tloss 0.9269 (1.0093)\taccu 93.750 (91.176)\n",
      "Epoch-37  400 batches\tloss 1.0238 (1.0097)\taccu 87.500 (91.160)\n",
      "Epoch-37  420 batches\tloss 1.0107 (1.0105)\taccu 93.750 (91.090)\n",
      "Epoch-37  440 batches\tloss 0.9709 (1.0123)\taccu 93.750 (91.037)\n",
      "Epoch-37  460 batches\tloss 1.0515 (1.0144)\taccu 87.500 (90.941)\n",
      "Epoch-37  480 batches\tloss 0.9063 (1.0153)\taccu 95.312 (90.866)\n",
      "Epoch-37  127.4s\tTrain: loss 1.0162\taccu 90.8270\tValid: loss 1.1241\taccu 87.2052\n",
      "Epoch 37: val_acc did not improve\n",
      "37 0.001\n",
      "Epoch-38   20 batches\tloss 1.0429 (0.9964)\taccu 87.500 (91.875)\n",
      "Epoch-38   40 batches\tloss 0.9679 (0.9918)\taccu 92.188 (91.992)\n",
      "Epoch-38   60 batches\tloss 1.0474 (0.9903)\taccu 87.500 (92.240)\n",
      "Epoch-38   80 batches\tloss 1.1137 (0.9869)\taccu 89.062 (92.207)\n",
      "Epoch-38  100 batches\tloss 0.8885 (0.9844)\taccu 96.875 (92.438)\n",
      "Epoch-38  120 batches\tloss 1.0498 (0.9871)\taccu 90.625 (92.214)\n",
      "Epoch-38  140 batches\tloss 1.1113 (0.9905)\taccu 84.375 (92.121)\n",
      "Epoch-38  160 batches\tloss 0.9606 (0.9928)\taccu 93.750 (92.041)\n",
      "Epoch-38  180 batches\tloss 0.9731 (0.9932)\taccu 92.188 (91.962)\n",
      "Epoch-38  200 batches\tloss 0.9268 (0.9951)\taccu 93.750 (91.797)\n",
      "Epoch-38  220 batches\tloss 1.2805 (0.9964)\taccu 76.562 (91.832)\n",
      "Epoch-38  240 batches\tloss 1.0817 (0.9990)\taccu 87.500 (91.758)\n",
      "Epoch-38  260 batches\tloss 1.0701 (1.0014)\taccu 85.938 (91.635)\n",
      "Epoch-38  280 batches\tloss 0.9328 (1.0037)\taccu 95.312 (91.596)\n",
      "Epoch-38  300 batches\tloss 1.0623 (1.0046)\taccu 89.062 (91.583)\n",
      "Epoch-38  320 batches\tloss 0.9749 (1.0060)\taccu 95.312 (91.514)\n",
      "Epoch-38  340 batches\tloss 0.9329 (1.0069)\taccu 95.312 (91.484)\n",
      "Epoch-38  360 batches\tloss 0.9911 (1.0092)\taccu 89.062 (91.341)\n",
      "Epoch-38  380 batches\tloss 1.1183 (1.0104)\taccu 87.500 (91.291)\n",
      "Epoch-38  400 batches\tloss 1.2061 (1.0114)\taccu 84.375 (91.246)\n",
      "Epoch-38  420 batches\tloss 1.0204 (1.0112)\taccu 90.625 (91.287)\n",
      "Epoch-38  440 batches\tloss 1.0593 (1.0133)\taccu 87.500 (91.190)\n",
      "Epoch-38  460 batches\tloss 1.0936 (1.0134)\taccu 87.500 (91.168)\n",
      "Epoch-38  480 batches\tloss 0.9633 (1.0146)\taccu 95.312 (91.136)\n",
      "Epoch-38  93.2s\tTrain: loss 1.0155\taccu 91.0827\tValid: loss 1.1330\taccu 87.0873\n",
      "Epoch 38: val_acc did not improve\n",
      "38 0.001\n",
      "Epoch-39   20 batches\tloss 0.9098 (1.0007)\taccu 96.875 (92.109)\n",
      "Epoch-39   40 batches\tloss 0.8878 (0.9950)\taccu 95.312 (92.344)\n",
      "Epoch-39   60 batches\tloss 0.9932 (0.9902)\taccu 90.625 (92.448)\n",
      "Epoch-39   80 batches\tloss 1.0067 (0.9905)\taccu 90.625 (92.285)\n",
      "Epoch-39  100 batches\tloss 1.0292 (0.9911)\taccu 89.062 (92.188)\n",
      "Epoch-39  120 batches\tloss 0.9062 (0.9848)\taccu 95.312 (92.513)\n",
      "Epoch-39  140 batches\tloss 0.9749 (0.9797)\taccu 95.312 (92.634)\n",
      "Epoch-39  160 batches\tloss 1.1380 (0.9803)\taccu 87.500 (92.520)\n",
      "Epoch-39  180 batches\tloss 1.1061 (0.9832)\taccu 87.500 (92.448)\n",
      "Epoch-39  200 batches\tloss 1.0212 (0.9859)\taccu 92.188 (92.328)\n",
      "Epoch-39  220 batches\tloss 1.0263 (0.9906)\taccu 95.312 (92.173)\n",
      "Epoch-39  240 batches\tloss 1.0558 (0.9912)\taccu 89.062 (92.064)\n",
      "Epoch-39  260 batches\tloss 1.0043 (0.9940)\taccu 92.188 (91.911)\n",
      "Epoch-39  280 batches\tloss 0.9271 (0.9968)\taccu 96.875 (91.797)\n",
      "Epoch-39  300 batches\tloss 0.9694 (0.9984)\taccu 98.438 (91.781)\n",
      "Epoch-39  320 batches\tloss 0.9991 (0.9990)\taccu 93.750 (91.768)\n",
      "Epoch-39  340 batches\tloss 1.0778 (1.0008)\taccu 85.938 (91.687)\n",
      "Epoch-39  360 batches\tloss 1.0147 (1.0016)\taccu 93.750 (91.619)\n",
      "Epoch-39  380 batches\tloss 0.9241 (1.0022)\taccu 95.312 (91.595)\n",
      "Epoch-39  400 batches\tloss 1.0737 (1.0042)\taccu 90.625 (91.488)\n",
      "Epoch-39  420 batches\tloss 1.1143 (1.0060)\taccu 84.375 (91.354)\n",
      "Epoch-39  440 batches\tloss 1.0096 (1.0066)\taccu 92.188 (91.328)\n",
      "Epoch-39  460 batches\tloss 0.9492 (1.0067)\taccu 93.750 (91.332)\n",
      "Epoch-39  480 batches\tloss 1.0018 (1.0067)\taccu 89.062 (91.309)\n",
      "Epoch-39  92.8s\tTrain: loss 1.0078\taccu 91.2468\tValid: loss 1.1021\taccu 88.0749\n",
      "Epoch 39: val_acc improved from 87.7211 to 88.0749, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "39 0.001\n",
      "Epoch-40   20 batches\tloss 0.9306 (0.9678)\taccu 93.750 (92.734)\n",
      "Epoch-40   40 batches\tloss 1.0126 (0.9794)\taccu 92.188 (92.188)\n",
      "Epoch-40   60 batches\tloss 1.0701 (0.9876)\taccu 90.625 (92.083)\n",
      "Epoch-40   80 batches\tloss 0.9842 (0.9890)\taccu 90.625 (91.875)\n",
      "Epoch-40  100 batches\tloss 1.0548 (0.9908)\taccu 93.750 (91.797)\n",
      "Epoch-40  120 batches\tloss 1.0713 (0.9909)\taccu 89.062 (91.823)\n",
      "Epoch-40  140 batches\tloss 1.0879 (0.9933)\taccu 89.062 (91.674)\n",
      "Epoch-40  160 batches\tloss 1.0577 (0.9970)\taccu 89.062 (91.582)\n",
      "Epoch-40  180 batches\tloss 0.9754 (0.9987)\taccu 92.188 (91.502)\n",
      "Epoch-40  200 batches\tloss 0.9693 (0.9971)\taccu 93.750 (91.602)\n",
      "Epoch-40  220 batches\tloss 1.1844 (1.0006)\taccu 79.688 (91.435)\n",
      "Epoch-40  240 batches\tloss 0.9850 (1.0008)\taccu 92.188 (91.491)\n",
      "Epoch-40  260 batches\tloss 0.9818 (1.0012)\taccu 95.312 (91.466)\n",
      "Epoch-40  280 batches\tloss 1.0350 (1.0021)\taccu 92.188 (91.417)\n",
      "Epoch-40  300 batches\tloss 1.0616 (1.0029)\taccu 90.625 (91.354)\n",
      "Epoch-40  320 batches\tloss 0.9767 (1.0025)\taccu 90.625 (91.372)\n",
      "Epoch-40  340 batches\tloss 0.9886 (1.0038)\taccu 92.188 (91.333)\n",
      "Epoch-40  360 batches\tloss 1.2567 (1.0046)\taccu 73.438 (91.276)\n",
      "Epoch-40  380 batches\tloss 0.9277 (1.0029)\taccu 95.312 (91.340)\n",
      "Epoch-40  400 batches\tloss 0.9298 (1.0030)\taccu 96.875 (91.332)\n",
      "Epoch-40  420 batches\tloss 1.0601 (1.0048)\taccu 93.750 (91.295)\n",
      "Epoch-40  440 batches\tloss 1.0290 (1.0080)\taccu 90.625 (91.207)\n",
      "Epoch-40  460 batches\tloss 1.1068 (1.0085)\taccu 87.500 (91.196)\n",
      "Epoch-40  480 batches\tloss 1.0104 (1.0095)\taccu 90.625 (91.143)\n",
      "Epoch-40  116.8s\tTrain: loss 1.0115\taccu 91.0417\tValid: loss 1.1190\taccu 87.1905\n",
      "Epoch 40: val_acc did not improve\n",
      "40 0.001\n",
      "Epoch-41   20 batches\tloss 1.0402 (0.9987)\taccu 89.062 (91.641)\n",
      "Epoch-41   40 batches\tloss 0.9302 (0.9914)\taccu 95.312 (91.914)\n",
      "Epoch-41   60 batches\tloss 0.8832 (0.9877)\taccu 96.875 (92.266)\n",
      "Epoch-41   80 batches\tloss 0.9956 (0.9879)\taccu 95.312 (92.246)\n",
      "Epoch-41  100 batches\tloss 0.9802 (0.9882)\taccu 92.188 (92.141)\n",
      "Epoch-41  120 batches\tloss 1.0000 (0.9906)\taccu 95.312 (92.122)\n",
      "Epoch-41  140 batches\tloss 0.9689 (0.9884)\taccu 93.750 (92.299)\n",
      "Epoch-41  160 batches\tloss 1.0057 (0.9876)\taccu 93.750 (92.295)\n",
      "Epoch-41  180 batches\tloss 0.9464 (0.9868)\taccu 93.750 (92.326)\n",
      "Epoch-41  200 batches\tloss 0.9462 (0.9865)\taccu 92.188 (92.289)\n",
      "Epoch-41  220 batches\tloss 0.9515 (0.9870)\taccu 95.312 (92.266)\n",
      "Epoch-41  240 batches\tloss 0.9624 (0.9884)\taccu 92.188 (92.240)\n",
      "Epoch-41  260 batches\tloss 1.0336 (0.9888)\taccu 87.500 (92.236)\n",
      "Epoch-41  280 batches\tloss 0.9052 (0.9881)\taccu 96.875 (92.266)\n",
      "Epoch-41  300 batches\tloss 0.9614 (0.9883)\taccu 90.625 (92.245)\n",
      "Epoch-41  320 batches\tloss 0.9929 (0.9898)\taccu 93.750 (92.188)\n",
      "Epoch-41  340 batches\tloss 0.9400 (0.9893)\taccu 93.750 (92.233)\n",
      "Epoch-41  360 batches\tloss 0.9602 (0.9907)\taccu 93.750 (92.166)\n",
      "Epoch-41  380 batches\tloss 0.9580 (0.9917)\taccu 95.312 (92.130)\n",
      "Epoch-41  400 batches\tloss 0.9677 (0.9928)\taccu 93.750 (92.102)\n",
      "Epoch-41  420 batches\tloss 1.1372 (0.9955)\taccu 81.250 (91.942)\n",
      "Epoch-41  440 batches\tloss 1.0788 (0.9967)\taccu 84.375 (91.864)\n",
      "Epoch-41  460 batches\tloss 1.0537 (0.9966)\taccu 84.375 (91.858)\n",
      "Epoch-41  480 batches\tloss 1.1467 (0.9983)\taccu 85.938 (91.761)\n",
      "Epoch-41  127.0s\tTrain: loss 0.9980\taccu 91.7551\tValid: loss 1.1105\taccu 87.9864\n",
      "Epoch 41: val_acc did not improve\n",
      "41 0.001\n",
      "Epoch-42   20 batches\tloss 0.9170 (0.9573)\taccu 95.312 (93.906)\n",
      "Epoch-42   40 batches\tloss 0.9271 (0.9738)\taccu 93.750 (93.086)\n",
      "Epoch-42   60 batches\tloss 0.9221 (0.9771)\taccu 95.312 (92.760)\n",
      "Epoch-42   80 batches\tloss 0.9415 (0.9773)\taccu 92.188 (92.734)\n",
      "Epoch-42  100 batches\tloss 0.9946 (0.9820)\taccu 93.750 (92.484)\n",
      "Epoch-42  120 batches\tloss 1.0255 (0.9813)\taccu 90.625 (92.669)\n",
      "Epoch-42  140 batches\tloss 0.9511 (0.9836)\taccu 89.062 (92.522)\n",
      "Epoch-42  160 batches\tloss 0.8998 (0.9858)\taccu 98.438 (92.412)\n",
      "Epoch-42  180 batches\tloss 0.9336 (0.9872)\taccu 93.750 (92.283)\n",
      "Epoch-42  200 batches\tloss 0.9152 (0.9868)\taccu 95.312 (92.188)\n",
      "Epoch-42  220 batches\tloss 1.0470 (0.9876)\taccu 85.938 (92.131)\n",
      "Epoch-42  240 batches\tloss 0.8717 (0.9868)\taccu 98.438 (92.155)\n",
      "Epoch-42  260 batches\tloss 1.0385 (0.9892)\taccu 90.625 (92.079)\n",
      "Epoch-42  280 batches\tloss 1.0842 (0.9896)\taccu 89.062 (92.020)\n",
      "Epoch-42  300 batches\tloss 0.9538 (0.9904)\taccu 93.750 (91.974)\n",
      "Epoch-42  320 batches\tloss 1.1475 (0.9907)\taccu 85.938 (91.982)\n",
      "Epoch-42  340 batches\tloss 0.9519 (0.9933)\taccu 95.312 (91.857)\n",
      "Epoch-42  360 batches\tloss 0.9253 (0.9936)\taccu 93.750 (91.849)\n",
      "Epoch-42  380 batches\tloss 1.0984 (0.9956)\taccu 82.812 (91.756)\n",
      "Epoch-42  400 batches\tloss 1.0593 (0.9972)\taccu 89.062 (91.684)\n",
      "Epoch-42  420 batches\tloss 1.2404 (0.9987)\taccu 79.688 (91.633)\n",
      "Epoch-42  440 batches\tloss 1.0421 (1.0013)\taccu 87.500 (91.534)\n",
      "Epoch-42  460 batches\tloss 1.0072 (1.0028)\taccu 89.062 (91.450)\n",
      "Epoch-42  480 batches\tloss 1.1062 (1.0044)\taccu 87.500 (91.370)\n",
      "Epoch-42  126.6s\tTrain: loss 1.0048\taccu 91.3794\tValid: loss 1.1270\taccu 86.5713\n",
      "Epoch 42: val_acc did not improve\n",
      "42 0.001\n",
      "Epoch-43   20 batches\tloss 0.9688 (1.0098)\taccu 93.750 (91.328)\n",
      "Epoch-43   40 batches\tloss 1.1307 (0.9902)\taccu 89.062 (92.383)\n",
      "Epoch-43   60 batches\tloss 0.9412 (0.9814)\taccu 90.625 (92.604)\n",
      "Epoch-43   80 batches\tloss 1.1243 (0.9777)\taccu 87.500 (92.695)\n",
      "Epoch-43  100 batches\tloss 1.0722 (0.9776)\taccu 90.625 (92.672)\n",
      "Epoch-43  120 batches\tloss 1.0102 (0.9777)\taccu 89.062 (92.669)\n",
      "Epoch-43  140 batches\tloss 1.0266 (0.9792)\taccu 87.500 (92.556)\n",
      "Epoch-43  160 batches\tloss 0.9021 (0.9799)\taccu 98.438 (92.529)\n",
      "Epoch-43  180 batches\tloss 1.0330 (0.9813)\taccu 85.938 (92.500)\n",
      "Epoch-43  200 batches\tloss 0.8966 (0.9825)\taccu 95.312 (92.422)\n",
      "Epoch-43  220 batches\tloss 1.0879 (0.9855)\taccu 87.500 (92.294)\n",
      "Epoch-43  240 batches\tloss 0.8803 (0.9868)\taccu 95.312 (92.168)\n",
      "Epoch-43  260 batches\tloss 1.0613 (0.9888)\taccu 87.500 (92.079)\n",
      "Epoch-43  280 batches\tloss 1.0110 (0.9893)\taccu 89.062 (92.037)\n",
      "Epoch-43  300 batches\tloss 1.0753 (0.9907)\taccu 89.062 (91.974)\n",
      "Epoch-43  320 batches\tloss 1.0939 (0.9930)\taccu 85.938 (91.938)\n",
      "Epoch-43  340 batches\tloss 1.1578 (0.9926)\taccu 84.375 (91.962)\n",
      "Epoch-43  360 batches\tloss 1.2455 (0.9946)\taccu 78.125 (91.875)\n",
      "Epoch-43  380 batches\tloss 1.1559 (0.9967)\taccu 85.938 (91.780)\n",
      "Epoch-43  400 batches\tloss 0.9781 (0.9978)\taccu 93.750 (91.703)\n",
      "Epoch-43  420 batches\tloss 1.1277 (0.9985)\taccu 84.375 (91.711)\n",
      "Epoch-43  440 batches\tloss 1.1734 (0.9990)\taccu 85.938 (91.673)\n",
      "Epoch-43  460 batches\tloss 1.1630 (0.9991)\taccu 85.938 (91.647)\n",
      "Epoch-43  480 batches\tloss 0.9471 (0.9993)\taccu 95.312 (91.618)\n",
      "Epoch-43  127.0s\tTrain: loss 0.9988\taccu 91.6477\tValid: loss 1.1013\taccu 87.8390\n",
      "Epoch 43: val_acc did not improve\n",
      "43 0.001\n",
      "Epoch-44   20 batches\tloss 0.9228 (0.9823)\taccu 95.312 (92.188)\n",
      "Epoch-44   40 batches\tloss 1.0094 (0.9720)\taccu 93.750 (92.422)\n",
      "Epoch-44   60 batches\tloss 1.0272 (0.9748)\taccu 90.625 (92.578)\n",
      "Epoch-44   80 batches\tloss 0.9282 (0.9831)\taccu 93.750 (92.227)\n",
      "Epoch-44  100 batches\tloss 0.9390 (0.9805)\taccu 95.312 (92.469)\n",
      "Epoch-44  120 batches\tloss 0.9833 (0.9760)\taccu 93.750 (92.604)\n",
      "Epoch-44  140 batches\tloss 1.0331 (0.9754)\taccu 85.938 (92.645)\n",
      "Epoch-44  160 batches\tloss 0.9571 (0.9766)\taccu 92.188 (92.549)\n",
      "Epoch-44  180 batches\tloss 0.9422 (0.9785)\taccu 95.312 (92.483)\n",
      "Epoch-44  200 batches\tloss 0.9326 (0.9794)\taccu 93.750 (92.469)\n",
      "Epoch-44  220 batches\tloss 0.9483 (0.9789)\taccu 95.312 (92.450)\n",
      "Epoch-44  240 batches\tloss 0.9913 (0.9814)\taccu 90.625 (92.344)\n",
      "Epoch-44  260 batches\tloss 0.9711 (0.9827)\taccu 90.625 (92.218)\n",
      "Epoch-44  280 batches\tloss 0.9948 (0.9854)\taccu 93.750 (92.098)\n",
      "Epoch-44  300 batches\tloss 0.8740 (0.9857)\taccu 100.000 (92.068)\n",
      "Epoch-44  320 batches\tloss 1.0280 (0.9878)\taccu 87.500 (91.958)\n",
      "Epoch-44  340 batches\tloss 0.9787 (0.9905)\taccu 93.750 (91.875)\n",
      "Epoch-44  360 batches\tloss 1.0194 (0.9902)\taccu 89.062 (91.897)\n",
      "Epoch-44  380 batches\tloss 1.0180 (0.9922)\taccu 93.750 (91.826)\n",
      "Epoch-44  400 batches\tloss 0.9615 (0.9931)\taccu 92.188 (91.797)\n",
      "Epoch-44  420 batches\tloss 0.9908 (0.9928)\taccu 90.625 (91.786)\n",
      "Epoch-44  440 batches\tloss 0.9934 (0.9945)\taccu 90.625 (91.697)\n",
      "Epoch-44  460 batches\tloss 1.0192 (0.9967)\taccu 92.188 (91.610)\n",
      "Epoch-44  480 batches\tloss 0.9349 (0.9984)\taccu 96.875 (91.559)\n",
      "Epoch-44  127.1s\tTrain: loss 0.9979\taccu 91.5751\tValid: loss 1.1130\taccu 87.5884\n",
      "Epoch 44: val_acc did not improve\n",
      "44 0.001\n",
      "Epoch-45   20 batches\tloss 0.8856 (0.9631)\taccu 96.875 (93.750)\n",
      "Epoch-45   40 batches\tloss 0.8781 (0.9768)\taccu 96.875 (92.461)\n",
      "Epoch-45   60 batches\tloss 0.9878 (0.9843)\taccu 96.875 (92.344)\n",
      "Epoch-45   80 batches\tloss 0.9278 (0.9838)\taccu 93.750 (92.539)\n",
      "Epoch-45  100 batches\tloss 0.9406 (0.9897)\taccu 95.312 (92.297)\n",
      "Epoch-45  120 batches\tloss 0.9591 (0.9914)\taccu 95.312 (92.227)\n",
      "Epoch-45  140 batches\tloss 0.9051 (0.9913)\taccu 96.875 (92.143)\n",
      "Epoch-45  160 batches\tloss 0.9334 (0.9926)\taccu 95.312 (92.031)\n",
      "Epoch-45  180 batches\tloss 1.1782 (0.9946)\taccu 85.938 (91.892)\n",
      "Epoch-45  200 batches\tloss 0.9956 (0.9967)\taccu 93.750 (91.781)\n",
      "Epoch-45  220 batches\tloss 0.9762 (0.9955)\taccu 93.750 (91.854)\n",
      "Epoch-45  240 batches\tloss 0.9857 (0.9950)\taccu 95.312 (91.895)\n",
      "Epoch-45  260 batches\tloss 1.0359 (0.9962)\taccu 92.188 (91.851)\n",
      "Epoch-45  280 batches\tloss 1.0355 (0.9980)\taccu 89.062 (91.786)\n",
      "Epoch-45  300 batches\tloss 0.9089 (0.9975)\taccu 95.312 (91.771)\n",
      "Epoch-45  320 batches\tloss 1.0721 (0.9975)\taccu 92.188 (91.792)\n",
      "Epoch-45  340 batches\tloss 1.0333 (0.9973)\taccu 92.188 (91.820)\n",
      "Epoch-45  360 batches\tloss 0.9483 (0.9974)\taccu 93.750 (91.832)\n",
      "Epoch-45  380 batches\tloss 1.0654 (0.9970)\taccu 85.938 (91.850)\n",
      "Epoch-45  400 batches\tloss 1.2295 (0.9986)\taccu 84.375 (91.809)\n",
      "Epoch-45  420 batches\tloss 0.9321 (0.9994)\taccu 96.875 (91.778)\n",
      "Epoch-45  440 batches\tloss 0.9956 (0.9985)\taccu 92.188 (91.800)\n",
      "Epoch-45  460 batches\tloss 0.9579 (0.9986)\taccu 95.312 (91.770)\n",
      "Epoch-45  480 batches\tloss 1.1434 (0.9983)\taccu 85.938 (91.781)\n",
      "Epoch-45  127.6s\tTrain: loss 0.9991\taccu 91.7898\tValid: loss 1.1025\taccu 87.6474\n",
      "Epoch 45: val_acc did not improve\n",
      "45 0.001\n",
      "Epoch-46   20 batches\tloss 1.0521 (0.9850)\taccu 89.062 (92.109)\n",
      "Epoch-46   40 batches\tloss 0.9709 (0.9693)\taccu 90.625 (92.891)\n",
      "Epoch-46   60 batches\tloss 1.1508 (0.9735)\taccu 78.125 (92.578)\n",
      "Epoch-46   80 batches\tloss 0.9425 (0.9712)\taccu 96.875 (92.754)\n",
      "Epoch-46  100 batches\tloss 0.9108 (0.9700)\taccu 93.750 (92.875)\n",
      "Epoch-46  120 batches\tloss 0.8763 (0.9698)\taccu 98.438 (92.865)\n",
      "Epoch-46  140 batches\tloss 1.0493 (0.9675)\taccu 89.062 (93.025)\n",
      "Epoch-46  160 batches\tloss 0.8807 (0.9675)\taccu 96.875 (92.920)\n",
      "Epoch-46  180 batches\tloss 0.9281 (0.9677)\taccu 96.875 (92.882)\n",
      "Epoch-46  200 batches\tloss 0.9592 (0.9682)\taccu 92.188 (92.844)\n",
      "Epoch-46  220 batches\tloss 0.9925 (0.9717)\taccu 92.188 (92.749)\n",
      "Epoch-46  240 batches\tloss 1.0052 (0.9734)\taccu 87.500 (92.663)\n",
      "Epoch-46  260 batches\tloss 0.9277 (0.9749)\taccu 96.875 (92.596)\n",
      "Epoch-46  280 batches\tloss 1.0252 (0.9770)\taccu 87.500 (92.517)\n",
      "Epoch-46  300 batches\tloss 0.8814 (0.9770)\taccu 96.875 (92.495)\n",
      "Epoch-46  320 batches\tloss 1.0990 (0.9793)\taccu 87.500 (92.417)\n",
      "Epoch-46  340 batches\tloss 0.9729 (0.9810)\taccu 95.312 (92.348)\n",
      "Epoch-46  360 batches\tloss 0.9527 (0.9802)\taccu 96.875 (92.396)\n",
      "Epoch-46  380 batches\tloss 0.9991 (0.9820)\taccu 89.062 (92.307)\n",
      "Epoch-46  400 batches\tloss 1.0504 (0.9843)\taccu 85.938 (92.184)\n",
      "Epoch-46  420 batches\tloss 0.9861 (0.9853)\taccu 90.625 (92.117)\n",
      "Epoch-46  440 batches\tloss 1.0188 (0.9849)\taccu 87.500 (92.120)\n",
      "Epoch-46  460 batches\tloss 0.9275 (0.9849)\taccu 90.625 (92.140)\n",
      "Epoch-46  480 batches\tloss 0.9866 (0.9860)\taccu 89.062 (92.061)\n",
      "Epoch-46  127.4s\tTrain: loss 0.9875\taccu 91.9886\tValid: loss 1.1175\taccu 87.0283\n",
      "Epoch 46: val_acc did not improve\n",
      "46 0.001\n",
      "Epoch-47   20 batches\tloss 0.9405 (0.9690)\taccu 89.062 (92.109)\n",
      "Epoch-47   40 batches\tloss 1.0045 (0.9619)\taccu 90.625 (92.656)\n",
      "Epoch-47   60 batches\tloss 0.9361 (0.9690)\taccu 95.312 (92.344)\n",
      "Epoch-47   80 batches\tloss 0.9243 (0.9646)\taccu 92.188 (92.793)\n",
      "Epoch-47  100 batches\tloss 0.9698 (0.9696)\taccu 92.188 (92.781)\n",
      "Epoch-47  120 batches\tloss 1.1907 (0.9756)\taccu 84.375 (92.591)\n",
      "Epoch-47  140 batches\tloss 0.8802 (0.9720)\taccu 93.750 (92.746)\n",
      "Epoch-47  160 batches\tloss 0.8492 (0.9694)\taccu 100.000 (92.773)\n",
      "Epoch-47  180 batches\tloss 1.0325 (0.9673)\taccu 92.188 (92.995)\n",
      "Epoch-47  200 batches\tloss 0.9753 (0.9713)\taccu 93.750 (92.836)\n",
      "Epoch-47  220 batches\tloss 0.9912 (0.9717)\taccu 95.312 (92.805)\n",
      "Epoch-47  240 batches\tloss 1.0813 (0.9724)\taccu 87.500 (92.799)\n",
      "Epoch-47  260 batches\tloss 0.9502 (0.9729)\taccu 95.312 (92.819)\n",
      "Epoch-47  280 batches\tloss 0.9591 (0.9744)\taccu 93.750 (92.734)\n",
      "Epoch-47  300 batches\tloss 0.9727 (0.9759)\taccu 92.188 (92.661)\n",
      "Epoch-47  320 batches\tloss 0.9752 (0.9767)\taccu 92.188 (92.588)\n",
      "Epoch-47  340 batches\tloss 0.9599 (0.9788)\taccu 93.750 (92.482)\n",
      "Epoch-47  360 batches\tloss 0.8899 (0.9791)\taccu 95.312 (92.470)\n",
      "Epoch-47  380 batches\tloss 1.0091 (0.9803)\taccu 89.062 (92.422)\n",
      "Epoch-47  400 batches\tloss 1.0383 (0.9809)\taccu 89.062 (92.379)\n",
      "Epoch-47  420 batches\tloss 0.9529 (0.9829)\taccu 95.312 (92.314)\n",
      "Epoch-47  440 batches\tloss 1.1239 (0.9847)\taccu 87.500 (92.255)\n",
      "Epoch-47  460 batches\tloss 1.0805 (0.9879)\taccu 89.062 (92.143)\n",
      "Epoch-47  480 batches\tloss 1.0797 (0.9886)\taccu 84.375 (92.113)\n",
      "Epoch-47  126.1s\tTrain: loss 0.9890\taccu 92.0928\tValid: loss 1.1031\taccu 87.6621\n",
      "Epoch 47: val_acc did not improve\n",
      "47 0.001\n",
      "Epoch-48   20 batches\tloss 0.9321 (0.9552)\taccu 93.750 (92.891)\n",
      "Epoch-48   40 batches\tloss 0.9628 (0.9801)\taccu 93.750 (92.344)\n",
      "Epoch-48   60 batches\tloss 0.9772 (0.9759)\taccu 93.750 (92.604)\n",
      "Epoch-48   80 batches\tloss 0.9476 (0.9760)\taccu 95.312 (92.676)\n",
      "Epoch-48  100 batches\tloss 1.0312 (0.9734)\taccu 90.625 (92.703)\n",
      "Epoch-48  120 batches\tloss 0.9853 (0.9734)\taccu 92.188 (92.786)\n",
      "Epoch-48  140 batches\tloss 0.9300 (0.9715)\taccu 92.188 (92.801)\n",
      "Epoch-48  160 batches\tloss 0.8976 (0.9710)\taccu 98.438 (92.881)\n",
      "Epoch-48  180 batches\tloss 0.9154 (0.9742)\taccu 93.750 (92.743)\n",
      "Epoch-48  200 batches\tloss 0.9363 (0.9755)\taccu 93.750 (92.727)\n",
      "Epoch-48  220 batches\tloss 0.9602 (0.9769)\taccu 95.312 (92.678)\n",
      "Epoch-48  240 batches\tloss 1.0150 (0.9775)\taccu 90.625 (92.689)\n",
      "Epoch-48  260 batches\tloss 0.9273 (0.9796)\taccu 98.438 (92.602)\n",
      "Epoch-48  280 batches\tloss 1.0374 (0.9802)\taccu 89.062 (92.533)\n",
      "Epoch-48  300 batches\tloss 1.0638 (0.9799)\taccu 84.375 (92.583)\n",
      "Epoch-48  320 batches\tloss 1.0207 (0.9827)\taccu 85.938 (92.505)\n",
      "Epoch-48  340 batches\tloss 1.0300 (0.9839)\taccu 93.750 (92.472)\n",
      "Epoch-48  360 batches\tloss 1.0408 (0.9837)\taccu 90.625 (92.483)\n",
      "Epoch-48  380 batches\tloss 0.9231 (0.9840)\taccu 96.875 (92.451)\n",
      "Epoch-48  400 batches\tloss 0.9847 (0.9829)\taccu 90.625 (92.480)\n",
      "Epoch-48  420 batches\tloss 1.0607 (0.9826)\taccu 87.500 (92.493)\n",
      "Epoch-48  440 batches\tloss 0.9844 (0.9840)\taccu 90.625 (92.401)\n",
      "Epoch-48  460 batches\tloss 1.0205 (0.9835)\taccu 85.938 (92.429)\n",
      "Epoch-48  480 batches\tloss 1.0918 (0.9830)\taccu 89.062 (92.448)\n",
      "Epoch-48  93.0s\tTrain: loss 0.9838\taccu 92.3990\tValid: loss 1.0948\taccu 88.1486\n",
      "Epoch 48: val_acc improved from 88.0749 to 88.1486, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "48 0.001\n",
      "Epoch-49   20 batches\tloss 0.9447 (0.9461)\taccu 95.312 (94.219)\n",
      "Epoch-49   40 batches\tloss 0.8909 (0.9438)\taccu 98.438 (94.297)\n",
      "Epoch-49   60 batches\tloss 0.9412 (0.9527)\taccu 93.750 (93.698)\n",
      "Epoch-49   80 batches\tloss 0.8934 (0.9554)\taccu 95.312 (93.418)\n",
      "Epoch-49  100 batches\tloss 1.0456 (0.9604)\taccu 90.625 (93.250)\n",
      "Epoch-49  120 batches\tloss 0.9534 (0.9627)\taccu 93.750 (93.359)\n",
      "Epoch-49  140 batches\tloss 1.2008 (0.9652)\taccu 75.000 (93.125)\n",
      "Epoch-49  160 batches\tloss 0.9132 (0.9671)\taccu 96.875 (93.096)\n",
      "Epoch-49  180 batches\tloss 0.9875 (0.9720)\taccu 89.062 (92.847)\n",
      "Epoch-49  200 batches\tloss 0.9003 (0.9718)\taccu 95.312 (92.805)\n",
      "Epoch-49  220 batches\tloss 0.9546 (0.9758)\taccu 92.188 (92.614)\n",
      "Epoch-49  240 batches\tloss 0.9933 (0.9786)\taccu 89.062 (92.474)\n",
      "Epoch-49  260 batches\tloss 0.9534 (0.9780)\taccu 93.750 (92.536)\n",
      "Epoch-49  280 batches\tloss 0.8856 (0.9789)\taccu 95.312 (92.506)\n",
      "Epoch-49  300 batches\tloss 1.1043 (0.9794)\taccu 85.938 (92.448)\n",
      "Epoch-49  320 batches\tloss 0.9674 (0.9792)\taccu 92.188 (92.461)\n",
      "Epoch-49  340 batches\tloss 0.8849 (0.9798)\taccu 96.875 (92.449)\n",
      "Epoch-49  360 batches\tloss 1.0570 (0.9800)\taccu 89.062 (92.461)\n",
      "Epoch-49  380 batches\tloss 0.9374 (0.9810)\taccu 95.312 (92.414)\n",
      "Epoch-49  400 batches\tloss 0.9606 (0.9815)\taccu 93.750 (92.430)\n",
      "Epoch-49  420 batches\tloss 0.9000 (0.9819)\taccu 95.312 (92.377)\n",
      "Epoch-49  440 batches\tloss 1.1023 (0.9828)\taccu 87.500 (92.308)\n",
      "Epoch-49  460 batches\tloss 1.1663 (0.9846)\taccu 84.375 (92.228)\n",
      "Epoch-49  480 batches\tloss 0.8978 (0.9847)\taccu 96.875 (92.275)\n",
      "Epoch-49  92.7s\tTrain: loss 0.9862\taccu 92.1907\tValid: loss 1.1041\taccu 87.4705\n",
      "Epoch 49: val_acc did not improve\n",
      "49 0.001\n",
      "Epoch-50   20 batches\tloss 1.0197 (0.9739)\taccu 89.062 (92.500)\n",
      "Epoch-50   40 batches\tloss 0.9302 (0.9739)\taccu 98.438 (92.617)\n",
      "Epoch-50   60 batches\tloss 0.8843 (0.9706)\taccu 96.875 (92.995)\n",
      "Epoch-50   80 batches\tloss 0.8174 (0.9701)\taccu 100.000 (92.988)\n",
      "Epoch-50  100 batches\tloss 0.9475 (0.9673)\taccu 92.188 (92.984)\n",
      "Epoch-50  120 batches\tloss 1.0762 (0.9678)\taccu 92.188 (92.930)\n",
      "Epoch-50  140 batches\tloss 0.8717 (0.9642)\taccu 96.875 (93.192)\n",
      "Epoch-50  160 batches\tloss 0.8988 (0.9652)\taccu 95.312 (93.154)\n",
      "Epoch-50  180 batches\tloss 0.9038 (0.9670)\taccu 93.750 (93.134)\n",
      "Epoch-50  200 batches\tloss 0.9533 (0.9688)\taccu 89.062 (92.984)\n",
      "Epoch-50  220 batches\tloss 1.0458 (0.9734)\taccu 92.188 (92.706)\n",
      "Epoch-50  240 batches\tloss 0.8823 (0.9763)\taccu 96.875 (92.552)\n",
      "Epoch-50  260 batches\tloss 0.9362 (0.9749)\taccu 90.625 (92.644)\n",
      "Epoch-50  280 batches\tloss 1.0318 (0.9750)\taccu 89.062 (92.634)\n",
      "Epoch-50  300 batches\tloss 0.9744 (0.9766)\taccu 93.750 (92.536)\n",
      "Epoch-50  320 batches\tloss 1.0259 (0.9777)\taccu 93.750 (92.451)\n",
      "Epoch-50  340 batches\tloss 0.9883 (0.9789)\taccu 90.625 (92.385)\n",
      "Epoch-50  360 batches\tloss 1.0082 (0.9801)\taccu 95.312 (92.370)\n",
      "Epoch-50  380 batches\tloss 0.9201 (0.9794)\taccu 92.188 (92.389)\n",
      "Epoch-50  400 batches\tloss 1.0054 (0.9813)\taccu 87.500 (92.309)\n",
      "Epoch-50  420 batches\tloss 1.0569 (0.9832)\taccu 93.750 (92.243)\n",
      "Epoch-50  440 batches\tloss 1.0003 (0.9835)\taccu 87.500 (92.255)\n",
      "Epoch-50  460 batches\tloss 0.9364 (0.9835)\taccu 93.750 (92.262)\n",
      "Epoch-50  480 batches\tloss 0.9265 (0.9837)\taccu 96.875 (92.246)\n",
      "Epoch-50  118.8s\tTrain: loss 0.9839\taccu 92.2033\tValid: loss 1.1116\taccu 87.5737\n",
      "Epoch 50: val_acc did not improve\n",
      "50 0.001\n",
      "Epoch-51   20 batches\tloss 0.8839 (0.9579)\taccu 95.312 (93.516)\n",
      "Epoch-51   40 batches\tloss 0.9481 (0.9582)\taccu 95.312 (93.711)\n",
      "Epoch-51   60 batches\tloss 0.8824 (0.9541)\taccu 100.000 (93.698)\n",
      "Epoch-51   80 batches\tloss 0.9194 (0.9563)\taccu 96.875 (93.555)\n",
      "Epoch-51  100 batches\tloss 0.9147 (0.9607)\taccu 96.875 (93.375)\n",
      "Epoch-51  120 batches\tloss 0.9274 (0.9593)\taccu 90.625 (93.307)\n",
      "Epoch-51  140 batches\tloss 0.8896 (0.9609)\taccu 96.875 (93.315)\n",
      "Epoch-51  160 batches\tloss 0.9619 (0.9652)\taccu 96.875 (93.076)\n",
      "Epoch-51  180 batches\tloss 1.0639 (0.9645)\taccu 87.500 (93.125)\n",
      "Epoch-51  200 batches\tloss 0.9670 (0.9637)\taccu 90.625 (93.180)\n",
      "Epoch-51  220 batches\tloss 1.0860 (0.9671)\taccu 89.062 (93.004)\n",
      "Epoch-51  240 batches\tloss 0.9817 (0.9665)\taccu 93.750 (93.040)\n",
      "Epoch-51  260 batches\tloss 1.0601 (0.9669)\taccu 85.938 (93.005)\n",
      "Epoch-51  280 batches\tloss 0.9313 (0.9691)\taccu 93.750 (92.840)\n",
      "Epoch-51  300 batches\tloss 0.9207 (0.9712)\taccu 96.875 (92.781)\n",
      "Epoch-51  320 batches\tloss 1.0148 (0.9752)\taccu 87.500 (92.617)\n",
      "Epoch-51  340 batches\tloss 0.9712 (0.9750)\taccu 92.188 (92.606)\n",
      "Epoch-51  360 batches\tloss 0.9964 (0.9768)\taccu 92.188 (92.504)\n",
      "Epoch-51  380 batches\tloss 1.0769 (0.9793)\taccu 90.625 (92.405)\n",
      "Epoch-51  400 batches\tloss 0.9894 (0.9794)\taccu 90.625 (92.414)\n",
      "Epoch-51  420 batches\tloss 0.9350 (0.9795)\taccu 93.750 (92.418)\n",
      "Epoch-51  440 batches\tloss 0.9246 (0.9797)\taccu 96.875 (92.390)\n",
      "Epoch-51  460 batches\tloss 0.8706 (0.9803)\taccu 98.438 (92.408)\n",
      "Epoch-51  480 batches\tloss 1.0539 (0.9808)\taccu 92.188 (92.412)\n",
      "Epoch-51  127.2s\tTrain: loss 0.9809\taccu 92.3958\tValid: loss 1.0991\taccu 87.7506\n",
      "Epoch 51: val_acc did not improve\n",
      "51 0.001\n",
      "Epoch-52   20 batches\tloss 0.9330 (0.9420)\taccu 96.875 (94.844)\n",
      "Epoch-52   40 batches\tloss 1.0725 (0.9614)\taccu 89.062 (93.555)\n",
      "Epoch-52   60 batches\tloss 0.9239 (0.9709)\taccu 95.312 (92.969)\n",
      "Epoch-52   80 batches\tloss 0.9543 (0.9716)\taccu 96.875 (93.008)\n",
      "Epoch-52  100 batches\tloss 0.9626 (0.9752)\taccu 89.062 (92.797)\n",
      "Epoch-52  120 batches\tloss 0.9250 (0.9685)\taccu 95.312 (93.060)\n",
      "Epoch-52  140 batches\tloss 0.9678 (0.9688)\taccu 93.750 (92.946)\n",
      "Epoch-52  160 batches\tloss 0.9474 (0.9649)\taccu 92.188 (93.076)\n",
      "Epoch-52  180 batches\tloss 0.8597 (0.9635)\taccu 96.875 (93.073)\n",
      "Epoch-52  200 batches\tloss 0.9987 (0.9641)\taccu 89.062 (93.039)\n",
      "Epoch-52  220 batches\tloss 1.0232 (0.9659)\taccu 92.188 (92.919)\n",
      "Epoch-52  240 batches\tloss 1.0729 (0.9674)\taccu 92.188 (92.891)\n",
      "Epoch-52  260 batches\tloss 0.8938 (0.9684)\taccu 93.750 (92.873)\n",
      "Epoch-52  280 batches\tloss 1.0544 (0.9706)\taccu 89.062 (92.785)\n",
      "Epoch-52  300 batches\tloss 0.9010 (0.9717)\taccu 96.875 (92.750)\n",
      "Epoch-52  320 batches\tloss 0.9750 (0.9717)\taccu 92.188 (92.744)\n",
      "Epoch-52  340 batches\tloss 0.9777 (0.9733)\taccu 93.750 (92.707)\n",
      "Epoch-52  360 batches\tloss 1.0395 (0.9743)\taccu 89.062 (92.639)\n",
      "Epoch-52  380 batches\tloss 0.9140 (0.9756)\taccu 93.750 (92.566)\n",
      "Epoch-52  400 batches\tloss 1.0422 (0.9766)\taccu 89.062 (92.531)\n",
      "Epoch-52  420 batches\tloss 0.9894 (0.9769)\taccu 95.312 (92.530)\n",
      "Epoch-52  440 batches\tloss 1.0246 (0.9773)\taccu 87.500 (92.500)\n",
      "Epoch-52  460 batches\tloss 0.9299 (0.9781)\taccu 93.750 (92.466)\n",
      "Epoch-52  480 batches\tloss 0.9551 (0.9774)\taccu 93.750 (92.497)\n",
      "Epoch-52  126.8s\tTrain: loss 0.9782\taccu 92.4779\tValid: loss 1.0966\taccu 87.9864\n",
      "Epoch 52: val_acc did not improve\n",
      "52 0.001\n",
      "Epoch-53   20 batches\tloss 0.8321 (0.9330)\taccu 100.000 (94.141)\n",
      "Epoch-53   40 batches\tloss 0.8673 (0.9293)\taccu 96.875 (94.219)\n",
      "Epoch-53   60 batches\tloss 0.9826 (0.9389)\taccu 93.750 (94.010)\n",
      "Epoch-53   80 batches\tloss 0.9767 (0.9490)\taccu 90.625 (93.477)\n",
      "Epoch-53  100 batches\tloss 0.8586 (0.9498)\taccu 96.875 (93.516)\n",
      "Epoch-53  120 batches\tloss 1.0575 (0.9551)\taccu 92.188 (93.320)\n",
      "Epoch-53  140 batches\tloss 1.0127 (0.9579)\taccu 90.625 (93.304)\n",
      "Epoch-53  160 batches\tloss 0.8732 (0.9577)\taccu 98.438 (93.281)\n",
      "Epoch-53  180 batches\tloss 1.0165 (0.9598)\taccu 89.062 (93.151)\n",
      "Epoch-53  200 batches\tloss 0.9606 (0.9616)\taccu 92.188 (93.102)\n",
      "Epoch-53  220 batches\tloss 0.9277 (0.9605)\taccu 96.875 (93.168)\n",
      "Epoch-53  240 batches\tloss 0.8394 (0.9623)\taccu 100.000 (93.066)\n",
      "Epoch-53  260 batches\tloss 1.1178 (0.9630)\taccu 85.938 (93.011)\n",
      "Epoch-53  280 batches\tloss 0.9784 (0.9644)\taccu 92.188 (92.941)\n",
      "Epoch-53  300 batches\tloss 0.9381 (0.9670)\taccu 92.188 (92.797)\n",
      "Epoch-53  320 batches\tloss 1.0382 (0.9688)\taccu 92.188 (92.754)\n",
      "Epoch-53  340 batches\tloss 0.9761 (0.9707)\taccu 93.750 (92.688)\n",
      "Epoch-53  360 batches\tloss 0.9997 (0.9716)\taccu 90.625 (92.669)\n",
      "Epoch-53  380 batches\tloss 1.0496 (0.9734)\taccu 90.625 (92.599)\n",
      "Epoch-53  400 batches\tloss 1.1055 (0.9746)\taccu 85.938 (92.543)\n",
      "Epoch-53  420 batches\tloss 0.9959 (0.9768)\taccu 90.625 (92.440)\n",
      "Epoch-53  440 batches\tloss 1.1554 (0.9782)\taccu 85.938 (92.401)\n",
      "Epoch-53  460 batches\tloss 0.9372 (0.9784)\taccu 93.750 (92.374)\n",
      "Epoch-53  480 batches\tloss 0.9490 (0.9783)\taccu 93.750 (92.373)\n",
      "Epoch-53  127.0s\tTrain: loss 0.9799\taccu 92.3296\tValid: loss 1.1016\taccu 87.5147\n",
      "Epoch 53: val_acc did not improve\n",
      "53 0.001\n",
      "Epoch-54   20 batches\tloss 0.9379 (0.9496)\taccu 96.875 (94.062)\n",
      "Epoch-54   40 batches\tloss 0.9803 (0.9541)\taccu 92.188 (93.516)\n",
      "Epoch-54   60 batches\tloss 0.9035 (0.9459)\taccu 96.875 (93.932)\n",
      "Epoch-54   80 batches\tloss 0.9443 (0.9488)\taccu 92.188 (93.809)\n",
      "Epoch-54  100 batches\tloss 0.9461 (0.9504)\taccu 92.188 (93.797)\n",
      "Epoch-54  120 batches\tloss 0.9522 (0.9528)\taccu 92.188 (93.646)\n",
      "Epoch-54  140 batches\tloss 0.9810 (0.9539)\taccu 90.625 (93.571)\n",
      "Epoch-54  160 batches\tloss 0.9593 (0.9551)\taccu 93.750 (93.496)\n",
      "Epoch-54  180 batches\tloss 0.9922 (0.9585)\taccu 90.625 (93.325)\n",
      "Epoch-54  200 batches\tloss 0.8773 (0.9609)\taccu 96.875 (93.156)\n",
      "Epoch-54  220 batches\tloss 0.9052 (0.9623)\taccu 96.875 (93.139)\n",
      "Epoch-54  240 batches\tloss 0.8667 (0.9613)\taccu 96.875 (93.197)\n",
      "Epoch-54  260 batches\tloss 0.9355 (0.9617)\taccu 92.188 (93.227)\n",
      "Epoch-54  280 batches\tloss 0.8620 (0.9611)\taccu 96.875 (93.292)\n",
      "Epoch-54  300 batches\tloss 1.0171 (0.9613)\taccu 87.500 (93.260)\n",
      "Epoch-54  320 batches\tloss 0.9562 (0.9609)\taccu 92.188 (93.232)\n",
      "Epoch-54  340 batches\tloss 0.9605 (0.9621)\taccu 93.750 (93.157)\n",
      "Epoch-54  360 batches\tloss 0.9848 (0.9626)\taccu 93.750 (93.121)\n",
      "Epoch-54  380 batches\tloss 0.8761 (0.9647)\taccu 98.438 (93.026)\n",
      "Epoch-54  400 batches\tloss 0.9849 (0.9660)\taccu 89.062 (92.953)\n",
      "Epoch-54  420 batches\tloss 1.0156 (0.9664)\taccu 87.500 (92.913)\n",
      "Epoch-54  440 batches\tloss 0.9855 (0.9670)\taccu 90.625 (92.837)\n",
      "Epoch-54  460 batches\tloss 0.9316 (0.9685)\taccu 95.312 (92.768)\n",
      "Epoch-54  480 batches\tloss 0.9198 (0.9691)\taccu 93.750 (92.738)\n",
      "Epoch-54  126.9s\tTrain: loss 0.9700\taccu 92.6831\tValid: loss 1.0917\taccu 88.4434\n",
      "Epoch 54: val_acc improved from 88.1486 to 88.4434, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "54 0.001\n",
      "Epoch-55   20 batches\tloss 0.9941 (0.9575)\taccu 93.750 (93.047)\n",
      "Epoch-55   40 batches\tloss 0.9068 (0.9545)\taccu 98.438 (93.438)\n",
      "Epoch-55   60 batches\tloss 0.8834 (0.9675)\taccu 95.312 (92.708)\n",
      "Epoch-55   80 batches\tloss 0.9168 (0.9689)\taccu 96.875 (92.656)\n",
      "Epoch-55  100 batches\tloss 0.9534 (0.9722)\taccu 93.750 (92.406)\n",
      "Epoch-55  120 batches\tloss 0.8826 (0.9726)\taccu 95.312 (92.422)\n",
      "Epoch-55  140 batches\tloss 0.9103 (0.9742)\taccu 96.875 (92.422)\n",
      "Epoch-55  160 batches\tloss 0.9961 (0.9710)\taccu 90.625 (92.480)\n",
      "Epoch-55  180 batches\tloss 1.1538 (0.9711)\taccu 87.500 (92.509)\n",
      "Epoch-55  200 batches\tloss 0.9238 (0.9695)\taccu 96.875 (92.617)\n",
      "Epoch-55  220 batches\tloss 0.8873 (0.9711)\taccu 93.750 (92.599)\n",
      "Epoch-55  240 batches\tloss 0.9709 (0.9714)\taccu 92.188 (92.604)\n",
      "Epoch-55  260 batches\tloss 0.9265 (0.9754)\taccu 95.312 (92.404)\n",
      "Epoch-55  280 batches\tloss 1.0797 (0.9768)\taccu 84.375 (92.260)\n",
      "Epoch-55  300 batches\tloss 0.9861 (0.9765)\taccu 90.625 (92.224)\n",
      "Epoch-55  320 batches\tloss 0.9264 (0.9767)\taccu 93.750 (92.261)\n",
      "Epoch-55  340 batches\tloss 0.9918 (0.9773)\taccu 90.625 (92.233)\n",
      "Epoch-55  360 batches\tloss 0.9697 (0.9759)\taccu 92.188 (92.292)\n",
      "Epoch-55  380 batches\tloss 0.9645 (0.9768)\taccu 90.625 (92.237)\n",
      "Epoch-55  400 batches\tloss 0.9428 (0.9788)\taccu 93.750 (92.102)\n",
      "Epoch-55  420 batches\tloss 1.1779 (0.9806)\taccu 81.250 (92.031)\n",
      "Epoch-55  440 batches\tloss 0.9834 (0.9799)\taccu 93.750 (92.092)\n",
      "Epoch-55  460 batches\tloss 1.0478 (0.9814)\taccu 85.938 (92.062)\n",
      "Epoch-55  480 batches\tloss 0.9817 (0.9811)\taccu 92.188 (92.070)\n",
      "Epoch-55  126.8s\tTrain: loss 0.9818\taccu 92.0833\tValid: loss 1.0930\taccu 87.7211\n",
      "Epoch 55: val_acc did not improve\n",
      "55 0.001\n",
      "Epoch-56   20 batches\tloss 0.9021 (0.9388)\taccu 96.875 (93.438)\n",
      "Epoch-56   40 batches\tloss 0.8801 (0.9506)\taccu 95.312 (93.516)\n",
      "Epoch-56   60 batches\tloss 0.9430 (0.9549)\taccu 95.312 (93.151)\n",
      "Epoch-56   80 batches\tloss 0.9013 (0.9445)\taccu 96.875 (93.711)\n",
      "Epoch-56  100 batches\tloss 0.9125 (0.9472)\taccu 92.188 (93.609)\n",
      "Epoch-56  120 batches\tloss 1.0836 (0.9460)\taccu 93.750 (93.711)\n",
      "Epoch-56  140 batches\tloss 0.9636 (0.9433)\taccu 92.188 (93.962)\n",
      "Epoch-56  160 batches\tloss 0.8945 (0.9449)\taccu 96.875 (93.867)\n",
      "Epoch-56  180 batches\tloss 1.0085 (0.9467)\taccu 92.188 (93.828)\n",
      "Epoch-56  200 batches\tloss 0.9335 (0.9493)\taccu 96.875 (93.734)\n",
      "Epoch-56  220 batches\tloss 1.1015 (0.9520)\taccu 89.062 (93.594)\n",
      "Epoch-56  240 batches\tloss 0.9592 (0.9540)\taccu 90.625 (93.457)\n",
      "Epoch-56  260 batches\tloss 0.9133 (0.9547)\taccu 93.750 (93.431)\n",
      "Epoch-56  280 batches\tloss 0.9556 (0.9551)\taccu 93.750 (93.387)\n",
      "Epoch-56  300 batches\tloss 0.9309 (0.9554)\taccu 96.875 (93.349)\n",
      "Epoch-56  320 batches\tloss 0.9761 (0.9582)\taccu 92.188 (93.237)\n",
      "Epoch-56  340 batches\tloss 0.9748 (0.9577)\taccu 89.062 (93.217)\n",
      "Epoch-56  360 batches\tloss 1.1282 (0.9596)\taccu 84.375 (93.138)\n",
      "Epoch-56  380 batches\tloss 0.9288 (0.9611)\taccu 98.438 (93.092)\n",
      "Epoch-56  400 batches\tloss 0.9068 (0.9625)\taccu 95.312 (93.012)\n",
      "Epoch-56  420 batches\tloss 0.9121 (0.9621)\taccu 98.438 (93.069)\n",
      "Epoch-56  440 batches\tloss 1.0058 (0.9639)\taccu 93.750 (92.983)\n",
      "Epoch-56  460 batches\tloss 1.0613 (0.9642)\taccu 89.062 (92.969)\n",
      "Epoch-56  480 batches\tloss 1.0596 (0.9654)\taccu 90.625 (92.923)\n",
      "Epoch-56  127.3s\tTrain: loss 0.9669\taccu 92.8504\tValid: loss 1.1089\taccu 87.9864\n",
      "Epoch 56: val_acc did not improve\n",
      "56 0.001\n",
      "Epoch-57   20 batches\tloss 0.9537 (0.9382)\taccu 93.750 (94.766)\n",
      "Epoch-57   40 batches\tloss 1.0438 (0.9619)\taccu 87.500 (93.086)\n",
      "Epoch-57   60 batches\tloss 0.9769 (0.9557)\taccu 89.062 (93.464)\n",
      "Epoch-57   80 batches\tloss 1.0184 (0.9526)\taccu 89.062 (93.574)\n",
      "Epoch-57  100 batches\tloss 1.2009 (0.9521)\taccu 81.250 (93.531)\n",
      "Epoch-57  120 batches\tloss 0.9723 (0.9466)\taccu 93.750 (93.737)\n",
      "Epoch-57  140 batches\tloss 0.8792 (0.9469)\taccu 98.438 (93.817)\n",
      "Epoch-57  160 batches\tloss 0.8986 (0.9478)\taccu 95.312 (93.809)\n",
      "Epoch-57  180 batches\tloss 1.0153 (0.9501)\taccu 89.062 (93.715)\n",
      "Epoch-57  200 batches\tloss 0.9500 (0.9507)\taccu 96.875 (93.695)\n",
      "Epoch-57  220 batches\tloss 1.2270 (0.9543)\taccu 85.938 (93.509)\n",
      "Epoch-57  240 batches\tloss 0.9428 (0.9537)\taccu 96.875 (93.555)\n",
      "Epoch-57  260 batches\tloss 0.9827 (0.9545)\taccu 92.188 (93.474)\n",
      "Epoch-57  280 batches\tloss 0.9275 (0.9540)\taccu 95.312 (93.482)\n",
      "Epoch-57  300 batches\tloss 0.8994 (0.9551)\taccu 98.438 (93.422)\n",
      "Epoch-57  320 batches\tloss 1.0701 (0.9557)\taccu 89.062 (93.403)\n",
      "Epoch-57  340 batches\tloss 0.9491 (0.9579)\taccu 92.188 (93.318)\n",
      "Epoch-57  360 batches\tloss 1.0193 (0.9584)\taccu 90.625 (93.290)\n",
      "Epoch-57  380 batches\tloss 0.9581 (0.9586)\taccu 92.188 (93.248)\n",
      "Epoch-57  400 batches\tloss 1.0180 (0.9598)\taccu 89.062 (93.172)\n",
      "Epoch-57  420 batches\tloss 0.9614 (0.9609)\taccu 92.188 (93.114)\n",
      "Epoch-57  440 batches\tloss 0.8733 (0.9627)\taccu 96.875 (93.022)\n",
      "Epoch-57  460 batches\tloss 0.8985 (0.9646)\taccu 95.312 (92.942)\n",
      "Epoch-57  480 batches\tloss 0.9767 (0.9659)\taccu 93.750 (92.910)\n",
      "Epoch-57  93.7s\tTrain: loss 0.9662\taccu 92.9198\tValid: loss 1.1026\taccu 87.5590\n",
      "Epoch 57: val_acc did not improve\n",
      "57 0.001\n",
      "Epoch-58   20 batches\tloss 0.9960 (0.9570)\taccu 92.188 (93.594)\n",
      "Epoch-58   40 batches\tloss 0.9157 (0.9502)\taccu 95.312 (93.750)\n",
      "Epoch-58   60 batches\tloss 0.9959 (0.9476)\taccu 92.188 (93.828)\n",
      "Epoch-58   80 batches\tloss 0.9071 (0.9498)\taccu 96.875 (93.789)\n",
      "Epoch-58  100 batches\tloss 0.8615 (0.9500)\taccu 100.000 (93.812)\n",
      "Epoch-58  120 batches\tloss 0.8646 (0.9504)\taccu 98.438 (93.802)\n",
      "Epoch-58  140 batches\tloss 0.9852 (0.9500)\taccu 90.625 (93.772)\n",
      "Epoch-58  160 batches\tloss 0.9644 (0.9487)\taccu 89.062 (93.838)\n",
      "Epoch-58  180 batches\tloss 0.8878 (0.9475)\taccu 98.438 (93.898)\n",
      "Epoch-58  200 batches\tloss 0.9009 (0.9464)\taccu 95.312 (93.953)\n",
      "Epoch-58  220 batches\tloss 1.0655 (0.9468)\taccu 87.500 (93.942)\n",
      "Epoch-58  240 batches\tloss 0.9097 (0.9478)\taccu 93.750 (93.913)\n",
      "Epoch-58  260 batches\tloss 0.9041 (0.9483)\taccu 95.312 (93.870)\n",
      "Epoch-58  280 batches\tloss 0.9398 (0.9488)\taccu 95.312 (93.873)\n",
      "Epoch-58  300 batches\tloss 0.9444 (0.9493)\taccu 93.750 (93.833)\n",
      "Epoch-58  320 batches\tloss 0.9167 (0.9502)\taccu 93.750 (93.784)\n",
      "Epoch-58  340 batches\tloss 1.0533 (0.9529)\taccu 85.938 (93.621)\n",
      "Epoch-58  360 batches\tloss 0.9933 (0.9541)\taccu 90.625 (93.550)\n",
      "Epoch-58  380 batches\tloss 1.0229 (0.9553)\taccu 87.500 (93.450)\n",
      "Epoch-58  400 batches\tloss 1.0532 (0.9572)\taccu 84.375 (93.320)\n",
      "Epoch-58  420 batches\tloss 0.8837 (0.9591)\taccu 93.750 (93.222)\n",
      "Epoch-58  440 batches\tloss 0.9198 (0.9608)\taccu 96.875 (93.164)\n",
      "Epoch-58  460 batches\tloss 1.1077 (0.9625)\taccu 87.500 (93.060)\n",
      "Epoch-58  480 batches\tloss 0.9687 (0.9641)\taccu 92.188 (92.979)\n",
      "Epoch-58  89.2s\tTrain: loss 0.9649\taccu 92.9451\tValid: loss 1.1003\taccu 87.1020\n",
      "Epoch 58: val_acc did not improve\n",
      "58 0.001\n",
      "Epoch-59   20 batches\tloss 0.8744 (0.9589)\taccu 95.312 (92.891)\n",
      "Epoch-59   40 batches\tloss 0.9504 (0.9623)\taccu 93.750 (93.125)\n",
      "Epoch-59   60 batches\tloss 0.9271 (0.9598)\taccu 95.312 (93.073)\n",
      "Epoch-59   80 batches\tloss 1.0510 (0.9561)\taccu 89.062 (93.281)\n",
      "Epoch-59  100 batches\tloss 1.0003 (0.9514)\taccu 90.625 (93.516)\n",
      "Epoch-59  120 batches\tloss 0.9312 (0.9562)\taccu 96.875 (93.281)\n",
      "Epoch-59  140 batches\tloss 0.9735 (0.9528)\taccu 93.750 (93.482)\n",
      "Epoch-59  160 batches\tloss 0.9502 (0.9523)\taccu 95.312 (93.584)\n",
      "Epoch-59  180 batches\tloss 0.9515 (0.9548)\taccu 92.188 (93.438)\n",
      "Epoch-59  200 batches\tloss 0.9775 (0.9568)\taccu 92.188 (93.359)\n",
      "Epoch-59  220 batches\tloss 0.9396 (0.9576)\taccu 93.750 (93.366)\n",
      "Epoch-59  240 batches\tloss 1.0007 (0.9588)\taccu 93.750 (93.262)\n",
      "Epoch-59  260 batches\tloss 0.9068 (0.9613)\taccu 96.875 (93.161)\n",
      "Epoch-59  280 batches\tloss 0.9394 (0.9614)\taccu 93.750 (93.136)\n",
      "Epoch-59  300 batches\tloss 0.9873 (0.9618)\taccu 92.188 (93.094)\n",
      "Epoch-59  320 batches\tloss 0.9815 (0.9635)\taccu 93.750 (93.003)\n",
      "Epoch-59  340 batches\tloss 0.9951 (0.9638)\taccu 87.500 (92.987)\n",
      "Epoch-59  360 batches\tloss 1.0679 (0.9647)\taccu 87.500 (92.956)\n",
      "Epoch-59  380 batches\tloss 0.9713 (0.9662)\taccu 95.312 (92.866)\n",
      "Epoch-59  400 batches\tloss 0.9116 (0.9672)\taccu 90.625 (92.820)\n",
      "Epoch-59  420 batches\tloss 0.9713 (0.9676)\taccu 93.750 (92.820)\n",
      "Epoch-59  440 batches\tloss 0.9654 (0.9682)\taccu 95.312 (92.820)\n",
      "Epoch-59  460 batches\tloss 1.0275 (0.9692)\taccu 89.062 (92.772)\n",
      "Epoch-59  480 batches\tloss 1.0239 (0.9694)\taccu 90.625 (92.721)\n",
      "Epoch-59  89.3s\tTrain: loss 0.9701\taccu 92.6799\tValid: loss 1.0954\taccu 88.5908\n",
      "Epoch 59: val_acc improved from 88.4434 to 88.5908, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "59 0.001\n",
      "Epoch-60   20 batches\tloss 0.9588 (0.9792)\taccu 95.312 (91.953)\n",
      "Epoch-60   40 batches\tloss 0.8832 (0.9671)\taccu 98.438 (93.086)\n",
      "Epoch-60   60 batches\tloss 0.8915 (0.9632)\taccu 98.438 (93.229)\n",
      "Epoch-60   80 batches\tloss 1.1376 (0.9618)\taccu 87.500 (93.203)\n",
      "Epoch-60  100 batches\tloss 0.9246 (0.9558)\taccu 95.312 (93.578)\n",
      "Epoch-60  120 batches\tloss 0.9099 (0.9560)\taccu 95.312 (93.555)\n",
      "Epoch-60  140 batches\tloss 0.8759 (0.9544)\taccu 98.438 (93.638)\n",
      "Epoch-60  160 batches\tloss 0.8689 (0.9540)\taccu 98.438 (93.682)\n",
      "Epoch-60  180 batches\tloss 0.9711 (0.9564)\taccu 93.750 (93.559)\n",
      "Epoch-60  200 batches\tloss 0.9092 (0.9573)\taccu 93.750 (93.461)\n",
      "Epoch-60  220 batches\tloss 0.8970 (0.9575)\taccu 95.312 (93.416)\n",
      "Epoch-60  240 batches\tloss 0.8873 (0.9574)\taccu 95.312 (93.444)\n",
      "Epoch-60  260 batches\tloss 0.9714 (0.9573)\taccu 90.625 (93.377)\n",
      "Epoch-60  280 batches\tloss 1.0173 (0.9594)\taccu 92.188 (93.253)\n",
      "Epoch-60  300 batches\tloss 0.9106 (0.9603)\taccu 98.438 (93.214)\n",
      "Epoch-60  320 batches\tloss 0.9400 (0.9616)\taccu 92.188 (93.149)\n",
      "Epoch-60  340 batches\tloss 1.0445 (0.9625)\taccu 89.062 (93.093)\n",
      "Epoch-60  360 batches\tloss 0.9731 (0.9627)\taccu 90.625 (93.069)\n",
      "Epoch-60  380 batches\tloss 0.9227 (0.9632)\taccu 95.312 (93.035)\n",
      "Epoch-60  400 batches\tloss 0.9740 (0.9632)\taccu 95.312 (93.051)\n",
      "Epoch-60  420 batches\tloss 0.9469 (0.9632)\taccu 93.750 (93.039)\n",
      "Epoch-60  440 batches\tloss 1.0246 (0.9642)\taccu 87.500 (92.969)\n",
      "Epoch-60  460 batches\tloss 1.0047 (0.9650)\taccu 89.062 (92.935)\n",
      "Epoch-60  480 batches\tloss 1.0689 (0.9658)\taccu 85.938 (92.871)\n",
      "Epoch-60  89.0s\tTrain: loss 0.9662\taccu 92.8756\tValid: loss 1.0790\taccu 88.6940\n",
      "Epoch 60: val_acc improved from 88.5908 to 88.6940, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "60 0.0001\n",
      "Epoch-61   20 batches\tloss 0.9060 (0.9382)\taccu 96.875 (94.375)\n",
      "Epoch-61   40 batches\tloss 0.9098 (0.9335)\taccu 98.438 (94.453)\n",
      "Epoch-61   60 batches\tloss 0.8726 (0.9231)\taccu 98.438 (95.052)\n",
      "Epoch-61   80 batches\tloss 0.9461 (0.9161)\taccu 95.312 (95.254)\n",
      "Epoch-61  100 batches\tloss 0.8537 (0.9083)\taccu 98.438 (95.562)\n",
      "Epoch-61  120 batches\tloss 0.8870 (0.9038)\taccu 95.312 (95.768)\n",
      "Epoch-61  140 batches\tloss 0.9037 (0.9020)\taccu 95.312 (95.826)\n",
      "Epoch-61  160 batches\tloss 0.8795 (0.8978)\taccu 96.875 (95.957)\n",
      "Epoch-61  180 batches\tloss 0.8331 (0.8987)\taccu 98.438 (95.894)\n",
      "Epoch-61  200 batches\tloss 0.8533 (0.8970)\taccu 98.438 (96.008)\n",
      "Epoch-61  220 batches\tloss 0.8747 (0.8948)\taccu 100.000 (96.108)\n",
      "Epoch-61  240 batches\tloss 0.9044 (0.8928)\taccu 96.875 (96.165)\n",
      "Epoch-61  260 batches\tloss 0.9209 (0.8919)\taccu 93.750 (96.172)\n",
      "Epoch-61  280 batches\tloss 0.9142 (0.8905)\taccu 93.750 (96.194)\n",
      "Epoch-61  300 batches\tloss 0.9076 (0.8890)\taccu 96.875 (96.240)\n",
      "Epoch-61  320 batches\tloss 0.8295 (0.8879)\taccu 98.438 (96.318)\n",
      "Epoch-61  340 batches\tloss 0.8277 (0.8867)\taccu 98.438 (96.319)\n",
      "Epoch-61  360 batches\tloss 0.9123 (0.8856)\taccu 95.312 (96.372)\n",
      "Epoch-61  380 batches\tloss 0.8306 (0.8843)\taccu 98.438 (96.439)\n",
      "Epoch-61  400 batches\tloss 0.9414 (0.8835)\taccu 92.188 (96.473)\n",
      "Epoch-61  420 batches\tloss 0.8581 (0.8831)\taccu 96.875 (96.458)\n",
      "Epoch-61  440 batches\tloss 0.8304 (0.8816)\taccu 100.000 (96.534)\n",
      "Epoch-61  460 batches\tloss 0.8458 (0.8814)\taccu 98.438 (96.549)\n",
      "Epoch-61  480 batches\tloss 0.8737 (0.8809)\taccu 100.000 (96.595)\n",
      "Epoch-61  89.0s\tTrain: loss 0.8803\taccu 96.6225\tValid: loss 1.0155\taccu 91.3178\n",
      "Epoch 61: val_acc improved from 88.6940 to 91.3178, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "61 0.0001\n",
      "Epoch-62   20 batches\tloss 0.8140 (0.8639)\taccu 100.000 (97.188)\n",
      "Epoch-62   40 batches\tloss 0.8603 (0.8721)\taccu 100.000 (96.719)\n",
      "Epoch-62   60 batches\tloss 0.8948 (0.8655)\taccu 95.312 (96.979)\n",
      "Epoch-62   80 batches\tloss 0.8483 (0.8651)\taccu 96.875 (96.934)\n",
      "Epoch-62  100 batches\tloss 0.8531 (0.8632)\taccu 98.438 (96.969)\n",
      "Epoch-62  120 batches\tloss 0.9829 (0.8651)\taccu 92.188 (96.888)\n",
      "Epoch-62  140 batches\tloss 0.8024 (0.8649)\taccu 100.000 (96.987)\n",
      "Epoch-62  160 batches\tloss 0.8295 (0.8648)\taccu 98.438 (97.021)\n",
      "Epoch-62  180 batches\tloss 0.8235 (0.8650)\taccu 98.438 (97.023)\n",
      "Epoch-62  200 batches\tloss 0.8342 (0.8629)\taccu 100.000 (97.102)\n",
      "Epoch-62  220 batches\tloss 0.8367 (0.8615)\taccu 96.875 (97.159)\n",
      "Epoch-62  240 batches\tloss 0.7924 (0.8612)\taccu 100.000 (97.161)\n",
      "Epoch-62  260 batches\tloss 0.8447 (0.8602)\taccu 96.875 (97.200)\n",
      "Epoch-62  280 batches\tloss 0.8433 (0.8604)\taccu 98.438 (97.249)\n",
      "Epoch-62  300 batches\tloss 0.8988 (0.8604)\taccu 96.875 (97.260)\n",
      "Epoch-62  320 batches\tloss 0.8659 (0.8603)\taccu 95.312 (97.275)\n",
      "Epoch-62  340 batches\tloss 0.8800 (0.8601)\taccu 95.312 (97.261)\n",
      "Epoch-62  360 batches\tloss 0.8788 (0.8601)\taccu 93.750 (97.253)\n",
      "Epoch-62  380 batches\tloss 0.8224 (0.8594)\taccu 100.000 (97.274)\n",
      "Epoch-62  400 batches\tloss 0.8324 (0.8592)\taccu 98.438 (97.301)\n",
      "Epoch-62  420 batches\tloss 0.8058 (0.8587)\taccu 100.000 (97.325)\n",
      "Epoch-62  440 batches\tloss 0.8456 (0.8589)\taccu 98.438 (97.330)\n",
      "Epoch-62  460 batches\tloss 0.9223 (0.8588)\taccu 93.750 (97.320)\n",
      "Epoch-62  480 batches\tloss 0.7840 (0.8586)\taccu 100.000 (97.324)\n",
      "Epoch-62  89.2s\tTrain: loss 0.8590\taccu 97.3138\tValid: loss 1.0022\taccu 91.3031\n",
      "Epoch 62: val_acc did not improve\n",
      "62 0.0001\n",
      "Epoch-63   20 batches\tloss 0.8456 (0.8392)\taccu 98.438 (97.812)\n",
      "Epoch-63   40 batches\tloss 0.8576 (0.8478)\taccu 96.875 (97.695)\n",
      "Epoch-63   60 batches\tloss 0.8539 (0.8443)\taccu 98.438 (97.734)\n",
      "Epoch-63   80 batches\tloss 0.8845 (0.8491)\taccu 96.875 (97.656)\n",
      "Epoch-63  100 batches\tloss 0.7914 (0.8462)\taccu 100.000 (97.781)\n",
      "Epoch-63  120 batches\tloss 0.8634 (0.8479)\taccu 98.438 (97.734)\n",
      "Epoch-63  140 batches\tloss 0.8283 (0.8468)\taccu 96.875 (97.723)\n",
      "Epoch-63  160 batches\tloss 0.8075 (0.8464)\taccu 98.438 (97.734)\n",
      "Epoch-63  180 batches\tloss 0.8594 (0.8477)\taccu 96.875 (97.691)\n",
      "Epoch-63  200 batches\tloss 0.9218 (0.8477)\taccu 93.750 (97.680)\n",
      "Epoch-63  220 batches\tloss 0.8898 (0.8472)\taccu 96.875 (97.713)\n",
      "Epoch-63  240 batches\tloss 0.8611 (0.8479)\taccu 96.875 (97.728)\n",
      "Epoch-63  260 batches\tloss 0.8713 (0.8477)\taccu 95.312 (97.710)\n",
      "Epoch-63  280 batches\tloss 0.9983 (0.8479)\taccu 92.188 (97.667)\n",
      "Epoch-63  300 batches\tloss 0.8931 (0.8483)\taccu 96.875 (97.656)\n",
      "Epoch-63  320 batches\tloss 0.8742 (0.8480)\taccu 96.875 (97.705)\n",
      "Epoch-63  340 batches\tloss 0.8589 (0.8477)\taccu 96.875 (97.721)\n",
      "Epoch-63  360 batches\tloss 0.8087 (0.8465)\taccu 98.438 (97.782)\n",
      "Epoch-63  380 batches\tloss 0.8198 (0.8463)\taccu 100.000 (97.771)\n",
      "Epoch-63  400 batches\tloss 0.8316 (0.8459)\taccu 98.438 (97.785)\n",
      "Epoch-63  420 batches\tloss 0.8352 (0.8460)\taccu 96.875 (97.772)\n",
      "Epoch-63  440 batches\tloss 0.8376 (0.8454)\taccu 98.438 (97.791)\n",
      "Epoch-63  460 batches\tloss 0.8171 (0.8452)\taccu 98.438 (97.792)\n",
      "Epoch-63  480 batches\tloss 0.8521 (0.8451)\taccu 96.875 (97.780)\n",
      "Epoch-63  89.4s\tTrain: loss 0.8452\taccu 97.7778\tValid: loss 0.9987\taccu 91.7600\n",
      "Epoch 63: val_acc improved from 91.3178 to 91.7600, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "63 0.0001\n",
      "Epoch-64   20 batches\tloss 0.7906 (0.8366)\taccu 100.000 (98.203)\n",
      "Epoch-64   40 batches\tloss 0.8393 (0.8338)\taccu 100.000 (98.047)\n",
      "Epoch-64   60 batches\tloss 0.8474 (0.8393)\taccu 98.438 (97.813)\n",
      "Epoch-64   80 batches\tloss 0.7914 (0.8397)\taccu 100.000 (97.852)\n",
      "Epoch-64  100 batches\tloss 0.8979 (0.8401)\taccu 95.312 (97.812)\n",
      "Epoch-64  120 batches\tloss 0.7994 (0.8382)\taccu 98.438 (97.904)\n",
      "Epoch-64  140 batches\tloss 0.8282 (0.8380)\taccu 100.000 (97.879)\n",
      "Epoch-64  160 batches\tloss 0.8388 (0.8381)\taccu 95.312 (97.832)\n",
      "Epoch-64  180 batches\tloss 0.9004 (0.8392)\taccu 92.188 (97.778)\n",
      "Epoch-64  200 batches\tloss 0.7786 (0.8381)\taccu 100.000 (97.859)\n",
      "Epoch-64  220 batches\tloss 0.8346 (0.8388)\taccu 100.000 (97.876)\n",
      "Epoch-64  240 batches\tloss 0.8436 (0.8385)\taccu 98.438 (97.943)\n",
      "Epoch-64  260 batches\tloss 0.8165 (0.8375)\taccu 98.438 (97.981)\n",
      "Epoch-64  280 batches\tloss 0.8215 (0.8372)\taccu 100.000 (97.985)\n",
      "Epoch-64  300 batches\tloss 0.8382 (0.8374)\taccu 98.438 (97.984)\n",
      "Epoch-64  320 batches\tloss 0.8130 (0.8378)\taccu 98.438 (97.998)\n",
      "Epoch-64  340 batches\tloss 0.8006 (0.8374)\taccu 98.438 (98.006)\n",
      "Epoch-64  360 batches\tloss 0.8045 (0.8373)\taccu 98.438 (98.003)\n",
      "Epoch-64  380 batches\tloss 0.8325 (0.8373)\taccu 98.438 (98.002)\n",
      "Epoch-64  400 batches\tloss 0.8508 (0.8379)\taccu 98.438 (97.965)\n",
      "Epoch-64  420 batches\tloss 0.8177 (0.8382)\taccu 100.000 (97.939)\n",
      "Epoch-64  440 batches\tloss 0.7672 (0.8385)\taccu 100.000 (97.944)\n",
      "Epoch-64  460 batches\tloss 0.7873 (0.8381)\taccu 100.000 (97.962)\n",
      "Epoch-64  480 batches\tloss 0.8158 (0.8384)\taccu 98.438 (97.952)\n",
      "Epoch-64  89.0s\tTrain: loss 0.8391\taccu 97.9325\tValid: loss 0.9954\taccu 91.5537\n",
      "Epoch 64: val_acc did not improve\n",
      "64 0.0001\n",
      "Epoch-65   20 batches\tloss 0.8086 (0.8352)\taccu 100.000 (97.891)\n",
      "Epoch-65   40 batches\tloss 0.8138 (0.8411)\taccu 98.438 (97.539)\n",
      "Epoch-65   60 batches\tloss 0.7893 (0.8368)\taccu 100.000 (97.786)\n",
      "Epoch-65   80 batches\tloss 0.8535 (0.8361)\taccu 96.875 (97.930)\n",
      "Epoch-65  100 batches\tloss 0.7956 (0.8379)\taccu 100.000 (97.875)\n",
      "Epoch-65  120 batches\tloss 0.7871 (0.8391)\taccu 100.000 (97.904)\n",
      "Epoch-65  140 batches\tloss 0.7917 (0.8374)\taccu 100.000 (97.991)\n",
      "Epoch-65  160 batches\tloss 0.8103 (0.8365)\taccu 98.438 (98.057)\n",
      "Epoch-65  180 batches\tloss 0.8553 (0.8357)\taccu 95.312 (98.082)\n",
      "Epoch-65  200 batches\tloss 0.8168 (0.8351)\taccu 96.875 (98.070)\n",
      "Epoch-65  220 batches\tloss 0.8998 (0.8353)\taccu 95.312 (98.054)\n",
      "Epoch-65  240 batches\tloss 0.8248 (0.8344)\taccu 98.438 (98.105)\n",
      "Epoch-65  260 batches\tloss 0.8277 (0.8338)\taccu 98.438 (98.143)\n",
      "Epoch-65  280 batches\tloss 0.8472 (0.8349)\taccu 95.312 (98.086)\n",
      "Epoch-65  300 batches\tloss 0.8223 (0.8344)\taccu 96.875 (98.130)\n",
      "Epoch-65  320 batches\tloss 0.8058 (0.8351)\taccu 96.875 (98.110)\n",
      "Epoch-65  340 batches\tloss 0.8099 (0.8348)\taccu 100.000 (98.116)\n",
      "Epoch-65  360 batches\tloss 0.8300 (0.8351)\taccu 98.438 (98.069)\n",
      "Epoch-65  380 batches\tloss 0.8258 (0.8354)\taccu 98.438 (98.035)\n",
      "Epoch-65  400 batches\tloss 0.8934 (0.8360)\taccu 93.750 (98.000)\n",
      "Epoch-65  420 batches\tloss 0.8124 (0.8358)\taccu 100.000 (98.013)\n",
      "Epoch-65  440 batches\tloss 0.8327 (0.8360)\taccu 98.438 (98.011)\n",
      "Epoch-65  460 batches\tloss 0.7902 (0.8356)\taccu 100.000 (98.033)\n",
      "Epoch-65  480 batches\tloss 0.9771 (0.8355)\taccu 93.750 (98.040)\n",
      "Epoch-65  88.7s\tTrain: loss 0.8360\taccu 98.0145\tValid: loss 0.9896\taccu 91.7305\n",
      "Epoch 65: val_acc did not improve\n",
      "65 0.0001\n",
      "Epoch-66   20 batches\tloss 0.8472 (0.8255)\taccu 95.312 (98.203)\n",
      "Epoch-66   40 batches\tloss 0.8391 (0.8269)\taccu 96.875 (98.320)\n",
      "Epoch-66   60 batches\tloss 0.8070 (0.8319)\taccu 100.000 (97.969)\n",
      "Epoch-66   80 batches\tloss 0.8211 (0.8302)\taccu 98.438 (98.008)\n",
      "Epoch-66  100 batches\tloss 0.8044 (0.8298)\taccu 98.438 (98.078)\n",
      "Epoch-66  120 batches\tloss 0.8509 (0.8312)\taccu 95.312 (98.060)\n",
      "Epoch-66  140 batches\tloss 0.8513 (0.8309)\taccu 96.875 (98.080)\n",
      "Epoch-66  160 batches\tloss 0.8763 (0.8326)\taccu 95.312 (97.949)\n",
      "Epoch-66  180 batches\tloss 0.7860 (0.8309)\taccu 100.000 (98.021)\n",
      "Epoch-66  200 batches\tloss 0.8518 (0.8295)\taccu 95.312 (98.062)\n",
      "Epoch-66  220 batches\tloss 0.8487 (0.8303)\taccu 96.875 (98.082)\n",
      "Epoch-66  240 batches\tloss 0.8220 (0.8305)\taccu 100.000 (98.073)\n",
      "Epoch-66  260 batches\tloss 0.8323 (0.8314)\taccu 100.000 (98.059)\n",
      "Epoch-66  280 batches\tloss 0.7981 (0.8307)\taccu 100.000 (98.075)\n",
      "Epoch-66  300 batches\tloss 0.7909 (0.8309)\taccu 98.438 (98.068)\n",
      "Epoch-66  320 batches\tloss 0.8586 (0.8309)\taccu 95.312 (98.066)\n",
      "Epoch-66  340 batches\tloss 0.8155 (0.8314)\taccu 98.438 (98.074)\n",
      "Epoch-66  360 batches\tloss 0.8359 (0.8309)\taccu 95.312 (98.082)\n",
      "Epoch-66  380 batches\tloss 0.7977 (0.8308)\taccu 100.000 (98.113)\n",
      "Epoch-66  400 batches\tloss 0.8014 (0.8309)\taccu 100.000 (98.109)\n",
      "Epoch-66  420 batches\tloss 0.8457 (0.8303)\taccu 95.312 (98.132)\n",
      "Epoch-66  440 batches\tloss 0.7885 (0.8303)\taccu 100.000 (98.146)\n",
      "Epoch-66  460 batches\tloss 0.9073 (0.8312)\taccu 93.750 (98.115)\n",
      "Epoch-66  480 batches\tloss 0.7947 (0.8308)\taccu 100.000 (98.141)\n",
      "Epoch-66  89.0s\tTrain: loss 0.8307\taccu 98.1534\tValid: loss 0.9894\taccu 91.8190\n",
      "Epoch 66: val_acc improved from 91.7600 to 91.8190, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "66 0.0001\n",
      "Epoch-67   20 batches\tloss 0.8304 (0.8220)\taccu 98.438 (98.672)\n",
      "Epoch-67   40 batches\tloss 0.8708 (0.8280)\taccu 98.438 (98.359)\n",
      "Epoch-67   60 batches\tloss 0.8208 (0.8246)\taccu 100.000 (98.490)\n",
      "Epoch-67   80 batches\tloss 0.8260 (0.8239)\taccu 98.438 (98.516)\n",
      "Epoch-67  100 batches\tloss 0.8928 (0.8256)\taccu 96.875 (98.484)\n",
      "Epoch-67  120 batches\tloss 0.8524 (0.8248)\taccu 96.875 (98.516)\n",
      "Epoch-67  140 batches\tloss 0.8003 (0.8254)\taccu 100.000 (98.438)\n",
      "Epoch-67  160 batches\tloss 0.8360 (0.8264)\taccu 98.438 (98.418)\n",
      "Epoch-67  180 batches\tloss 0.8120 (0.8248)\taccu 98.438 (98.446)\n",
      "Epoch-67  200 batches\tloss 0.8222 (0.8241)\taccu 98.438 (98.453)\n",
      "Epoch-67  220 batches\tloss 0.8050 (0.8232)\taccu 98.438 (98.487)\n",
      "Epoch-67  240 batches\tloss 0.7996 (0.8229)\taccu 100.000 (98.483)\n",
      "Epoch-67  260 batches\tloss 0.7939 (0.8247)\taccu 100.000 (98.377)\n",
      "Epoch-67  280 batches\tloss 0.7924 (0.8252)\taccu 96.875 (98.371)\n",
      "Epoch-67  300 batches\tloss 0.7890 (0.8249)\taccu 100.000 (98.375)\n",
      "Epoch-67  320 batches\tloss 0.8513 (0.8250)\taccu 96.875 (98.389)\n",
      "Epoch-67  340 batches\tloss 0.8561 (0.8252)\taccu 95.312 (98.364)\n",
      "Epoch-67  360 batches\tloss 0.8728 (0.8272)\taccu 95.312 (98.299)\n",
      "Epoch-67  380 batches\tloss 0.9113 (0.8272)\taccu 92.188 (98.302)\n",
      "Epoch-67  400 batches\tloss 0.8413 (0.8272)\taccu 96.875 (98.297)\n",
      "Epoch-67  420 batches\tloss 0.8140 (0.8266)\taccu 100.000 (98.311)\n",
      "Epoch-67  440 batches\tloss 0.8144 (0.8272)\taccu 100.000 (98.278)\n",
      "Epoch-67  460 batches\tloss 0.8395 (0.8268)\taccu 98.438 (98.291)\n",
      "Epoch-67  480 batches\tloss 0.7935 (0.8269)\taccu 100.000 (98.291)\n",
      "Epoch-67  89.4s\tTrain: loss 0.8277\taccu 98.2702\tValid: loss 0.9932\taccu 91.7600\n",
      "Epoch 67: val_acc did not improve\n",
      "67 0.0001\n",
      "Epoch-68   20 batches\tloss 0.8005 (0.8282)\taccu 100.000 (98.750)\n",
      "Epoch-68   40 batches\tloss 0.9090 (0.8332)\taccu 93.750 (98.438)\n",
      "Epoch-68   60 batches\tloss 0.7970 (0.8270)\taccu 100.000 (98.568)\n",
      "Epoch-68   80 batches\tloss 0.8135 (0.8234)\taccu 96.875 (98.652)\n",
      "Epoch-68  100 batches\tloss 0.8563 (0.8232)\taccu 96.875 (98.672)\n",
      "Epoch-68  120 batches\tloss 0.7764 (0.8217)\taccu 100.000 (98.724)\n",
      "Epoch-68  140 batches\tloss 0.7883 (0.8206)\taccu 100.000 (98.683)\n",
      "Epoch-68  160 batches\tloss 0.8015 (0.8197)\taccu 100.000 (98.672)\n",
      "Epoch-68  180 batches\tloss 0.8358 (0.8198)\taccu 98.438 (98.646)\n",
      "Epoch-68  200 batches\tloss 0.8882 (0.8211)\taccu 95.312 (98.594)\n",
      "Epoch-68  220 batches\tloss 0.7740 (0.8216)\taccu 100.000 (98.530)\n",
      "Epoch-68  240 batches\tloss 0.8513 (0.8225)\taccu 100.000 (98.490)\n",
      "Epoch-68  260 batches\tloss 0.8109 (0.8222)\taccu 100.000 (98.504)\n",
      "Epoch-68  280 batches\tloss 0.8375 (0.8222)\taccu 98.438 (98.521)\n",
      "Epoch-68  300 batches\tloss 0.8062 (0.8227)\taccu 96.875 (98.464)\n",
      "Epoch-68  320 batches\tloss 0.8340 (0.8227)\taccu 98.438 (98.472)\n",
      "Epoch-68  340 batches\tloss 0.8062 (0.8226)\taccu 100.000 (98.470)\n",
      "Epoch-68  360 batches\tloss 0.8180 (0.8222)\taccu 96.875 (98.485)\n",
      "Epoch-68  380 batches\tloss 0.7804 (0.8233)\taccu 100.000 (98.466)\n",
      "Epoch-68  400 batches\tloss 0.8309 (0.8238)\taccu 96.875 (98.438)\n",
      "Epoch-68  420 batches\tloss 0.8421 (0.8237)\taccu 95.312 (98.430)\n",
      "Epoch-68  440 batches\tloss 0.8815 (0.8236)\taccu 95.312 (98.430)\n",
      "Epoch-68  460 batches\tloss 0.8501 (0.8233)\taccu 98.438 (98.448)\n",
      "Epoch-68  480 batches\tloss 0.8286 (0.8232)\taccu 100.000 (98.464)\n",
      "Epoch-68  89.1s\tTrain: loss 0.8234\taccu 98.4501\tValid: loss 0.9911\taccu 91.9517\n",
      "Epoch 68: val_acc improved from 91.8190 to 91.9517, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "68 0.0001\n",
      "Epoch-69   20 batches\tloss 0.8185 (0.8185)\taccu 98.438 (98.750)\n",
      "Epoch-69   40 batches\tloss 0.8002 (0.8145)\taccu 100.000 (98.789)\n",
      "Epoch-69   60 batches\tloss 0.7917 (0.8140)\taccu 100.000 (98.776)\n",
      "Epoch-69   80 batches\tloss 0.7802 (0.8155)\taccu 100.000 (98.789)\n",
      "Epoch-69  100 batches\tloss 0.8259 (0.8182)\taccu 100.000 (98.594)\n",
      "Epoch-69  120 batches\tloss 0.8174 (0.8193)\taccu 98.438 (98.542)\n",
      "Epoch-69  140 batches\tloss 0.8302 (0.8191)\taccu 96.875 (98.527)\n",
      "Epoch-69  160 batches\tloss 0.8212 (0.8193)\taccu 98.438 (98.477)\n",
      "Epoch-69  180 batches\tloss 0.8372 (0.8190)\taccu 98.438 (98.472)\n",
      "Epoch-69  200 batches\tloss 0.7980 (0.8192)\taccu 98.438 (98.430)\n",
      "Epoch-69  220 batches\tloss 0.8317 (0.8183)\taccu 98.438 (98.473)\n",
      "Epoch-69  240 batches\tloss 0.7830 (0.8190)\taccu 100.000 (98.464)\n",
      "Epoch-69  260 batches\tloss 0.7972 (0.8187)\taccu 100.000 (98.450)\n",
      "Epoch-69  280 batches\tloss 0.8332 (0.8187)\taccu 96.875 (98.465)\n",
      "Epoch-69  300 batches\tloss 0.8056 (0.8184)\taccu 98.438 (98.479)\n",
      "Epoch-69  320 batches\tloss 0.8501 (0.8184)\taccu 98.438 (98.486)\n",
      "Epoch-69  340 batches\tloss 0.8809 (0.8194)\taccu 95.312 (98.465)\n",
      "Epoch-69  360 batches\tloss 0.8100 (0.8196)\taccu 100.000 (98.451)\n",
      "Epoch-69  380 batches\tloss 0.7769 (0.8195)\taccu 100.000 (98.450)\n",
      "Epoch-69  400 batches\tloss 0.8003 (0.8195)\taccu 100.000 (98.461)\n",
      "Epoch-69  420 batches\tloss 0.8103 (0.8196)\taccu 98.438 (98.460)\n",
      "Epoch-69  440 batches\tloss 0.8552 (0.8192)\taccu 98.438 (98.477)\n",
      "Epoch-69  460 batches\tloss 0.8046 (0.8196)\taccu 100.000 (98.495)\n",
      "Epoch-69  480 batches\tloss 0.8099 (0.8200)\taccu 98.438 (98.483)\n",
      "Epoch-69  89.0s\tTrain: loss 0.8204\taccu 98.4754\tValid: loss 0.9887\taccu 91.7895\n",
      "Epoch 69: val_acc did not improve\n",
      "69 0.0001\n",
      "Epoch-70   20 batches\tloss 0.7752 (0.8025)\taccu 100.000 (98.516)\n",
      "Epoch-70   40 batches\tloss 0.7778 (0.8129)\taccu 100.000 (98.281)\n",
      "Epoch-70   60 batches\tloss 0.8556 (0.8109)\taccu 96.875 (98.542)\n",
      "Epoch-70   80 batches\tloss 0.8617 (0.8125)\taccu 96.875 (98.535)\n",
      "Epoch-70  100 batches\tloss 0.8524 (0.8135)\taccu 98.438 (98.547)\n",
      "Epoch-70  120 batches\tloss 0.8354 (0.8149)\taccu 96.875 (98.516)\n",
      "Epoch-70  140 batches\tloss 0.7739 (0.8167)\taccu 100.000 (98.460)\n",
      "Epoch-70  160 batches\tloss 0.8341 (0.8167)\taccu 96.875 (98.477)\n",
      "Epoch-70  180 batches\tloss 0.8378 (0.8173)\taccu 100.000 (98.472)\n",
      "Epoch-70  200 batches\tloss 0.7837 (0.8176)\taccu 100.000 (98.492)\n",
      "Epoch-70  220 batches\tloss 0.8273 (0.8179)\taccu 96.875 (98.494)\n",
      "Epoch-70  240 batches\tloss 0.8023 (0.8179)\taccu 98.438 (98.490)\n",
      "Epoch-70  260 batches\tloss 0.8720 (0.8190)\taccu 96.875 (98.462)\n",
      "Epoch-70  280 batches\tloss 0.7842 (0.8187)\taccu 100.000 (98.488)\n",
      "Epoch-70  300 batches\tloss 0.8087 (0.8181)\taccu 98.438 (98.510)\n",
      "Epoch-70  320 batches\tloss 0.8397 (0.8183)\taccu 96.875 (98.511)\n",
      "Epoch-70  340 batches\tloss 0.7873 (0.8180)\taccu 100.000 (98.539)\n",
      "Epoch-70  360 batches\tloss 0.8359 (0.8181)\taccu 96.875 (98.550)\n",
      "Epoch-70  380 batches\tloss 0.7910 (0.8183)\taccu 98.438 (98.540)\n",
      "Epoch-70  400 batches\tloss 0.7656 (0.8184)\taccu 100.000 (98.527)\n",
      "Epoch-70  420 batches\tloss 0.7989 (0.8186)\taccu 98.438 (98.519)\n",
      "Epoch-70  440 batches\tloss 0.8951 (0.8184)\taccu 93.750 (98.516)\n",
      "Epoch-70  460 batches\tloss 0.7702 (0.8190)\taccu 100.000 (98.505)\n",
      "Epoch-70  480 batches\tloss 0.9710 (0.8195)\taccu 93.750 (98.516)\n",
      "Epoch-70  89.3s\tTrain: loss 0.8198\taccu 98.5006\tValid: loss 0.9924\taccu 91.5979\n",
      "Epoch 70: val_acc did not improve\n",
      "70 0.0001\n",
      "Epoch-71   20 batches\tloss 0.7965 (0.8027)\taccu 98.438 (98.672)\n",
      "Epoch-71   40 batches\tloss 0.8797 (0.8066)\taccu 96.875 (98.750)\n",
      "Epoch-71   60 batches\tloss 0.7973 (0.8174)\taccu 100.000 (98.516)\n",
      "Epoch-71   80 batches\tloss 0.8107 (0.8161)\taccu 98.438 (98.652)\n",
      "Epoch-71  100 batches\tloss 0.8278 (0.8156)\taccu 96.875 (98.609)\n",
      "Epoch-71  120 batches\tloss 0.8556 (0.8144)\taccu 96.875 (98.646)\n",
      "Epoch-71  140 batches\tloss 0.8256 (0.8134)\taccu 98.438 (98.739)\n",
      "Epoch-71  160 batches\tloss 0.8137 (0.8152)\taccu 98.438 (98.652)\n",
      "Epoch-71  180 batches\tloss 0.7750 (0.8149)\taccu 100.000 (98.707)\n",
      "Epoch-71  200 batches\tloss 0.8043 (0.8154)\taccu 100.000 (98.656)\n",
      "Epoch-71  220 batches\tloss 0.8218 (0.8153)\taccu 98.438 (98.665)\n",
      "Epoch-71  240 batches\tloss 0.7830 (0.8148)\taccu 100.000 (98.639)\n",
      "Epoch-71  260 batches\tloss 0.7931 (0.8147)\taccu 100.000 (98.642)\n",
      "Epoch-71  280 batches\tloss 0.7943 (0.8145)\taccu 100.000 (98.683)\n",
      "Epoch-71  300 batches\tloss 0.7647 (0.8135)\taccu 100.000 (98.703)\n",
      "Epoch-71  320 batches\tloss 0.8074 (0.8135)\taccu 98.438 (98.696)\n",
      "Epoch-71  340 batches\tloss 0.7888 (0.8132)\taccu 100.000 (98.718)\n",
      "Epoch-71  360 batches\tloss 0.7871 (0.8127)\taccu 98.438 (98.733)\n",
      "Epoch-71  380 batches\tloss 0.8226 (0.8127)\taccu 96.875 (98.729)\n",
      "Epoch-71  400 batches\tloss 0.7890 (0.8132)\taccu 98.438 (98.715)\n",
      "Epoch-71  420 batches\tloss 0.7793 (0.8136)\taccu 100.000 (98.694)\n",
      "Epoch-71  440 batches\tloss 0.8776 (0.8139)\taccu 96.875 (98.672)\n",
      "Epoch-71  460 batches\tloss 0.7968 (0.8139)\taccu 98.438 (98.679)\n",
      "Epoch-71  480 batches\tloss 0.8299 (0.8143)\taccu 96.875 (98.659)\n",
      "Epoch-71  89.0s\tTrain: loss 0.8145\taccu 98.6490\tValid: loss 0.9836\taccu 91.8337\n",
      "Epoch 71: val_acc did not improve\n",
      "71 0.0001\n",
      "Epoch-72   20 batches\tloss 0.8793 (0.8225)\taccu 92.188 (98.438)\n",
      "Epoch-72   40 batches\tloss 0.7984 (0.8233)\taccu 98.438 (98.125)\n",
      "Epoch-72   60 batches\tloss 0.8195 (0.8193)\taccu 96.875 (98.333)\n",
      "Epoch-72   80 batches\tloss 0.8544 (0.8170)\taccu 95.312 (98.418)\n",
      "Epoch-72  100 batches\tloss 0.7740 (0.8175)\taccu 100.000 (98.438)\n",
      "Epoch-72  120 batches\tloss 0.7792 (0.8163)\taccu 100.000 (98.529)\n",
      "Epoch-72  140 batches\tloss 0.9023 (0.8176)\taccu 93.750 (98.415)\n",
      "Epoch-72  160 batches\tloss 0.7916 (0.8181)\taccu 100.000 (98.418)\n",
      "Epoch-72  180 batches\tloss 0.8654 (0.8175)\taccu 96.875 (98.429)\n",
      "Epoch-72  200 batches\tloss 0.8519 (0.8178)\taccu 96.875 (98.438)\n",
      "Epoch-72  220 batches\tloss 0.8018 (0.8181)\taccu 100.000 (98.452)\n",
      "Epoch-72  240 batches\tloss 0.8029 (0.8178)\taccu 100.000 (98.483)\n",
      "Epoch-72  260 batches\tloss 0.8066 (0.8187)\taccu 98.438 (98.425)\n",
      "Epoch-72  280 batches\tloss 0.8504 (0.8197)\taccu 98.438 (98.376)\n",
      "Epoch-72  300 batches\tloss 0.8009 (0.8186)\taccu 100.000 (98.443)\n",
      "Epoch-72  320 batches\tloss 0.8292 (0.8183)\taccu 100.000 (98.452)\n",
      "Epoch-72  340 batches\tloss 0.8597 (0.8185)\taccu 95.312 (98.428)\n",
      "Epoch-72  360 batches\tloss 0.8578 (0.8186)\taccu 96.875 (98.438)\n",
      "Epoch-72  380 batches\tloss 0.7827 (0.8188)\taccu 100.000 (98.438)\n",
      "Epoch-72  400 batches\tloss 0.7805 (0.8184)\taccu 98.438 (98.453)\n",
      "Epoch-72  420 batches\tloss 0.8170 (0.8183)\taccu 98.438 (98.464)\n",
      "Epoch-72  440 batches\tloss 0.7999 (0.8187)\taccu 100.000 (98.448)\n",
      "Epoch-72  460 batches\tloss 0.7936 (0.8182)\taccu 100.000 (98.458)\n",
      "Epoch-72  480 batches\tloss 0.8179 (0.8180)\taccu 98.438 (98.467)\n",
      "Epoch-72  89.3s\tTrain: loss 0.8182\taccu 98.4438\tValid: loss 0.9829\taccu 91.8632\n",
      "Epoch 72: val_acc did not improve\n",
      "72 0.0001\n",
      "Epoch-73   20 batches\tloss 0.8265 (0.8194)\taccu 98.438 (98.359)\n",
      "Epoch-73   40 batches\tloss 0.7914 (0.8182)\taccu 100.000 (98.516)\n",
      "Epoch-73   60 batches\tloss 0.7857 (0.8161)\taccu 100.000 (98.620)\n",
      "Epoch-73   80 batches\tloss 0.8686 (0.8141)\taccu 98.438 (98.691)\n",
      "Epoch-73  100 batches\tloss 0.7847 (0.8146)\taccu 100.000 (98.672)\n",
      "Epoch-73  120 batches\tloss 0.8094 (0.8148)\taccu 98.438 (98.581)\n",
      "Epoch-73  140 batches\tloss 0.8726 (0.8139)\taccu 95.312 (98.583)\n",
      "Epoch-73  160 batches\tloss 0.8447 (0.8131)\taccu 98.438 (98.594)\n",
      "Epoch-73  180 batches\tloss 0.8180 (0.8136)\taccu 100.000 (98.568)\n",
      "Epoch-73  200 batches\tloss 0.8074 (0.8144)\taccu 100.000 (98.602)\n",
      "Epoch-73  220 batches\tloss 0.8612 (0.8156)\taccu 98.438 (98.530)\n",
      "Epoch-73  240 batches\tloss 0.7918 (0.8163)\taccu 100.000 (98.483)\n",
      "Epoch-73  260 batches\tloss 0.8067 (0.8167)\taccu 98.438 (98.486)\n",
      "Epoch-73  280 batches\tloss 0.8487 (0.8164)\taccu 100.000 (98.516)\n",
      "Epoch-73  300 batches\tloss 0.8039 (0.8157)\taccu 98.438 (98.552)\n",
      "Epoch-73  320 batches\tloss 0.8200 (0.8160)\taccu 96.875 (98.535)\n",
      "Epoch-73  340 batches\tloss 0.8015 (0.8163)\taccu 100.000 (98.520)\n",
      "Epoch-73  360 batches\tloss 0.8078 (0.8162)\taccu 98.438 (98.511)\n",
      "Epoch-73  380 batches\tloss 0.8145 (0.8164)\taccu 98.438 (98.507)\n",
      "Epoch-73  400 batches\tloss 0.8393 (0.8166)\taccu 96.875 (98.496)\n",
      "Epoch-73  420 batches\tloss 0.7976 (0.8167)\taccu 100.000 (98.504)\n",
      "Epoch-73  440 batches\tloss 0.8378 (0.8165)\taccu 98.438 (98.512)\n",
      "Epoch-73  460 batches\tloss 0.7967 (0.8162)\taccu 100.000 (98.539)\n",
      "Epoch-73  480 batches\tloss 0.8212 (0.8164)\taccu 100.000 (98.542)\n",
      "Epoch-73  89.0s\tTrain: loss 0.8162\taccu 98.5480\tValid: loss 0.9895\taccu 91.5537\n",
      "Epoch 73: val_acc did not improve\n",
      "73 0.0001\n",
      "Epoch-74   20 batches\tloss 0.7758 (0.8041)\taccu 100.000 (99.375)\n",
      "Epoch-74   40 batches\tloss 0.8374 (0.8059)\taccu 96.875 (98.945)\n",
      "Epoch-74   60 batches\tloss 0.8095 (0.8082)\taccu 98.438 (98.802)\n",
      "Epoch-74   80 batches\tloss 0.8130 (0.8080)\taccu 96.875 (98.848)\n",
      "Epoch-74  100 batches\tloss 0.8277 (0.8094)\taccu 98.438 (98.797)\n",
      "Epoch-74  120 batches\tloss 0.7754 (0.8095)\taccu 100.000 (98.763)\n",
      "Epoch-74  140 batches\tloss 0.7899 (0.8109)\taccu 100.000 (98.705)\n",
      "Epoch-74  160 batches\tloss 0.7741 (0.8103)\taccu 100.000 (98.711)\n",
      "Epoch-74  180 batches\tloss 0.8391 (0.8101)\taccu 96.875 (98.707)\n",
      "Epoch-74  200 batches\tloss 0.8744 (0.8099)\taccu 95.312 (98.695)\n",
      "Epoch-74  220 batches\tloss 0.8415 (0.8104)\taccu 100.000 (98.693)\n",
      "Epoch-74  240 batches\tloss 0.7783 (0.8100)\taccu 100.000 (98.698)\n",
      "Epoch-74  260 batches\tloss 0.7897 (0.8101)\taccu 100.000 (98.726)\n",
      "Epoch-74  280 batches\tloss 0.8349 (0.8099)\taccu 98.438 (98.722)\n",
      "Epoch-74  300 batches\tloss 0.8394 (0.8106)\taccu 98.438 (98.693)\n",
      "Epoch-74  320 batches\tloss 0.8118 (0.8107)\taccu 100.000 (98.706)\n",
      "Epoch-74  340 batches\tloss 0.7751 (0.8113)\taccu 100.000 (98.681)\n",
      "Epoch-74  360 batches\tloss 0.8229 (0.8118)\taccu 98.438 (98.659)\n",
      "Epoch-74  380 batches\tloss 0.8099 (0.8117)\taccu 96.875 (98.643)\n",
      "Epoch-74  400 batches\tloss 0.7933 (0.8119)\taccu 100.000 (98.645)\n",
      "Epoch-74  420 batches\tloss 0.7930 (0.8117)\taccu 100.000 (98.672)\n",
      "Epoch-74  440 batches\tloss 0.7961 (0.8120)\taccu 100.000 (98.675)\n",
      "Epoch-74  460 batches\tloss 0.8178 (0.8126)\taccu 98.438 (98.665)\n",
      "Epoch-74  480 batches\tloss 0.8407 (0.8126)\taccu 96.875 (98.662)\n",
      "Epoch-74  89.2s\tTrain: loss 0.8127\taccu 98.6458\tValid: loss 0.9862\taccu 91.7748\n",
      "Epoch 74: val_acc did not improve\n",
      "74 0.0001\n",
      "Epoch-75   20 batches\tloss 0.8520 (0.8197)\taccu 96.875 (98.281)\n",
      "Epoch-75   40 batches\tloss 0.7893 (0.8159)\taccu 100.000 (98.594)\n",
      "Epoch-75   60 batches\tloss 0.7705 (0.8142)\taccu 100.000 (98.698)\n",
      "Epoch-75   80 batches\tloss 0.8623 (0.8152)\taccu 96.875 (98.633)\n",
      "Epoch-75  100 batches\tloss 0.7914 (0.8134)\taccu 100.000 (98.734)\n",
      "Epoch-75  120 batches\tloss 0.8184 (0.8127)\taccu 98.438 (98.724)\n",
      "Epoch-75  140 batches\tloss 0.8818 (0.8125)\taccu 95.312 (98.717)\n",
      "Epoch-75  160 batches\tloss 0.8433 (0.8137)\taccu 96.875 (98.682)\n",
      "Epoch-75  180 batches\tloss 0.8433 (0.8134)\taccu 98.438 (98.663)\n",
      "Epoch-75  200 batches\tloss 0.8456 (0.8133)\taccu 96.875 (98.656)\n",
      "Epoch-75  220 batches\tloss 0.7848 (0.8128)\taccu 100.000 (98.665)\n",
      "Epoch-75  240 batches\tloss 0.8028 (0.8133)\taccu 98.438 (98.620)\n",
      "Epoch-75  260 batches\tloss 0.8531 (0.8142)\taccu 95.312 (98.582)\n",
      "Epoch-75  280 batches\tloss 0.8283 (0.8138)\taccu 96.875 (98.588)\n",
      "Epoch-75  300 batches\tloss 0.7900 (0.8136)\taccu 100.000 (98.562)\n",
      "Epoch-75  320 batches\tloss 0.7999 (0.8135)\taccu 100.000 (98.540)\n",
      "Epoch-75  340 batches\tloss 0.7719 (0.8129)\taccu 100.000 (98.580)\n",
      "Epoch-75  360 batches\tloss 0.8270 (0.8135)\taccu 98.438 (98.581)\n",
      "Epoch-75  380 batches\tloss 0.7904 (0.8131)\taccu 100.000 (98.586)\n",
      "Epoch-75  400 batches\tloss 0.8170 (0.8131)\taccu 98.438 (98.605)\n",
      "Epoch-75  420 batches\tloss 0.7969 (0.8131)\taccu 98.438 (98.597)\n",
      "Epoch-75  440 batches\tloss 0.7983 (0.8136)\taccu 100.000 (98.583)\n",
      "Epoch-75  460 batches\tloss 0.8028 (0.8138)\taccu 100.000 (98.587)\n",
      "Epoch-75  480 batches\tloss 0.8363 (0.8135)\taccu 98.438 (98.604)\n",
      "Epoch-75  89.2s\tTrain: loss 0.8141\taccu 98.5827\tValid: loss 0.9871\taccu 91.8042\n",
      "Epoch 75: val_acc did not improve\n",
      "75 0.0001\n",
      "Epoch-76   20 batches\tloss 0.7653 (0.8004)\taccu 100.000 (98.984)\n",
      "Epoch-76   40 batches\tloss 0.8197 (0.8001)\taccu 98.438 (99.102)\n",
      "Epoch-76   60 batches\tloss 0.8088 (0.8027)\taccu 98.438 (99.036)\n",
      "Epoch-76   80 batches\tloss 0.8808 (0.8050)\taccu 95.312 (98.945)\n",
      "Epoch-76  100 batches\tloss 0.7811 (0.8048)\taccu 100.000 (98.938)\n",
      "Epoch-76  120 batches\tloss 0.8226 (0.8054)\taccu 98.438 (98.893)\n",
      "Epoch-76  140 batches\tloss 0.7845 (0.8081)\taccu 100.000 (98.795)\n",
      "Epoch-76  160 batches\tloss 0.8711 (0.8084)\taccu 95.312 (98.770)\n",
      "Epoch-76  180 batches\tloss 0.7828 (0.8082)\taccu 100.000 (98.741)\n",
      "Epoch-76  200 batches\tloss 0.7902 (0.8094)\taccu 98.438 (98.727)\n",
      "Epoch-76  220 batches\tloss 0.7683 (0.8095)\taccu 100.000 (98.714)\n",
      "Epoch-76  240 batches\tloss 0.8146 (0.8092)\taccu 98.438 (98.724)\n",
      "Epoch-76  260 batches\tloss 0.7782 (0.8087)\taccu 100.000 (98.750)\n",
      "Epoch-76  280 batches\tloss 0.8050 (0.8090)\taccu 100.000 (98.744)\n",
      "Epoch-76  300 batches\tloss 0.7981 (0.8098)\taccu 98.438 (98.693)\n",
      "Epoch-76  320 batches\tloss 0.7938 (0.8093)\taccu 98.438 (98.711)\n",
      "Epoch-76  340 batches\tloss 0.8102 (0.8092)\taccu 96.875 (98.704)\n",
      "Epoch-76  360 batches\tloss 0.7701 (0.8100)\taccu 100.000 (98.681)\n",
      "Epoch-76  380 batches\tloss 0.7807 (0.8102)\taccu 98.438 (98.655)\n",
      "Epoch-76  400 batches\tloss 0.7818 (0.8105)\taccu 100.000 (98.637)\n",
      "Epoch-76  420 batches\tloss 0.8000 (0.8109)\taccu 100.000 (98.627)\n",
      "Epoch-76  440 batches\tloss 0.7955 (0.8109)\taccu 100.000 (98.633)\n",
      "Epoch-76  460 batches\tloss 0.8323 (0.8112)\taccu 98.438 (98.638)\n",
      "Epoch-76  480 batches\tloss 0.7822 (0.8118)\taccu 100.000 (98.607)\n",
      "Epoch-76  89.1s\tTrain: loss 0.8117\taccu 98.5953\tValid: loss 0.9844\taccu 91.9074\n",
      "Epoch 76: val_acc did not improve\n",
      "76 0.0001\n",
      "Epoch-77   20 batches\tloss 0.8179 (0.8005)\taccu 98.438 (98.672)\n",
      "Epoch-77   40 batches\tloss 0.8779 (0.8101)\taccu 96.875 (98.516)\n",
      "Epoch-77   60 batches\tloss 0.8378 (0.8088)\taccu 96.875 (98.646)\n",
      "Epoch-77   80 batches\tloss 0.8706 (0.8089)\taccu 95.312 (98.672)\n",
      "Epoch-77  100 batches\tloss 0.8088 (0.8091)\taccu 100.000 (98.719)\n",
      "Epoch-77  120 batches\tloss 0.7885 (0.8107)\taccu 100.000 (98.555)\n",
      "Epoch-77  140 batches\tloss 0.7843 (0.8098)\taccu 100.000 (98.594)\n",
      "Epoch-77  160 batches\tloss 0.8277 (0.8101)\taccu 96.875 (98.564)\n",
      "Epoch-77  180 batches\tloss 0.8161 (0.8096)\taccu 100.000 (98.628)\n",
      "Epoch-77  200 batches\tloss 0.7734 (0.8090)\taccu 100.000 (98.633)\n",
      "Epoch-77  220 batches\tloss 0.7966 (0.8091)\taccu 100.000 (98.665)\n",
      "Epoch-77  240 batches\tloss 0.7780 (0.8096)\taccu 100.000 (98.665)\n",
      "Epoch-77  260 batches\tloss 0.8465 (0.8099)\taccu 98.438 (98.672)\n",
      "Epoch-77  280 batches\tloss 0.8312 (0.8094)\taccu 98.438 (98.683)\n",
      "Epoch-77  300 batches\tloss 0.8011 (0.8095)\taccu 100.000 (98.693)\n",
      "Epoch-77  320 batches\tloss 0.7790 (0.8094)\taccu 100.000 (98.701)\n",
      "Epoch-77  340 batches\tloss 0.9098 (0.8094)\taccu 95.312 (98.713)\n",
      "Epoch-77  360 batches\tloss 0.8281 (0.8094)\taccu 100.000 (98.728)\n",
      "Epoch-77  380 batches\tloss 0.7967 (0.8088)\taccu 98.438 (98.746)\n",
      "Epoch-77  400 batches\tloss 0.7867 (0.8091)\taccu 100.000 (98.738)\n",
      "Epoch-77  420 batches\tloss 0.7677 (0.8089)\taccu 100.000 (98.757)\n",
      "Epoch-77  440 batches\tloss 0.8084 (0.8088)\taccu 98.438 (98.764)\n",
      "Epoch-77  460 batches\tloss 0.7987 (0.8089)\taccu 98.438 (98.774)\n",
      "Epoch-77  480 batches\tloss 0.8167 (0.8093)\taccu 98.438 (98.773)\n",
      "Epoch-77  88.8s\tTrain: loss 0.8092\taccu 98.7879\tValid: loss 0.9849\taccu 91.8485\n",
      "Epoch 77: val_acc did not improve\n",
      "77 0.0001\n",
      "Epoch-78   20 batches\tloss 0.7956 (0.8007)\taccu 100.000 (98.906)\n",
      "Epoch-78   40 batches\tloss 0.7842 (0.8013)\taccu 100.000 (98.984)\n",
      "Epoch-78   60 batches\tloss 0.8071 (0.8055)\taccu 98.438 (98.802)\n",
      "Epoch-78   80 batches\tloss 0.7899 (0.8056)\taccu 100.000 (98.867)\n",
      "Epoch-78  100 batches\tloss 0.8157 (0.8084)\taccu 98.438 (98.766)\n",
      "Epoch-78  120 batches\tloss 0.8007 (0.8083)\taccu 100.000 (98.776)\n",
      "Epoch-78  140 batches\tloss 0.7818 (0.8076)\taccu 100.000 (98.761)\n",
      "Epoch-78  160 batches\tloss 0.7710 (0.8084)\taccu 100.000 (98.730)\n",
      "Epoch-78  180 batches\tloss 0.8635 (0.8083)\taccu 98.438 (98.750)\n",
      "Epoch-78  200 batches\tloss 0.7936 (0.8085)\taccu 100.000 (98.742)\n",
      "Epoch-78  220 batches\tloss 0.7703 (0.8082)\taccu 100.000 (98.736)\n",
      "Epoch-78  240 batches\tloss 0.8134 (0.8075)\taccu 98.438 (98.770)\n",
      "Epoch-78  260 batches\tloss 0.7849 (0.8075)\taccu 100.000 (98.762)\n",
      "Epoch-78  280 batches\tloss 0.8380 (0.8082)\taccu 96.875 (98.744)\n",
      "Epoch-78  300 batches\tloss 0.7592 (0.8080)\taccu 100.000 (98.776)\n",
      "Epoch-78  320 batches\tloss 0.8436 (0.8086)\taccu 100.000 (98.799)\n",
      "Epoch-78  340 batches\tloss 0.8489 (0.8086)\taccu 98.438 (98.782)\n",
      "Epoch-78  360 batches\tloss 0.7811 (0.8093)\taccu 100.000 (98.798)\n",
      "Epoch-78  380 batches\tloss 0.8152 (0.8100)\taccu 100.000 (98.779)\n",
      "Epoch-78  400 batches\tloss 0.7774 (0.8102)\taccu 100.000 (98.742)\n",
      "Epoch-78  420 batches\tloss 0.8079 (0.8103)\taccu 100.000 (98.750)\n",
      "Epoch-78  440 batches\tloss 0.7954 (0.8104)\taccu 100.000 (98.750)\n",
      "Epoch-78  460 batches\tloss 0.7673 (0.8106)\taccu 100.000 (98.747)\n",
      "Epoch-78  480 batches\tloss 0.8291 (0.8104)\taccu 96.875 (98.747)\n",
      "Epoch-78  89.9s\tTrain: loss 0.8102\taccu 98.7437\tValid: loss 0.9851\taccu 92.0843\n",
      "Epoch 78: val_acc improved from 91.9517 to 92.0843, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "78 0.0001\n",
      "Epoch-79   20 batches\tloss 0.8133 (0.8107)\taccu 100.000 (98.750)\n",
      "Epoch-79   40 batches\tloss 0.7924 (0.8158)\taccu 100.000 (98.398)\n",
      "Epoch-79   60 batches\tloss 0.7942 (0.8130)\taccu 100.000 (98.594)\n",
      "Epoch-79   80 batches\tloss 0.7766 (0.8098)\taccu 100.000 (98.789)\n",
      "Epoch-79  100 batches\tloss 0.7671 (0.8070)\taccu 100.000 (98.859)\n",
      "Epoch-79  120 batches\tloss 0.7694 (0.8072)\taccu 100.000 (98.854)\n",
      "Epoch-79  140 batches\tloss 0.7831 (0.8058)\taccu 100.000 (98.862)\n",
      "Epoch-79  160 batches\tloss 0.8237 (0.8071)\taccu 98.438 (98.809)\n",
      "Epoch-79  180 batches\tloss 0.8035 (0.8061)\taccu 100.000 (98.828)\n",
      "Epoch-79  200 batches\tloss 0.7568 (0.8061)\taccu 100.000 (98.797)\n",
      "Epoch-79  220 batches\tloss 0.7924 (0.8062)\taccu 100.000 (98.786)\n",
      "Epoch-79  240 batches\tloss 0.7625 (0.8072)\taccu 100.000 (98.770)\n",
      "Epoch-79  260 batches\tloss 0.7648 (0.8065)\taccu 100.000 (98.786)\n",
      "Epoch-79  280 batches\tloss 0.7695 (0.8063)\taccu 100.000 (98.817)\n",
      "Epoch-79  300 batches\tloss 0.8151 (0.8078)\taccu 98.438 (98.776)\n",
      "Epoch-79  320 batches\tloss 0.7879 (0.8075)\taccu 100.000 (98.784)\n",
      "Epoch-79  340 batches\tloss 0.7872 (0.8077)\taccu 100.000 (98.805)\n",
      "Epoch-79  360 batches\tloss 0.8138 (0.8075)\taccu 100.000 (98.811)\n",
      "Epoch-79  380 batches\tloss 0.8080 (0.8073)\taccu 98.438 (98.808)\n",
      "Epoch-79  400 batches\tloss 0.7974 (0.8068)\taccu 98.438 (98.828)\n",
      "Epoch-79  420 batches\tloss 0.8082 (0.8067)\taccu 100.000 (98.850)\n",
      "Epoch-79  440 batches\tloss 0.8388 (0.8069)\taccu 96.875 (98.849)\n",
      "Epoch-79  460 batches\tloss 0.7921 (0.8072)\taccu 98.438 (98.835)\n",
      "Epoch-79  480 batches\tloss 0.8188 (0.8072)\taccu 98.438 (98.838)\n",
      "Epoch-79  89.0s\tTrain: loss 0.8075\taccu 98.8352\tValid: loss 0.9885\taccu 91.9517\n",
      "Epoch 79: val_acc did not improve\n",
      "79 0.0001\n",
      "Epoch-80   20 batches\tloss 0.7772 (0.7985)\taccu 100.000 (98.984)\n",
      "Epoch-80   40 batches\tloss 0.8682 (0.8070)\taccu 96.875 (98.828)\n",
      "Epoch-80   60 batches\tloss 0.8534 (0.8064)\taccu 100.000 (98.828)\n",
      "Epoch-80   80 batches\tloss 0.7847 (0.8053)\taccu 100.000 (98.828)\n",
      "Epoch-80  100 batches\tloss 0.7857 (0.8042)\taccu 100.000 (98.938)\n",
      "Epoch-80  120 batches\tloss 0.7629 (0.8061)\taccu 100.000 (98.906)\n",
      "Epoch-80  140 batches\tloss 0.7713 (0.8060)\taccu 100.000 (98.873)\n",
      "Epoch-80  160 batches\tloss 0.7989 (0.8056)\taccu 98.438 (98.867)\n",
      "Epoch-80  180 batches\tloss 0.7704 (0.8066)\taccu 100.000 (98.793)\n",
      "Epoch-80  200 batches\tloss 0.8483 (0.8071)\taccu 95.312 (98.812)\n",
      "Epoch-80  220 batches\tloss 0.8052 (0.8064)\taccu 100.000 (98.849)\n",
      "Epoch-80  240 batches\tloss 0.7775 (0.8065)\taccu 100.000 (98.848)\n",
      "Epoch-80  260 batches\tloss 0.8266 (0.8061)\taccu 98.438 (98.864)\n",
      "Epoch-80  280 batches\tloss 0.8164 (0.8061)\taccu 98.438 (98.850)\n",
      "Epoch-80  300 batches\tloss 0.7728 (0.8054)\taccu 100.000 (98.859)\n",
      "Epoch-80  320 batches\tloss 0.7795 (0.8047)\taccu 98.438 (98.887)\n",
      "Epoch-80  340 batches\tloss 0.8595 (0.8047)\taccu 98.438 (98.883)\n",
      "Epoch-80  360 batches\tloss 0.8153 (0.8049)\taccu 98.438 (98.872)\n",
      "Epoch-80  380 batches\tloss 0.7776 (0.8050)\taccu 100.000 (98.873)\n",
      "Epoch-80  400 batches\tloss 0.8435 (0.8054)\taccu 96.875 (98.895)\n",
      "Epoch-80  420 batches\tloss 0.7965 (0.8056)\taccu 98.438 (98.869)\n",
      "Epoch-80  440 batches\tloss 0.8186 (0.8060)\taccu 98.438 (98.860)\n",
      "Epoch-80  460 batches\tloss 0.8019 (0.8064)\taccu 100.000 (98.872)\n",
      "Epoch-80  480 batches\tloss 0.8311 (0.8062)\taccu 96.875 (98.867)\n",
      "Epoch-80  89.1s\tTrain: loss 0.8062\taccu 98.8573\tValid: loss 0.9828\taccu 92.0548\n",
      "Epoch 80: val_acc did not improve\n",
      "80 0.0001\n",
      "Epoch-81   20 batches\tloss 0.8581 (0.8081)\taccu 96.875 (98.672)\n",
      "Epoch-81   40 batches\tloss 0.8283 (0.8040)\taccu 98.438 (98.906)\n",
      "Epoch-81   60 batches\tloss 0.7825 (0.8058)\taccu 100.000 (98.828)\n",
      "Epoch-81   80 batches\tloss 0.7942 (0.8082)\taccu 98.438 (98.730)\n",
      "Epoch-81  100 batches\tloss 0.8341 (0.8074)\taccu 96.875 (98.797)\n",
      "Epoch-81  120 batches\tloss 0.7588 (0.8064)\taccu 100.000 (98.828)\n",
      "Epoch-81  140 batches\tloss 0.8725 (0.8062)\taccu 95.312 (98.906)\n",
      "Epoch-81  160 batches\tloss 0.7955 (0.8072)\taccu 100.000 (98.828)\n",
      "Epoch-81  180 batches\tloss 0.7922 (0.8062)\taccu 100.000 (98.863)\n",
      "Epoch-81  200 batches\tloss 0.7841 (0.8063)\taccu 100.000 (98.867)\n",
      "Epoch-81  220 batches\tloss 0.8382 (0.8066)\taccu 96.875 (98.878)\n",
      "Epoch-81  240 batches\tloss 0.7939 (0.8064)\taccu 100.000 (98.893)\n",
      "Epoch-81  260 batches\tloss 0.7869 (0.8067)\taccu 100.000 (98.828)\n",
      "Epoch-81  280 batches\tloss 0.7618 (0.8058)\taccu 100.000 (98.862)\n",
      "Epoch-81  300 batches\tloss 0.7828 (0.8060)\taccu 98.438 (98.844)\n",
      "Epoch-81  320 batches\tloss 0.7886 (0.8062)\taccu 98.438 (98.823)\n",
      "Epoch-81  340 batches\tloss 0.8300 (0.8056)\taccu 98.438 (98.842)\n",
      "Epoch-81  360 batches\tloss 0.7931 (0.8054)\taccu 100.000 (98.859)\n",
      "Epoch-81  380 batches\tloss 0.7985 (0.8058)\taccu 100.000 (98.853)\n",
      "Epoch-81  400 batches\tloss 0.7788 (0.8056)\taccu 100.000 (98.848)\n",
      "Epoch-81  420 batches\tloss 0.7853 (0.8056)\taccu 100.000 (98.865)\n",
      "Epoch-81  440 batches\tloss 0.7810 (0.8054)\taccu 98.438 (98.846)\n",
      "Epoch-81  460 batches\tloss 0.7878 (0.8048)\taccu 100.000 (98.876)\n",
      "Epoch-81  480 batches\tloss 0.8033 (0.8050)\taccu 98.438 (98.867)\n",
      "Epoch-81  89.6s\tTrain: loss 0.8046\taccu 98.8731\tValid: loss 0.9798\taccu 91.9074\n",
      "Epoch 81: val_acc did not improve\n",
      "81 0.0001\n",
      "Epoch-82   20 batches\tloss 0.7900 (0.7921)\taccu 98.438 (99.297)\n",
      "Epoch-82   40 batches\tloss 0.7601 (0.7990)\taccu 100.000 (99.180)\n",
      "Epoch-82   60 batches\tloss 0.7891 (0.7990)\taccu 100.000 (99.193)\n",
      "Epoch-82   80 batches\tloss 0.8171 (0.7990)\taccu 98.438 (99.141)\n",
      "Epoch-82  100 batches\tloss 0.8113 (0.7988)\taccu 100.000 (99.188)\n",
      "Epoch-82  120 batches\tloss 0.8037 (0.8005)\taccu 98.438 (99.115)\n",
      "Epoch-82  140 batches\tloss 0.8301 (0.8016)\taccu 98.438 (99.029)\n",
      "Epoch-82  160 batches\tloss 0.7967 (0.8017)\taccu 98.438 (99.043)\n",
      "Epoch-82  180 batches\tloss 0.7761 (0.8003)\taccu 100.000 (99.115)\n",
      "Epoch-82  200 batches\tloss 0.8424 (0.8011)\taccu 96.875 (99.109)\n",
      "Epoch-82  220 batches\tloss 0.7550 (0.8012)\taccu 100.000 (99.070)\n",
      "Epoch-82  240 batches\tloss 0.8243 (0.8017)\taccu 95.312 (99.030)\n",
      "Epoch-82  260 batches\tloss 0.8201 (0.8018)\taccu 100.000 (99.050)\n",
      "Epoch-82  280 batches\tloss 0.8418 (0.8025)\taccu 98.438 (99.040)\n",
      "Epoch-82  300 batches\tloss 0.7912 (0.8030)\taccu 100.000 (98.974)\n",
      "Epoch-82  320 batches\tloss 0.8631 (0.8033)\taccu 96.875 (98.965)\n",
      "Epoch-82  340 batches\tloss 0.8120 (0.8034)\taccu 100.000 (98.966)\n",
      "Epoch-82  360 batches\tloss 0.8250 (0.8041)\taccu 96.875 (98.937)\n",
      "Epoch-82  380 batches\tloss 0.8071 (0.8043)\taccu 100.000 (98.927)\n",
      "Epoch-82  400 batches\tloss 0.7574 (0.8050)\taccu 100.000 (98.891)\n",
      "Epoch-82  420 batches\tloss 0.7925 (0.8048)\taccu 98.438 (98.891)\n",
      "Epoch-82  440 batches\tloss 0.8098 (0.8046)\taccu 100.000 (98.899)\n",
      "Epoch-82  460 batches\tloss 0.8479 (0.8045)\taccu 96.875 (98.893)\n",
      "Epoch-82  480 batches\tloss 0.7684 (0.8042)\taccu 100.000 (98.900)\n",
      "Epoch-82  89.0s\tTrain: loss 0.8043\taccu 98.9110\tValid: loss 0.9827\taccu 91.8927\n",
      "Epoch 82: val_acc did not improve\n",
      "82 0.0001\n",
      "Epoch-83   20 batches\tloss 0.7764 (0.7884)\taccu 100.000 (99.219)\n",
      "Epoch-83   40 batches\tloss 0.7867 (0.7952)\taccu 100.000 (99.062)\n",
      "Epoch-83   60 batches\tloss 0.7981 (0.7960)\taccu 100.000 (99.089)\n",
      "Epoch-83   80 batches\tloss 0.7806 (0.7964)\taccu 100.000 (99.082)\n",
      "Epoch-83  100 batches\tloss 0.7800 (0.7993)\taccu 98.438 (98.875)\n",
      "Epoch-83  120 batches\tloss 0.7711 (0.7996)\taccu 100.000 (98.854)\n",
      "Epoch-83  140 batches\tloss 0.8837 (0.8009)\taccu 96.875 (98.850)\n",
      "Epoch-83  160 batches\tloss 0.7954 (0.8013)\taccu 96.875 (98.848)\n",
      "Epoch-83  180 batches\tloss 0.8225 (0.8014)\taccu 98.438 (98.889)\n",
      "Epoch-83  200 batches\tloss 0.8790 (0.8023)\taccu 93.750 (98.852)\n",
      "Epoch-83  220 batches\tloss 0.7900 (0.8022)\taccu 100.000 (98.871)\n",
      "Epoch-83  240 batches\tloss 0.7802 (0.8020)\taccu 100.000 (98.926)\n",
      "Epoch-83  260 batches\tloss 0.8685 (0.8026)\taccu 95.312 (98.918)\n",
      "Epoch-83  280 batches\tloss 0.8095 (0.8023)\taccu 100.000 (98.940)\n",
      "Epoch-83  300 batches\tloss 0.7759 (0.8030)\taccu 100.000 (98.917)\n",
      "Epoch-83  320 batches\tloss 0.7860 (0.8030)\taccu 100.000 (98.940)\n",
      "Epoch-83  340 batches\tloss 0.8051 (0.8033)\taccu 98.438 (98.929)\n",
      "Epoch-83  360 batches\tloss 0.8455 (0.8041)\taccu 96.875 (98.880)\n",
      "Epoch-83  380 batches\tloss 0.8089 (0.8044)\taccu 100.000 (98.877)\n",
      "Epoch-83  400 batches\tloss 0.7794 (0.8047)\taccu 100.000 (98.859)\n",
      "Epoch-83  420 batches\tloss 0.7908 (0.8044)\taccu 98.438 (98.862)\n",
      "Epoch-83  440 batches\tloss 0.7895 (0.8041)\taccu 100.000 (98.871)\n",
      "Epoch-83  460 batches\tloss 0.8523 (0.8040)\taccu 96.875 (98.889)\n",
      "Epoch-83  480 batches\tloss 0.8682 (0.8041)\taccu 95.312 (98.887)\n",
      "Epoch-83  88.9s\tTrain: loss 0.8043\taccu 98.8857\tValid: loss 0.9812\taccu 91.8927\n",
      "Epoch 83: val_acc did not improve\n",
      "83 0.0001\n",
      "Epoch-84   20 batches\tloss 0.9207 (0.8096)\taccu 95.312 (98.516)\n",
      "Epoch-84   40 batches\tloss 0.7876 (0.8020)\taccu 100.000 (98.828)\n",
      "Epoch-84   60 batches\tloss 0.7752 (0.8024)\taccu 100.000 (98.880)\n",
      "Epoch-84   80 batches\tloss 0.8805 (0.8018)\taccu 98.438 (98.965)\n",
      "Epoch-84  100 batches\tloss 0.9413 (0.8048)\taccu 92.188 (98.859)\n",
      "Epoch-84  120 batches\tloss 0.8294 (0.8072)\taccu 96.875 (98.737)\n",
      "Epoch-84  140 batches\tloss 0.7950 (0.8066)\taccu 98.438 (98.728)\n",
      "Epoch-84  160 batches\tloss 0.8152 (0.8063)\taccu 98.438 (98.789)\n",
      "Epoch-84  180 batches\tloss 0.8360 (0.8062)\taccu 98.438 (98.785)\n",
      "Epoch-84  200 batches\tloss 0.8436 (0.8058)\taccu 98.438 (98.805)\n",
      "Epoch-84  220 batches\tloss 0.8072 (0.8044)\taccu 100.000 (98.842)\n",
      "Epoch-84  240 batches\tloss 0.7946 (0.8033)\taccu 98.438 (98.900)\n",
      "Epoch-84  260 batches\tloss 0.7999 (0.8025)\taccu 100.000 (98.918)\n",
      "Epoch-84  280 batches\tloss 0.7835 (0.8026)\taccu 98.438 (98.923)\n",
      "Epoch-84  300 batches\tloss 0.8107 (0.8032)\taccu 98.438 (98.901)\n",
      "Epoch-84  320 batches\tloss 0.8146 (0.8029)\taccu 98.438 (98.911)\n",
      "Epoch-84  340 batches\tloss 0.8047 (0.8031)\taccu 98.438 (98.897)\n",
      "Epoch-84  360 batches\tloss 0.7774 (0.8033)\taccu 100.000 (98.898)\n",
      "Epoch-84  380 batches\tloss 0.8106 (0.8036)\taccu 98.438 (98.873)\n",
      "Epoch-84  400 batches\tloss 0.8316 (0.8036)\taccu 96.875 (98.871)\n",
      "Epoch-84  420 batches\tloss 0.7825 (0.8034)\taccu 100.000 (98.884)\n",
      "Epoch-84  440 batches\tloss 0.7843 (0.8036)\taccu 100.000 (98.871)\n",
      "Epoch-84  460 batches\tloss 0.8646 (0.8036)\taccu 96.875 (98.865)\n",
      "Epoch-84  480 batches\tloss 0.7941 (0.8035)\taccu 100.000 (98.854)\n",
      "Epoch-84  88.7s\tTrain: loss 0.8037\taccu 98.8447\tValid: loss 0.9823\taccu 92.0106\n",
      "Epoch 84: val_acc did not improve\n",
      "84 0.0001\n",
      "Epoch-85   20 batches\tloss 0.8040 (0.7941)\taccu 98.438 (98.906)\n",
      "Epoch-85   40 batches\tloss 0.7767 (0.7942)\taccu 100.000 (98.945)\n",
      "Epoch-85   60 batches\tloss 0.7783 (0.7953)\taccu 98.438 (99.010)\n",
      "Epoch-85   80 batches\tloss 0.7927 (0.7971)\taccu 100.000 (98.945)\n",
      "Epoch-85  100 batches\tloss 0.7878 (0.7981)\taccu 100.000 (98.953)\n",
      "Epoch-85  120 batches\tloss 0.7878 (0.7976)\taccu 100.000 (98.971)\n",
      "Epoch-85  140 batches\tloss 0.7721 (0.7965)\taccu 100.000 (98.984)\n",
      "Epoch-85  160 batches\tloss 0.7799 (0.7973)\taccu 100.000 (98.955)\n",
      "Epoch-85  180 batches\tloss 0.7838 (0.7977)\taccu 98.438 (98.958)\n",
      "Epoch-85  200 batches\tloss 0.8121 (0.7983)\taccu 98.438 (98.922)\n",
      "Epoch-85  220 batches\tloss 0.7589 (0.7983)\taccu 100.000 (98.928)\n",
      "Epoch-85  240 batches\tloss 0.7900 (0.7977)\taccu 100.000 (98.971)\n",
      "Epoch-85  260 batches\tloss 0.7856 (0.7988)\taccu 98.438 (98.888)\n",
      "Epoch-85  280 batches\tloss 0.8386 (0.7993)\taccu 100.000 (98.890)\n",
      "Epoch-85  300 batches\tloss 0.8272 (0.8002)\taccu 98.438 (98.880)\n",
      "Epoch-85  320 batches\tloss 0.7858 (0.8006)\taccu 98.438 (98.877)\n",
      "Epoch-85  340 batches\tloss 0.8468 (0.8002)\taccu 100.000 (98.906)\n",
      "Epoch-85  360 batches\tloss 0.7842 (0.8006)\taccu 100.000 (98.902)\n",
      "Epoch-85  380 batches\tloss 0.8285 (0.8005)\taccu 98.438 (98.914)\n",
      "Epoch-85  400 batches\tloss 0.7972 (0.8005)\taccu 100.000 (98.918)\n",
      "Epoch-85  420 batches\tloss 0.7848 (0.8012)\taccu 100.000 (98.899)\n",
      "Epoch-85  440 batches\tloss 0.7793 (0.8008)\taccu 98.438 (98.903)\n",
      "Epoch-85  460 batches\tloss 0.7806 (0.8008)\taccu 100.000 (98.910)\n",
      "Epoch-85  480 batches\tloss 0.7766 (0.8009)\taccu 100.000 (98.913)\n",
      "Epoch-85  88.6s\tTrain: loss 0.8008\taccu 98.9299\tValid: loss 0.9826\taccu 91.7895\n",
      "Epoch 85: val_acc did not improve\n",
      "85 0.0001\n",
      "Epoch-86   20 batches\tloss 0.7960 (0.8099)\taccu 100.000 (98.828)\n",
      "Epoch-86   40 batches\tloss 0.7900 (0.8014)\taccu 100.000 (99.062)\n",
      "Epoch-86   60 batches\tloss 0.8027 (0.7984)\taccu 100.000 (99.219)\n",
      "Epoch-86   80 batches\tloss 0.7668 (0.7999)\taccu 100.000 (98.965)\n",
      "Epoch-86  100 batches\tloss 0.8073 (0.7994)\taccu 98.438 (98.953)\n",
      "Epoch-86  120 batches\tloss 0.7753 (0.7996)\taccu 100.000 (98.945)\n",
      "Epoch-86  140 batches\tloss 0.7976 (0.7994)\taccu 100.000 (99.018)\n",
      "Epoch-86  160 batches\tloss 0.8794 (0.8022)\taccu 96.875 (98.936)\n",
      "Epoch-86  180 batches\tloss 0.8283 (0.8011)\taccu 98.438 (98.958)\n",
      "Epoch-86  200 batches\tloss 0.8223 (0.8012)\taccu 96.875 (98.977)\n",
      "Epoch-86  220 batches\tloss 0.7717 (0.8017)\taccu 100.000 (98.956)\n",
      "Epoch-86  240 batches\tloss 0.7802 (0.8010)\taccu 100.000 (98.978)\n",
      "Epoch-86  260 batches\tloss 0.7977 (0.8009)\taccu 98.438 (98.990)\n",
      "Epoch-86  280 batches\tloss 0.8441 (0.8006)\taccu 96.875 (98.996)\n",
      "Epoch-86  300 batches\tloss 0.8259 (0.8011)\taccu 98.438 (98.995)\n",
      "Epoch-86  320 batches\tloss 0.8038 (0.8012)\taccu 96.875 (98.989)\n",
      "Epoch-86  340 batches\tloss 0.7791 (0.8025)\taccu 100.000 (98.984)\n",
      "Epoch-86  360 batches\tloss 0.7851 (0.8027)\taccu 100.000 (98.976)\n",
      "Epoch-86  380 batches\tloss 0.7939 (0.8029)\taccu 98.438 (98.960)\n",
      "Epoch-86  400 batches\tloss 0.7759 (0.8028)\taccu 100.000 (98.980)\n",
      "Epoch-86  420 batches\tloss 0.7784 (0.8023)\taccu 100.000 (99.003)\n",
      "Epoch-86  440 batches\tloss 0.7667 (0.8018)\taccu 100.000 (99.016)\n",
      "Epoch-86  460 batches\tloss 0.7831 (0.8018)\taccu 98.438 (98.995)\n",
      "Epoch-86  480 batches\tloss 0.7944 (0.8017)\taccu 98.438 (98.978)\n",
      "Epoch-86  88.6s\tTrain: loss 0.8015\taccu 98.9899\tValid: loss 0.9810\taccu 92.0401\n",
      "Epoch 86: val_acc did not improve\n",
      "86 0.0001\n",
      "Epoch-87   20 batches\tloss 0.8101 (0.7944)\taccu 98.438 (99.375)\n",
      "Epoch-87   40 batches\tloss 0.8143 (0.7958)\taccu 98.438 (99.258)\n",
      "Epoch-87   60 batches\tloss 0.7673 (0.7999)\taccu 100.000 (99.141)\n",
      "Epoch-87   80 batches\tloss 0.7917 (0.8011)\taccu 100.000 (99.082)\n",
      "Epoch-87  100 batches\tloss 0.7768 (0.8021)\taccu 98.438 (99.000)\n",
      "Epoch-87  120 batches\tloss 0.7656 (0.8033)\taccu 100.000 (98.945)\n",
      "Epoch-87  140 batches\tloss 0.8201 (0.8029)\taccu 96.875 (98.940)\n",
      "Epoch-87  160 batches\tloss 0.7762 (0.8019)\taccu 100.000 (98.994)\n",
      "Epoch-87  180 batches\tloss 0.8233 (0.8032)\taccu 100.000 (98.967)\n",
      "Epoch-87  200 batches\tloss 0.8015 (0.8023)\taccu 100.000 (98.961)\n",
      "Epoch-87  220 batches\tloss 0.7585 (0.8026)\taccu 100.000 (98.956)\n",
      "Epoch-87  240 batches\tloss 0.8409 (0.8032)\taccu 100.000 (98.939)\n",
      "Epoch-87  260 batches\tloss 0.8662 (0.8024)\taccu 95.312 (98.972)\n",
      "Epoch-87  280 batches\tloss 0.7807 (0.8019)\taccu 100.000 (98.979)\n",
      "Epoch-87  300 batches\tloss 0.7998 (0.8020)\taccu 100.000 (99.005)\n",
      "Epoch-87  320 batches\tloss 0.8169 (0.8025)\taccu 98.438 (98.984)\n",
      "Epoch-87  340 batches\tloss 0.7998 (0.8023)\taccu 100.000 (98.994)\n",
      "Epoch-87  360 batches\tloss 0.7751 (0.8016)\taccu 98.438 (99.015)\n",
      "Epoch-87  380 batches\tloss 0.7980 (0.8020)\taccu 100.000 (98.984)\n",
      "Epoch-87  400 batches\tloss 0.7753 (0.8017)\taccu 100.000 (99.000)\n",
      "Epoch-87  420 batches\tloss 0.8662 (0.8014)\taccu 95.312 (99.007)\n",
      "Epoch-87  440 batches\tloss 0.8896 (0.8020)\taccu 93.750 (98.991)\n",
      "Epoch-87  460 batches\tloss 0.8318 (0.8026)\taccu 96.875 (98.964)\n",
      "Epoch-87  480 batches\tloss 0.7708 (0.8025)\taccu 100.000 (98.971)\n",
      "Epoch-87  89.1s\tTrain: loss 0.8022\taccu 98.9804\tValid: loss 0.9784\taccu 92.0548\n",
      "Epoch 87: val_acc did not improve\n",
      "87 0.0001\n",
      "Epoch-88   20 batches\tloss 0.8744 (0.7982)\taccu 96.875 (99.062)\n",
      "Epoch-88   40 batches\tloss 0.8285 (0.8019)\taccu 96.875 (98.711)\n",
      "Epoch-88   60 batches\tloss 0.7802 (0.7997)\taccu 100.000 (98.906)\n",
      "Epoch-88   80 batches\tloss 0.7994 (0.8002)\taccu 100.000 (98.965)\n",
      "Epoch-88  100 batches\tloss 0.8111 (0.8008)\taccu 100.000 (98.969)\n",
      "Epoch-88  120 batches\tloss 0.7679 (0.7980)\taccu 100.000 (99.036)\n",
      "Epoch-88  140 batches\tloss 0.8633 (0.7988)\taccu 95.312 (99.018)\n",
      "Epoch-88  160 batches\tloss 0.7758 (0.7993)\taccu 100.000 (99.014)\n",
      "Epoch-88  180 batches\tloss 0.7836 (0.7998)\taccu 100.000 (99.045)\n",
      "Epoch-88  200 batches\tloss 0.7884 (0.7990)\taccu 100.000 (99.047)\n",
      "Epoch-88  220 batches\tloss 0.7926 (0.7991)\taccu 98.438 (99.020)\n",
      "Epoch-88  240 batches\tloss 0.8456 (0.7993)\taccu 96.875 (99.004)\n",
      "Epoch-88  260 batches\tloss 0.8767 (0.7996)\taccu 98.438 (98.966)\n",
      "Epoch-88  280 batches\tloss 0.8184 (0.7991)\taccu 98.438 (98.990)\n",
      "Epoch-88  300 batches\tloss 0.8239 (0.7990)\taccu 100.000 (99.005)\n",
      "Epoch-88  320 batches\tloss 0.7680 (0.7993)\taccu 100.000 (99.009)\n",
      "Epoch-88  340 batches\tloss 0.7923 (0.7995)\taccu 100.000 (99.012)\n",
      "Epoch-88  360 batches\tloss 0.7758 (0.7992)\taccu 100.000 (99.006)\n",
      "Epoch-88  380 batches\tloss 0.7726 (0.7990)\taccu 100.000 (99.021)\n",
      "Epoch-88  400 batches\tloss 0.7738 (0.7993)\taccu 100.000 (99.020)\n",
      "Epoch-88  420 batches\tloss 0.8131 (0.7993)\taccu 100.000 (99.029)\n",
      "Epoch-88  440 batches\tloss 0.7826 (0.7998)\taccu 100.000 (98.999)\n",
      "Epoch-88  460 batches\tloss 0.7569 (0.8002)\taccu 100.000 (98.974)\n",
      "Epoch-88  480 batches\tloss 0.8299 (0.8004)\taccu 95.312 (98.962)\n",
      "Epoch-88  88.9s\tTrain: loss 0.8004\taccu 98.9678\tValid: loss 0.9843\taccu 91.9369\n",
      "Epoch 88: val_acc did not improve\n",
      "88 0.0001\n",
      "Epoch-89   20 batches\tloss 0.8009 (0.8047)\taccu 96.875 (98.828)\n",
      "Epoch-89   40 batches\tloss 0.7803 (0.8034)\taccu 100.000 (98.828)\n",
      "Epoch-89   60 batches\tloss 0.7978 (0.7976)\taccu 96.875 (99.036)\n",
      "Epoch-89   80 batches\tloss 0.7736 (0.7960)\taccu 100.000 (99.062)\n",
      "Epoch-89  100 batches\tloss 0.7809 (0.7969)\taccu 98.438 (98.969)\n",
      "Epoch-89  120 batches\tloss 0.8633 (0.7967)\taccu 96.875 (98.997)\n",
      "Epoch-89  140 batches\tloss 0.7468 (0.7960)\taccu 100.000 (99.062)\n",
      "Epoch-89  160 batches\tloss 0.7671 (0.7950)\taccu 100.000 (99.092)\n",
      "Epoch-89  180 batches\tloss 0.7885 (0.7953)\taccu 100.000 (99.097)\n",
      "Epoch-89  200 batches\tloss 0.8539 (0.7971)\taccu 96.875 (99.062)\n",
      "Epoch-89  220 batches\tloss 0.8230 (0.7971)\taccu 98.438 (99.091)\n",
      "Epoch-89  240 batches\tloss 0.7837 (0.7971)\taccu 98.438 (99.082)\n",
      "Epoch-89  260 batches\tloss 0.7734 (0.7978)\taccu 100.000 (99.050)\n",
      "Epoch-89  280 batches\tloss 0.7820 (0.7983)\taccu 100.000 (99.035)\n",
      "Epoch-89  300 batches\tloss 0.7974 (0.7988)\taccu 100.000 (99.057)\n",
      "Epoch-89  320 batches\tloss 0.8021 (0.7984)\taccu 96.875 (99.058)\n",
      "Epoch-89  340 batches\tloss 0.7911 (0.7983)\taccu 98.438 (99.053)\n",
      "Epoch-89  360 batches\tloss 0.7550 (0.7984)\taccu 100.000 (99.036)\n",
      "Epoch-89  380 batches\tloss 0.8277 (0.7988)\taccu 100.000 (99.021)\n",
      "Epoch-89  400 batches\tloss 0.7795 (0.7992)\taccu 98.438 (99.008)\n",
      "Epoch-89  420 batches\tloss 0.7759 (0.7992)\taccu 100.000 (99.003)\n",
      "Epoch-89  440 batches\tloss 0.8049 (0.7985)\taccu 98.438 (99.013)\n",
      "Epoch-89  460 batches\tloss 0.8029 (0.7995)\taccu 98.438 (98.984)\n",
      "Epoch-89  480 batches\tloss 0.8208 (0.7990)\taccu 100.000 (99.017)\n",
      "Epoch-89  89.1s\tTrain: loss 0.7987\taccu 99.0373\tValid: loss 0.9801\taccu 92.2465\n",
      "Epoch 89: val_acc improved from 92.0843 to 92.2465, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "89 0.0001\n",
      "Epoch-90   20 batches\tloss 0.8136 (0.8006)\taccu 98.438 (98.906)\n",
      "Epoch-90   40 batches\tloss 0.7766 (0.7991)\taccu 100.000 (99.062)\n",
      "Epoch-90   60 batches\tloss 0.7969 (0.7992)\taccu 98.438 (99.036)\n",
      "Epoch-90   80 batches\tloss 0.8015 (0.7978)\taccu 100.000 (99.141)\n",
      "Epoch-90  100 batches\tloss 0.8512 (0.7972)\taccu 98.438 (99.219)\n",
      "Epoch-90  120 batches\tloss 0.7923 (0.7974)\taccu 100.000 (99.154)\n",
      "Epoch-90  140 batches\tloss 0.8273 (0.7978)\taccu 98.438 (99.096)\n",
      "Epoch-90  160 batches\tloss 0.7941 (0.7977)\taccu 100.000 (99.121)\n",
      "Epoch-90  180 batches\tloss 0.8607 (0.7980)\taccu 96.875 (99.097)\n",
      "Epoch-90  200 batches\tloss 0.7950 (0.7979)\taccu 98.438 (99.062)\n",
      "Epoch-90  220 batches\tloss 0.8213 (0.7981)\taccu 98.438 (99.055)\n",
      "Epoch-90  240 batches\tloss 0.8106 (0.7988)\taccu 100.000 (99.010)\n",
      "Epoch-90  260 batches\tloss 0.8634 (0.7987)\taccu 95.312 (99.020)\n",
      "Epoch-90  280 batches\tloss 0.8004 (0.7979)\taccu 98.438 (99.040)\n",
      "Epoch-90  300 batches\tloss 0.7922 (0.7985)\taccu 100.000 (99.021)\n",
      "Epoch-90  320 batches\tloss 0.7911 (0.7984)\taccu 98.438 (99.019)\n",
      "Epoch-90  340 batches\tloss 0.7835 (0.7976)\taccu 100.000 (99.049)\n",
      "Epoch-90  360 batches\tloss 0.7754 (0.7977)\taccu 98.438 (99.041)\n",
      "Epoch-90  380 batches\tloss 0.8160 (0.7980)\taccu 100.000 (99.050)\n",
      "Epoch-90  400 batches\tloss 0.8599 (0.7983)\taccu 98.438 (99.059)\n",
      "Epoch-90  420 batches\tloss 0.7731 (0.7988)\taccu 98.438 (99.036)\n",
      "Epoch-90  440 batches\tloss 0.7828 (0.7983)\taccu 100.000 (99.034)\n",
      "Epoch-90  460 batches\tloss 0.8007 (0.7979)\taccu 98.438 (99.049)\n",
      "Epoch-90  480 batches\tloss 0.8019 (0.7975)\taccu 98.438 (99.063)\n",
      "Epoch-90  88.7s\tTrain: loss 0.7979\taccu 99.0499\tValid: loss 0.9762\taccu 92.0254\n",
      "Epoch 90: val_acc did not improve\n",
      "90 1e-05\n",
      "Epoch-91   20 batches\tloss 0.8166 (0.8060)\taccu 96.875 (98.438)\n",
      "Epoch-91   40 batches\tloss 0.7771 (0.7991)\taccu 100.000 (98.945)\n",
      "Epoch-91   60 batches\tloss 0.7865 (0.7961)\taccu 98.438 (99.010)\n",
      "Epoch-91   80 batches\tloss 0.8677 (0.7971)\taccu 93.750 (98.965)\n",
      "Epoch-91  100 batches\tloss 0.8066 (0.7982)\taccu 100.000 (98.969)\n",
      "Epoch-91  120 batches\tloss 0.8210 (0.7968)\taccu 98.438 (99.049)\n",
      "Epoch-91  140 batches\tloss 0.7983 (0.7968)\taccu 98.438 (99.029)\n",
      "Epoch-91  160 batches\tloss 0.8006 (0.7970)\taccu 98.438 (98.994)\n",
      "Epoch-91  180 batches\tloss 0.7825 (0.7958)\taccu 100.000 (99.036)\n",
      "Epoch-91  200 batches\tloss 0.7596 (0.7951)\taccu 100.000 (99.055)\n",
      "Epoch-91  220 batches\tloss 0.7853 (0.7951)\taccu 100.000 (99.048)\n",
      "Epoch-91  240 batches\tloss 0.8772 (0.7955)\taccu 93.750 (99.023)\n",
      "Epoch-91  260 batches\tloss 0.7866 (0.7955)\taccu 98.438 (99.014)\n",
      "Epoch-91  280 batches\tloss 0.7757 (0.7955)\taccu 98.438 (99.018)\n",
      "Epoch-91  300 batches\tloss 0.7705 (0.7956)\taccu 100.000 (99.026)\n",
      "Epoch-91  320 batches\tloss 0.7591 (0.7957)\taccu 100.000 (99.033)\n",
      "Epoch-91  340 batches\tloss 0.8210 (0.7953)\taccu 96.875 (99.049)\n",
      "Epoch-91  360 batches\tloss 0.7622 (0.7947)\taccu 100.000 (99.054)\n",
      "Epoch-91  380 batches\tloss 0.7838 (0.7942)\taccu 100.000 (99.071)\n",
      "Epoch-91  400 batches\tloss 0.7802 (0.7941)\taccu 100.000 (99.078)\n",
      "Epoch-91  420 batches\tloss 0.7587 (0.7939)\taccu 100.000 (99.077)\n",
      "Epoch-91  440 batches\tloss 0.7776 (0.7944)\taccu 100.000 (99.048)\n",
      "Epoch-91  460 batches\tloss 0.7450 (0.7942)\taccu 100.000 (99.059)\n",
      "Epoch-91  480 batches\tloss 0.7782 (0.7946)\taccu 100.000 (99.040)\n",
      "Epoch-91  89.1s\tTrain: loss 0.7947\taccu 99.0404\tValid: loss 0.9781\taccu 92.0696\n",
      "Epoch 91: val_acc did not improve\n",
      "91 1e-05\n",
      "Epoch-92   20 batches\tloss 0.8655 (0.7864)\taccu 96.875 (99.141)\n",
      "Epoch-92   40 batches\tloss 0.7766 (0.7931)\taccu 98.438 (99.141)\n",
      "Epoch-92   60 batches\tloss 0.7772 (0.7944)\taccu 98.438 (99.063)\n",
      "Epoch-92   80 batches\tloss 0.7901 (0.7930)\taccu 100.000 (99.062)\n",
      "Epoch-92  100 batches\tloss 0.7991 (0.7929)\taccu 100.000 (99.078)\n",
      "Epoch-92  120 batches\tloss 0.7591 (0.7911)\taccu 100.000 (99.180)\n",
      "Epoch-92  140 batches\tloss 0.8359 (0.7926)\taccu 98.438 (99.196)\n",
      "Epoch-92  160 batches\tloss 0.7850 (0.7917)\taccu 100.000 (99.229)\n",
      "Epoch-92  180 batches\tloss 0.7877 (0.7910)\taccu 100.000 (99.245)\n",
      "Epoch-92  200 batches\tloss 0.7723 (0.7900)\taccu 100.000 (99.242)\n",
      "Epoch-92  220 batches\tloss 0.7935 (0.7903)\taccu 98.438 (99.197)\n",
      "Epoch-92  240 batches\tloss 0.7545 (0.7905)\taccu 100.000 (99.180)\n",
      "Epoch-92  260 batches\tloss 0.8178 (0.7913)\taccu 95.312 (99.135)\n",
      "Epoch-92  280 batches\tloss 0.8110 (0.7912)\taccu 100.000 (99.152)\n",
      "Epoch-92  300 batches\tloss 0.7706 (0.7911)\taccu 100.000 (99.161)\n",
      "Epoch-92  320 batches\tloss 0.7967 (0.7906)\taccu 98.438 (99.185)\n",
      "Epoch-92  340 batches\tloss 0.7675 (0.7912)\taccu 100.000 (99.154)\n",
      "Epoch-92  360 batches\tloss 0.7938 (0.7911)\taccu 100.000 (99.145)\n",
      "Epoch-92  380 batches\tloss 0.7797 (0.7907)\taccu 100.000 (99.161)\n",
      "Epoch-92  400 batches\tloss 0.7856 (0.7914)\taccu 100.000 (99.148)\n",
      "Epoch-92  420 batches\tloss 0.8062 (0.7919)\taccu 100.000 (99.144)\n",
      "Epoch-92  440 batches\tloss 0.8165 (0.7923)\taccu 96.875 (99.109)\n",
      "Epoch-92  460 batches\tloss 0.8018 (0.7925)\taccu 100.000 (99.113)\n",
      "Epoch-92  480 batches\tloss 0.9209 (0.7929)\taccu 95.312 (99.115)\n",
      "Epoch-92  89.1s\tTrain: loss 0.7928\taccu 99.1320\tValid: loss 0.9762\taccu 92.1728\n",
      "Epoch 92: val_acc did not improve\n",
      "92 1e-05\n",
      "Epoch-93   20 batches\tloss 0.7886 (0.7845)\taccu 100.000 (99.688)\n",
      "Epoch-93   40 batches\tloss 0.8120 (0.7859)\taccu 100.000 (99.492)\n",
      "Epoch-93   60 batches\tloss 0.7631 (0.7905)\taccu 100.000 (99.349)\n",
      "Epoch-93   80 batches\tloss 0.7665 (0.7898)\taccu 100.000 (99.375)\n",
      "Epoch-93  100 batches\tloss 0.7858 (0.7906)\taccu 98.438 (99.359)\n",
      "Epoch-93  120 batches\tloss 0.7956 (0.7918)\taccu 100.000 (99.271)\n",
      "Epoch-93  140 batches\tloss 0.9597 (0.7918)\taccu 93.750 (99.230)\n",
      "Epoch-93  160 batches\tloss 0.8011 (0.7914)\taccu 96.875 (99.209)\n",
      "Epoch-93  180 batches\tloss 0.7867 (0.7917)\taccu 100.000 (99.193)\n",
      "Epoch-93  200 batches\tloss 0.7847 (0.7923)\taccu 98.438 (99.195)\n",
      "Epoch-93  220 batches\tloss 0.7910 (0.7913)\taccu 98.438 (99.219)\n",
      "Epoch-93  240 batches\tloss 0.8693 (0.7915)\taccu 95.312 (99.186)\n",
      "Epoch-93  260 batches\tloss 0.7770 (0.7905)\taccu 100.000 (99.219)\n",
      "Epoch-93  280 batches\tloss 0.7569 (0.7915)\taccu 100.000 (99.169)\n",
      "Epoch-93  300 batches\tloss 0.7741 (0.7914)\taccu 100.000 (99.188)\n",
      "Epoch-93  320 batches\tloss 0.7877 (0.7911)\taccu 98.438 (99.214)\n",
      "Epoch-93  340 batches\tloss 0.8251 (0.7914)\taccu 96.875 (99.205)\n",
      "Epoch-93  360 batches\tloss 0.8142 (0.7914)\taccu 98.438 (99.206)\n",
      "Epoch-93  380 batches\tloss 0.8069 (0.7922)\taccu 100.000 (99.202)\n",
      "Epoch-93  400 batches\tloss 0.7759 (0.7918)\taccu 100.000 (99.203)\n",
      "Epoch-93  420 batches\tloss 0.8106 (0.7921)\taccu 98.438 (99.189)\n",
      "Epoch-93  440 batches\tloss 0.7843 (0.7922)\taccu 98.438 (99.162)\n",
      "Epoch-93  460 batches\tloss 0.7831 (0.7928)\taccu 100.000 (99.144)\n",
      "Epoch-93  480 batches\tloss 0.8280 (0.7930)\taccu 96.875 (99.124)\n",
      "Epoch-93  89.6s\tTrain: loss 0.7930\taccu 99.1320\tValid: loss 0.9762\taccu 91.9222\n",
      "Epoch 93: val_acc did not improve\n",
      "93 1e-05\n",
      "Epoch-94   20 batches\tloss 0.7740 (0.7909)\taccu 100.000 (99.062)\n",
      "Epoch-94   40 batches\tloss 0.8343 (0.7948)\taccu 98.438 (99.102)\n",
      "Epoch-94   60 batches\tloss 0.7850 (0.7924)\taccu 100.000 (99.141)\n",
      "Epoch-94   80 batches\tloss 0.7745 (0.7943)\taccu 100.000 (99.082)\n",
      "Epoch-94  100 batches\tloss 0.7845 (0.7930)\taccu 100.000 (99.125)\n",
      "Epoch-94  120 batches\tloss 0.8928 (0.7945)\taccu 96.875 (99.089)\n",
      "Epoch-94  140 batches\tloss 0.8178 (0.7936)\taccu 98.438 (99.163)\n",
      "Epoch-94  160 batches\tloss 0.7876 (0.7931)\taccu 100.000 (99.189)\n",
      "Epoch-94  180 batches\tloss 0.8036 (0.7929)\taccu 98.438 (99.193)\n",
      "Epoch-94  200 batches\tloss 0.7720 (0.7927)\taccu 100.000 (99.203)\n",
      "Epoch-94  220 batches\tloss 0.8304 (0.7927)\taccu 98.438 (99.183)\n",
      "Epoch-94  240 batches\tloss 0.7962 (0.7927)\taccu 100.000 (99.167)\n",
      "Epoch-94  260 batches\tloss 0.7914 (0.7926)\taccu 100.000 (99.183)\n",
      "Epoch-94  280 batches\tloss 0.8364 (0.7920)\taccu 98.438 (99.191)\n",
      "Epoch-94  300 batches\tloss 0.7911 (0.7919)\taccu 100.000 (99.193)\n",
      "Epoch-94  320 batches\tloss 0.7665 (0.7914)\taccu 100.000 (99.204)\n",
      "Epoch-94  340 batches\tloss 0.7594 (0.7909)\taccu 100.000 (99.196)\n",
      "Epoch-94  360 batches\tloss 0.7939 (0.7909)\taccu 98.438 (99.171)\n",
      "Epoch-94  380 batches\tloss 0.7929 (0.7917)\taccu 98.438 (99.132)\n",
      "Epoch-94  400 batches\tloss 0.7455 (0.7916)\taccu 100.000 (99.133)\n",
      "Epoch-94  420 batches\tloss 0.7783 (0.7911)\taccu 100.000 (99.148)\n",
      "Epoch-94  440 batches\tloss 0.8398 (0.7911)\taccu 98.438 (99.155)\n",
      "Epoch-94  460 batches\tloss 0.7636 (0.7908)\taccu 100.000 (99.154)\n",
      "Epoch-94  480 batches\tloss 0.7849 (0.7908)\taccu 100.000 (99.154)\n",
      "Epoch-94  89.0s\tTrain: loss 0.7913\taccu 99.1256\tValid: loss 0.9783\taccu 92.1875\n",
      "Epoch 94: val_acc did not improve\n",
      "94 1e-05\n",
      "Epoch-95   20 batches\tloss 0.7769 (0.7932)\taccu 100.000 (98.984)\n",
      "Epoch-95   40 batches\tloss 0.7891 (0.7940)\taccu 98.438 (98.984)\n",
      "Epoch-95   60 batches\tloss 0.7846 (0.7914)\taccu 98.438 (99.141)\n",
      "Epoch-95   80 batches\tloss 0.7712 (0.7903)\taccu 100.000 (99.180)\n",
      "Epoch-95  100 batches\tloss 0.8053 (0.7885)\taccu 98.438 (99.234)\n",
      "Epoch-95  120 batches\tloss 0.7570 (0.7908)\taccu 100.000 (99.154)\n",
      "Epoch-95  140 batches\tloss 0.7671 (0.7915)\taccu 100.000 (99.174)\n",
      "Epoch-95  160 batches\tloss 0.8996 (0.7933)\taccu 96.875 (99.170)\n",
      "Epoch-95  180 batches\tloss 0.7755 (0.7925)\taccu 100.000 (99.184)\n",
      "Epoch-95  200 batches\tloss 0.7748 (0.7933)\taccu 98.438 (99.156)\n",
      "Epoch-95  220 batches\tloss 0.7861 (0.7925)\taccu 98.438 (99.176)\n",
      "Epoch-95  240 batches\tloss 0.8196 (0.7920)\taccu 100.000 (99.199)\n",
      "Epoch-95  260 batches\tloss 0.7698 (0.7915)\taccu 100.000 (99.225)\n",
      "Epoch-95  280 batches\tloss 0.7786 (0.7914)\taccu 100.000 (99.219)\n",
      "Epoch-95  300 batches\tloss 0.7626 (0.7910)\taccu 100.000 (99.245)\n",
      "Epoch-95  320 batches\tloss 0.7862 (0.7902)\taccu 96.875 (99.253)\n",
      "Epoch-95  340 batches\tloss 0.7828 (0.7899)\taccu 100.000 (99.246)\n",
      "Epoch-95  360 batches\tloss 0.7871 (0.7898)\taccu 100.000 (99.275)\n",
      "Epoch-95  380 batches\tloss 0.7878 (0.7896)\taccu 100.000 (99.264)\n",
      "Epoch-95  400 batches\tloss 0.8087 (0.7899)\taccu 98.438 (99.254)\n",
      "Epoch-95  420 batches\tloss 0.7888 (0.7899)\taccu 100.000 (99.245)\n",
      "Epoch-95  440 batches\tloss 0.8207 (0.7901)\taccu 98.438 (99.240)\n",
      "Epoch-95  460 batches\tloss 0.8852 (0.7901)\taccu 93.750 (99.229)\n",
      "Epoch-95  480 batches\tloss 0.8018 (0.7899)\taccu 100.000 (99.245)\n",
      "Epoch-95  89.0s\tTrain: loss 0.7902\taccu 99.2456\tValid: loss 0.9719\taccu 92.4528\n",
      "Epoch 95: val_acc improved from 92.2465 to 92.4528, saving model to ./SGN/pretrained/SGN\\0_best.pth\n",
      "95 1e-05\n",
      "Epoch-96   20 batches\tloss 0.7657 (0.7906)\taccu 100.000 (99.375)\n",
      "Epoch-96   40 batches\tloss 0.7706 (0.7934)\taccu 98.438 (99.062)\n",
      "Epoch-96   60 batches\tloss 0.7757 (0.7903)\taccu 100.000 (99.167)\n",
      "Epoch-96   80 batches\tloss 0.7640 (0.7897)\taccu 100.000 (99.199)\n",
      "Epoch-96  100 batches\tloss 0.7835 (0.7886)\taccu 100.000 (99.250)\n",
      "Epoch-96  120 batches\tloss 0.8159 (0.7882)\taccu 98.438 (99.245)\n",
      "Epoch-96  140 batches\tloss 0.7757 (0.7878)\taccu 100.000 (99.286)\n",
      "Epoch-96  160 batches\tloss 0.7642 (0.7888)\taccu 98.438 (99.248)\n",
      "Epoch-96  180 batches\tloss 0.7608 (0.7886)\taccu 100.000 (99.288)\n",
      "Epoch-96  200 batches\tloss 0.7915 (0.7897)\taccu 98.438 (99.266)\n",
      "Epoch-96  220 batches\tloss 0.7616 (0.7897)\taccu 100.000 (99.247)\n",
      "Epoch-96  240 batches\tloss 0.7752 (0.7896)\taccu 100.000 (99.264)\n",
      "Epoch-96  260 batches\tloss 0.7758 (0.7896)\taccu 100.000 (99.255)\n",
      "Epoch-96  280 batches\tloss 0.7918 (0.7895)\taccu 98.438 (99.235)\n",
      "Epoch-96  300 batches\tloss 0.7848 (0.7896)\taccu 100.000 (99.229)\n",
      "Epoch-96  320 batches\tloss 0.7877 (0.7895)\taccu 98.438 (99.248)\n",
      "Epoch-96  340 batches\tloss 0.7782 (0.7890)\taccu 100.000 (99.265)\n",
      "Epoch-96  360 batches\tloss 0.8758 (0.7898)\taccu 96.875 (99.227)\n",
      "Epoch-96  380 batches\tloss 0.7936 (0.7900)\taccu 100.000 (99.239)\n",
      "Epoch-96  400 batches\tloss 0.7792 (0.7896)\taccu 100.000 (99.246)\n",
      "Epoch-96  420 batches\tloss 0.7760 (0.7898)\taccu 100.000 (99.226)\n",
      "Epoch-96  440 batches\tloss 0.7974 (0.7896)\taccu 98.438 (99.240)\n",
      "Epoch-96  460 batches\tloss 0.7724 (0.7894)\taccu 98.438 (99.243)\n",
      "Epoch-96  480 batches\tloss 0.7697 (0.7890)\taccu 100.000 (99.251)\n",
      "Epoch-96  88.8s\tTrain: loss 0.7889\taccu 99.2645\tValid: loss 0.9768\taccu 92.0401\n",
      "Epoch 96: val_acc did not improve\n",
      "96 1e-05\n",
      "Epoch-97   20 batches\tloss 0.7671 (0.7854)\taccu 100.000 (99.688)\n",
      "Epoch-97   40 batches\tloss 0.7560 (0.7896)\taccu 100.000 (99.414)\n",
      "Epoch-97   60 batches\tloss 0.7945 (0.7891)\taccu 100.000 (99.349)\n",
      "Epoch-97   80 batches\tloss 0.7686 (0.7894)\taccu 100.000 (99.414)\n",
      "Epoch-97  100 batches\tloss 0.7881 (0.7881)\taccu 100.000 (99.438)\n",
      "Epoch-97  120 batches\tloss 0.8171 (0.7881)\taccu 96.875 (99.401)\n",
      "Epoch-97  140 batches\tloss 0.7624 (0.7879)\taccu 100.000 (99.353)\n",
      "Epoch-97  160 batches\tloss 0.7531 (0.7892)\taccu 100.000 (99.307)\n",
      "Epoch-97  180 batches\tloss 0.8008 (0.7896)\taccu 96.875 (99.271)\n",
      "Epoch-97  200 batches\tloss 0.7780 (0.7898)\taccu 98.438 (99.219)\n",
      "Epoch-97  220 batches\tloss 0.7649 (0.7903)\taccu 100.000 (99.226)\n",
      "Epoch-97  240 batches\tloss 0.8314 (0.7904)\taccu 98.438 (99.206)\n",
      "Epoch-97  260 batches\tloss 0.7806 (0.7906)\taccu 98.438 (99.183)\n",
      "Epoch-97  280 batches\tloss 0.7714 (0.7921)\taccu 100.000 (99.141)\n",
      "Epoch-97  300 batches\tloss 0.7944 (0.7918)\taccu 98.438 (99.125)\n",
      "Epoch-97  320 batches\tloss 0.7728 (0.7919)\taccu 100.000 (99.116)\n",
      "Epoch-97  340 batches\tloss 0.7780 (0.7918)\taccu 100.000 (99.113)\n",
      "Epoch-97  360 batches\tloss 0.7447 (0.7910)\taccu 100.000 (99.145)\n",
      "Epoch-97  380 batches\tloss 0.7695 (0.7914)\taccu 100.000 (99.132)\n",
      "Epoch-97  400 batches\tloss 0.7943 (0.7915)\taccu 96.875 (99.121)\n",
      "Epoch-97  420 batches\tloss 0.7812 (0.7915)\taccu 100.000 (99.137)\n",
      "Epoch-97  440 batches\tloss 0.7644 (0.7919)\taccu 100.000 (99.116)\n",
      "Epoch-97  460 batches\tloss 0.8021 (0.7913)\taccu 100.000 (99.137)\n",
      "Epoch-97  480 batches\tloss 0.8038 (0.7914)\taccu 100.000 (99.144)\n",
      "Epoch-97  89.0s\tTrain: loss 0.7912\taccu 99.1572\tValid: loss 0.9743\taccu 92.2170\n",
      "Epoch 97: val_acc did not improve\n",
      "97 1e-05\n",
      "Epoch-98   20 batches\tloss 0.7950 (0.7841)\taccu 100.000 (99.219)\n",
      "Epoch-98   40 batches\tloss 0.7861 (0.7843)\taccu 100.000 (99.336)\n",
      "Epoch-98   60 batches\tloss 0.7766 (0.7860)\taccu 98.438 (99.141)\n",
      "Epoch-98   80 batches\tloss 0.7655 (0.7851)\taccu 100.000 (99.219)\n",
      "Epoch-98  100 batches\tloss 0.8023 (0.7860)\taccu 100.000 (99.172)\n",
      "Epoch-98  120 batches\tloss 0.7862 (0.7866)\taccu 100.000 (99.219)\n",
      "Epoch-98  140 batches\tloss 0.7680 (0.7873)\taccu 100.000 (99.163)\n",
      "Epoch-98  160 batches\tloss 0.7970 (0.7872)\taccu 98.438 (99.170)\n",
      "Epoch-98  180 batches\tloss 0.7851 (0.7883)\taccu 98.438 (99.141)\n",
      "Epoch-98  200 batches\tloss 0.7649 (0.7887)\taccu 100.000 (99.164)\n",
      "Epoch-98  220 batches\tloss 0.8042 (0.7890)\taccu 98.438 (99.155)\n",
      "Epoch-98  240 batches\tloss 0.7735 (0.7897)\taccu 100.000 (99.160)\n",
      "Epoch-98  260 batches\tloss 0.8221 (0.7905)\taccu 98.438 (99.129)\n",
      "Epoch-98  280 batches\tloss 0.7713 (0.7898)\taccu 100.000 (99.146)\n",
      "Epoch-98  300 batches\tloss 0.7694 (0.7900)\taccu 100.000 (99.109)\n",
      "Epoch-98  320 batches\tloss 0.7885 (0.7904)\taccu 98.438 (99.092)\n",
      "Epoch-98  340 batches\tloss 0.8357 (0.7904)\taccu 100.000 (99.085)\n",
      "Epoch-98  360 batches\tloss 0.7796 (0.7902)\taccu 98.438 (99.080)\n",
      "Epoch-98  380 batches\tloss 0.7968 (0.7901)\taccu 98.438 (99.100)\n",
      "Epoch-98  400 batches\tloss 0.7867 (0.7901)\taccu 100.000 (99.102)\n",
      "Epoch-98  420 batches\tloss 0.7981 (0.7908)\taccu 98.438 (99.077)\n",
      "Epoch-98  440 batches\tloss 0.7835 (0.7908)\taccu 98.438 (99.077)\n",
      "Epoch-98  460 batches\tloss 0.7819 (0.7912)\taccu 100.000 (99.076)\n",
      "Epoch-98  480 batches\tloss 0.7821 (0.7908)\taccu 100.000 (99.092)\n",
      "Epoch-98  88.8s\tTrain: loss 0.7909\taccu 99.0909\tValid: loss 0.9746\taccu 92.2170\n",
      "Epoch 98: val_acc did not improve\n",
      "98 1e-05\n",
      "Epoch-99   20 batches\tloss 0.7834 (0.7820)\taccu 98.438 (99.609)\n",
      "Epoch-99   40 batches\tloss 0.7662 (0.7865)\taccu 100.000 (99.219)\n",
      "Epoch-99   60 batches\tloss 0.7763 (0.7866)\taccu 100.000 (99.297)\n",
      "Epoch-99   80 batches\tloss 0.7826 (0.7865)\taccu 100.000 (99.336)\n",
      "Epoch-99  100 batches\tloss 0.7767 (0.7861)\taccu 100.000 (99.297)\n",
      "Epoch-99  120 batches\tloss 0.8554 (0.7867)\taccu 98.438 (99.271)\n",
      "Epoch-99  140 batches\tloss 0.8113 (0.7863)\taccu 100.000 (99.319)\n",
      "Epoch-99  160 batches\tloss 0.7627 (0.7855)\taccu 100.000 (99.346)\n",
      "Epoch-99  180 batches\tloss 0.7710 (0.7864)\taccu 100.000 (99.314)\n",
      "Epoch-99  200 batches\tloss 0.7839 (0.7865)\taccu 98.438 (99.297)\n",
      "Epoch-99  220 batches\tloss 0.7728 (0.7875)\taccu 100.000 (99.290)\n",
      "Epoch-99  240 batches\tloss 0.7649 (0.7872)\taccu 100.000 (99.316)\n",
      "Epoch-99  260 batches\tloss 0.7935 (0.7880)\taccu 100.000 (99.309)\n",
      "Epoch-99  280 batches\tloss 0.8026 (0.7880)\taccu 98.438 (99.308)\n",
      "Epoch-99  300 batches\tloss 0.7559 (0.7877)\taccu 100.000 (99.318)\n",
      "Epoch-99  320 batches\tloss 0.8012 (0.7878)\taccu 98.438 (99.316)\n",
      "Epoch-99  340 batches\tloss 0.8091 (0.7874)\taccu 100.000 (99.343)\n",
      "Epoch-99  360 batches\tloss 0.7838 (0.7876)\taccu 100.000 (99.319)\n",
      "Epoch-99  380 batches\tloss 0.7934 (0.7879)\taccu 98.438 (99.330)\n",
      "Epoch-99  400 batches\tloss 0.7958 (0.7873)\taccu 98.438 (99.344)\n",
      "Epoch-99  420 batches\tloss 0.7788 (0.7876)\taccu 100.000 (99.330)\n",
      "Epoch-99  440 batches\tloss 0.7977 (0.7882)\taccu 96.875 (99.318)\n",
      "Epoch-99  460 batches\tloss 0.8122 (0.7881)\taccu 96.875 (99.324)\n",
      "Epoch-99  480 batches\tloss 0.8000 (0.7880)\taccu 100.000 (99.336)\n",
      "Epoch-99  89.1s\tTrain: loss 0.7880\taccu 99.3403\tValid: loss 0.9800\taccu 92.0548\n",
      "Epoch 99: val_acc did not improve\n",
      "99 1e-05\n",
      "Epoch-100  20 batches\tloss 0.7803 (0.7853)\taccu 98.438 (99.297)\n",
      "Epoch-100  40 batches\tloss 0.8156 (0.7894)\taccu 98.438 (99.180)\n",
      "Epoch-100  60 batches\tloss 0.7625 (0.7892)\taccu 100.000 (99.063)\n",
      "Epoch-100  80 batches\tloss 0.8055 (0.7908)\taccu 100.000 (99.102)\n",
      "Epoch-100 100 batches\tloss 0.8082 (0.7902)\taccu 100.000 (99.141)\n",
      "Epoch-100 120 batches\tloss 0.7989 (0.7899)\taccu 98.438 (99.180)\n",
      "Epoch-100 140 batches\tloss 0.7625 (0.7896)\taccu 100.000 (99.185)\n",
      "Epoch-100 160 batches\tloss 0.7868 (0.7899)\taccu 100.000 (99.219)\n",
      "Epoch-100 180 batches\tloss 0.7886 (0.7907)\taccu 100.000 (99.210)\n",
      "Epoch-100 200 batches\tloss 0.7733 (0.7893)\taccu 100.000 (99.219)\n",
      "Epoch-100 220 batches\tloss 0.7570 (0.7892)\taccu 100.000 (99.226)\n",
      "Epoch-100 240 batches\tloss 0.7561 (0.7894)\taccu 100.000 (99.238)\n",
      "Epoch-100 260 batches\tloss 0.7769 (0.7895)\taccu 100.000 (99.213)\n",
      "Epoch-100 280 batches\tloss 0.7897 (0.7894)\taccu 100.000 (99.230)\n",
      "Epoch-100 300 batches\tloss 0.7944 (0.7898)\taccu 98.438 (99.198)\n",
      "Epoch-100 320 batches\tloss 0.7652 (0.7894)\taccu 100.000 (99.219)\n",
      "Epoch-100 340 batches\tloss 0.8498 (0.7894)\taccu 95.312 (99.214)\n",
      "Epoch-100 360 batches\tloss 0.8095 (0.7899)\taccu 98.438 (99.210)\n",
      "Epoch-100 380 batches\tloss 0.8029 (0.7896)\taccu 96.875 (99.223)\n",
      "Epoch-100 400 batches\tloss 0.8380 (0.7901)\taccu 95.312 (99.188)\n",
      "Epoch-100 420 batches\tloss 0.7715 (0.7902)\taccu 100.000 (99.182)\n",
      "Epoch-100 440 batches\tloss 0.8749 (0.7905)\taccu 98.438 (99.158)\n",
      "Epoch-100 460 batches\tloss 0.7539 (0.7902)\taccu 100.000 (99.164)\n",
      "Epoch-100 480 batches\tloss 0.7783 (0.7902)\taccu 98.438 (99.170)\n",
      "Epoch-100 89.1s\tTrain: loss 0.7903\taccu 99.1698\tValid: loss 0.9741\taccu 92.2317\n",
      "Epoch 100: val_acc did not improve\n",
      "100 1e-05\n",
      "Epoch-101  20 batches\tloss 0.7776 (0.7880)\taccu 100.000 (99.375)\n",
      "Epoch-101  40 batches\tloss 0.7591 (0.7850)\taccu 100.000 (99.297)\n",
      "Epoch-101  60 batches\tloss 0.7876 (0.7829)\taccu 100.000 (99.427)\n",
      "Epoch-101  80 batches\tloss 0.8513 (0.7873)\taccu 100.000 (99.395)\n",
      "Epoch-101 100 batches\tloss 0.7643 (0.7871)\taccu 100.000 (99.359)\n",
      "Epoch-101 120 batches\tloss 0.7628 (0.7848)\taccu 100.000 (99.401)\n",
      "Epoch-101 140 batches\tloss 0.7759 (0.7851)\taccu 100.000 (99.386)\n",
      "Epoch-101 160 batches\tloss 0.8135 (0.7849)\taccu 100.000 (99.414)\n",
      "Epoch-101 180 batches\tloss 0.7695 (0.7849)\taccu 100.000 (99.410)\n",
      "Epoch-101 200 batches\tloss 0.8023 (0.7854)\taccu 98.438 (99.383)\n",
      "Epoch-101 220 batches\tloss 0.7743 (0.7845)\taccu 100.000 (99.411)\n",
      "Epoch-101 240 batches\tloss 0.8190 (0.7847)\taccu 98.438 (99.395)\n",
      "Epoch-101 260 batches\tloss 0.7594 (0.7845)\taccu 100.000 (99.399)\n",
      "Epoch-101 280 batches\tloss 0.7797 (0.7846)\taccu 100.000 (99.375)\n",
      "Epoch-101 300 batches\tloss 0.8142 (0.7852)\taccu 100.000 (99.349)\n",
      "Epoch-101 320 batches\tloss 0.7698 (0.7850)\taccu 100.000 (99.355)\n",
      "Epoch-101 340 batches\tloss 0.7770 (0.7851)\taccu 100.000 (99.343)\n",
      "Epoch-101 360 batches\tloss 0.7760 (0.7849)\taccu 100.000 (99.353)\n",
      "Epoch-101 380 batches\tloss 0.8365 (0.7853)\taccu 98.438 (99.354)\n",
      "Epoch-101 400 batches\tloss 0.7942 (0.7852)\taccu 98.438 (99.352)\n",
      "Epoch-101 420 batches\tloss 0.8046 (0.7852)\taccu 100.000 (99.368)\n",
      "Epoch-101 440 batches\tloss 0.7893 (0.7857)\taccu 100.000 (99.339)\n",
      "Epoch-101 460 batches\tloss 0.7973 (0.7858)\taccu 98.438 (99.338)\n",
      "Epoch-101 480 batches\tloss 0.7712 (0.7858)\taccu 100.000 (99.339)\n",
      "Epoch-101 88.9s\tTrain: loss 0.7854\taccu 99.3434\tValid: loss 0.9742\taccu 92.1728\n",
      "Epoch 101: val_acc did not improve\n",
      "101 1e-05\n",
      "Epoch-102  20 batches\tloss 0.7800 (0.7961)\taccu 100.000 (99.141)\n",
      "Epoch-102  40 batches\tloss 0.7924 (0.7890)\taccu 100.000 (99.297)\n",
      "Epoch-102  60 batches\tloss 0.7761 (0.7863)\taccu 100.000 (99.297)\n",
      "Epoch-102  80 batches\tloss 0.7772 (0.7869)\taccu 100.000 (99.297)\n",
      "Epoch-102 100 batches\tloss 0.7730 (0.7898)\taccu 98.438 (99.188)\n",
      "Epoch-102 120 batches\tloss 0.7598 (0.7907)\taccu 100.000 (99.154)\n",
      "Epoch-102 140 batches\tloss 0.7509 (0.7894)\taccu 100.000 (99.152)\n",
      "Epoch-102 160 batches\tloss 0.7698 (0.7886)\taccu 100.000 (99.180)\n",
      "Epoch-102 180 batches\tloss 0.7988 (0.7880)\taccu 98.438 (99.210)\n",
      "Epoch-102 200 batches\tloss 0.7996 (0.7885)\taccu 98.438 (99.211)\n",
      "Epoch-102 220 batches\tloss 0.7799 (0.7880)\taccu 100.000 (99.233)\n",
      "Epoch-102 240 batches\tloss 0.7923 (0.7887)\taccu 100.000 (99.225)\n",
      "Epoch-102 260 batches\tloss 0.8107 (0.7885)\taccu 98.438 (99.231)\n",
      "Epoch-102 280 batches\tloss 0.8074 (0.7895)\taccu 96.875 (99.174)\n",
      "Epoch-102 300 batches\tloss 0.7636 (0.7896)\taccu 100.000 (99.193)\n",
      "Epoch-102 320 batches\tloss 0.8109 (0.7891)\taccu 100.000 (99.224)\n",
      "Epoch-102 340 batches\tloss 0.7863 (0.7891)\taccu 98.438 (99.219)\n",
      "Epoch-102 360 batches\tloss 0.8112 (0.7892)\taccu 98.438 (99.223)\n",
      "Epoch-102 380 batches\tloss 0.8005 (0.7896)\taccu 100.000 (99.215)\n",
      "Epoch-102 400 batches\tloss 0.7912 (0.7897)\taccu 100.000 (99.215)\n",
      "Epoch-102 420 batches\tloss 0.7727 (0.7896)\taccu 98.438 (99.200)\n",
      "Epoch-102 440 batches\tloss 0.7582 (0.7895)\taccu 100.000 (99.197)\n",
      "Epoch-102 460 batches\tloss 0.7993 (0.7896)\taccu 98.438 (99.209)\n",
      "Epoch-102 480 batches\tloss 0.7481 (0.7894)\taccu 100.000 (99.212)\n",
      "Epoch-102 89.4s\tTrain: loss 0.7892\taccu 99.2266\tValid: loss 0.9769\taccu 92.1138\n",
      "Epoch 102: val_acc did not improve\n",
      "102 1e-05\n",
      "Epoch-103  20 batches\tloss 0.7770 (0.7953)\taccu 100.000 (98.984)\n",
      "Epoch-103  40 batches\tloss 0.7865 (0.7951)\taccu 98.438 (99.062)\n",
      "Epoch-103  60 batches\tloss 0.7685 (0.7941)\taccu 100.000 (99.010)\n",
      "Epoch-103  80 batches\tloss 0.9019 (0.7947)\taccu 93.750 (98.906)\n",
      "Epoch-103 100 batches\tloss 0.7765 (0.7963)\taccu 98.438 (98.859)\n",
      "Epoch-103 120 batches\tloss 0.7702 (0.7938)\taccu 98.438 (98.945)\n",
      "Epoch-103 140 batches\tloss 0.7884 (0.7945)\taccu 100.000 (98.940)\n",
      "Epoch-103 160 batches\tloss 0.8144 (0.7940)\taccu 98.438 (98.994)\n",
      "Epoch-103 180 batches\tloss 0.7796 (0.7931)\taccu 100.000 (99.080)\n",
      "Epoch-103 200 batches\tloss 0.8550 (0.7925)\taccu 100.000 (99.133)\n",
      "Epoch-103 220 batches\tloss 0.7474 (0.7917)\taccu 100.000 (99.176)\n",
      "Epoch-103 240 batches\tloss 0.7748 (0.7907)\taccu 100.000 (99.206)\n",
      "Epoch-103 260 batches\tloss 0.7977 (0.7908)\taccu 98.438 (99.201)\n",
      "Epoch-103 280 batches\tloss 0.7925 (0.7906)\taccu 98.438 (99.219)\n",
      "Epoch-103 300 batches\tloss 0.7838 (0.7908)\taccu 100.000 (99.229)\n",
      "Epoch-103 320 batches\tloss 0.7497 (0.7904)\taccu 100.000 (99.243)\n",
      "Epoch-103 340 batches\tloss 0.7943 (0.7900)\taccu 98.438 (99.269)\n",
      "Epoch-103 360 batches\tloss 0.7884 (0.7904)\taccu 100.000 (99.275)\n",
      "Epoch-103 380 batches\tloss 0.8127 (0.7903)\taccu 98.438 (99.260)\n",
      "Epoch-103 400 batches\tloss 0.7972 (0.7901)\taccu 98.438 (99.270)\n",
      "Epoch-103 420 batches\tloss 0.7715 (0.7901)\taccu 100.000 (99.275)\n",
      "Epoch-103 440 batches\tloss 0.8102 (0.7898)\taccu 100.000 (99.290)\n",
      "Epoch-103 460 batches\tloss 0.7971 (0.7899)\taccu 100.000 (99.293)\n",
      "Epoch-103 480 batches\tloss 0.8015 (0.7899)\taccu 96.875 (99.287)\n",
      "Epoch-103 89.4s\tTrain: loss 0.7899\taccu 99.2740\tValid: loss 0.9747\taccu 92.0106\n",
      "Epoch 103: val_acc did not improve\n",
      "103 1e-05\n",
      "Epoch-104  20 batches\tloss 0.7590 (0.7949)\taccu 100.000 (98.984)\n",
      "Epoch-104  40 batches\tloss 0.8055 (0.7878)\taccu 98.438 (99.180)\n",
      "Epoch-104  60 batches\tloss 0.7860 (0.7901)\taccu 100.000 (99.271)\n",
      "Epoch-104  80 batches\tloss 0.8446 (0.7893)\taccu 95.312 (99.277)\n",
      "Epoch-104 100 batches\tloss 0.7672 (0.7872)\taccu 98.438 (99.344)\n",
      "Epoch-104 120 batches\tloss 0.7734 (0.7866)\taccu 100.000 (99.362)\n",
      "Epoch-104 140 batches\tloss 0.7816 (0.7872)\taccu 100.000 (99.308)\n",
      "Epoch-104 160 batches\tloss 0.8202 (0.7879)\taccu 100.000 (99.307)\n",
      "Epoch-104 180 batches\tloss 0.7767 (0.7876)\taccu 100.000 (99.306)\n",
      "Epoch-104 200 batches\tloss 0.7897 (0.7874)\taccu 98.438 (99.297)\n",
      "Epoch-104 220 batches\tloss 0.8260 (0.7874)\taccu 98.438 (99.304)\n",
      "Epoch-104 240 batches\tloss 0.7629 (0.7867)\taccu 100.000 (99.342)\n",
      "Epoch-104 260 batches\tloss 0.7898 (0.7876)\taccu 98.438 (99.333)\n",
      "Epoch-104 280 batches\tloss 0.7873 (0.7877)\taccu 100.000 (99.336)\n",
      "Epoch-104 300 batches\tloss 0.8398 (0.7869)\taccu 96.875 (99.354)\n",
      "Epoch-104 320 batches\tloss 0.8490 (0.7873)\taccu 95.312 (99.331)\n",
      "Epoch-104 340 batches\tloss 0.7598 (0.7867)\taccu 100.000 (99.347)\n",
      "Epoch-104 360 batches\tloss 0.7607 (0.7873)\taccu 100.000 (99.323)\n",
      "Epoch-104 380 batches\tloss 0.7836 (0.7868)\taccu 98.438 (99.342)\n",
      "Epoch-104 400 batches\tloss 0.7563 (0.7868)\taccu 100.000 (99.332)\n",
      "Epoch-104 420 batches\tloss 0.7704 (0.7866)\taccu 98.438 (99.323)\n",
      "Epoch-104 440 batches\tloss 0.7671 (0.7871)\taccu 100.000 (99.311)\n",
      "Epoch-104 460 batches\tloss 0.7680 (0.7873)\taccu 100.000 (99.307)\n",
      "Epoch-104 480 batches\tloss 0.7678 (0.7877)\taccu 100.000 (99.287)\n",
      "Epoch-104 89.2s\tTrain: loss 0.7879\taccu 99.2582\tValid: loss 0.9744\taccu 92.2612\n",
      "Epoch 104: val_acc did not improve\n",
      "104 1e-05\n",
      "Epoch-105  20 batches\tloss 0.7619 (0.7864)\taccu 100.000 (99.219)\n",
      "Epoch-105  40 batches\tloss 0.7718 (0.7821)\taccu 100.000 (99.297)\n",
      "Epoch-105  60 batches\tloss 0.7787 (0.7856)\taccu 98.438 (99.141)\n",
      "Epoch-105  80 batches\tloss 0.7722 (0.7862)\taccu 100.000 (99.199)\n",
      "Epoch-105 100 batches\tloss 0.7643 (0.7878)\taccu 100.000 (99.156)\n",
      "Epoch-105 120 batches\tloss 0.7888 (0.7899)\taccu 100.000 (99.102)\n",
      "Epoch-105 140 batches\tloss 0.7574 (0.7882)\taccu 100.000 (99.174)\n",
      "Epoch-105 160 batches\tloss 0.7746 (0.7887)\taccu 100.000 (99.131)\n",
      "Epoch-105 180 batches\tloss 0.8220 (0.7894)\taccu 98.438 (99.123)\n",
      "Epoch-105 200 batches\tloss 0.7937 (0.7896)\taccu 100.000 (99.148)\n",
      "Epoch-105 220 batches\tloss 0.8108 (0.7900)\taccu 100.000 (99.148)\n",
      "Epoch-105 240 batches\tloss 0.7884 (0.7889)\taccu 100.000 (99.160)\n",
      "Epoch-105 260 batches\tloss 0.7719 (0.7883)\taccu 100.000 (99.195)\n",
      "Epoch-105 280 batches\tloss 0.8384 (0.7885)\taccu 96.875 (99.191)\n",
      "Epoch-105 300 batches\tloss 0.7835 (0.7883)\taccu 100.000 (99.198)\n",
      "Epoch-105 320 batches\tloss 0.7722 (0.7880)\taccu 98.438 (99.214)\n",
      "Epoch-105 340 batches\tloss 0.7854 (0.7878)\taccu 100.000 (99.210)\n",
      "Epoch-105 360 batches\tloss 0.7491 (0.7873)\taccu 100.000 (99.223)\n",
      "Epoch-105 380 batches\tloss 0.7661 (0.7880)\taccu 100.000 (99.194)\n",
      "Epoch-105 400 batches\tloss 0.8119 (0.7881)\taccu 98.438 (99.180)\n",
      "Epoch-105 420 batches\tloss 0.7701 (0.7880)\taccu 100.000 (99.189)\n",
      "Epoch-105 440 batches\tloss 0.8096 (0.7883)\taccu 98.438 (99.190)\n",
      "Epoch-105 460 batches\tloss 0.7636 (0.7879)\taccu 100.000 (99.202)\n",
      "Epoch-105 480 batches\tloss 0.7907 (0.7878)\taccu 98.438 (99.206)\n",
      "Epoch-105 89.1s\tTrain: loss 0.7879\taccu 99.2014\tValid: loss 0.9753\taccu 92.1285\n",
      "Epoch 105: val_acc did not improve\n",
      "105 1e-05\n",
      "Epoch-106  20 batches\tloss 0.8166 (0.7832)\taccu 98.438 (99.141)\n",
      "Epoch-106  40 batches\tloss 0.7843 (0.7857)\taccu 98.438 (99.102)\n",
      "Epoch-106  60 batches\tloss 0.7689 (0.7858)\taccu 100.000 (99.036)\n",
      "Epoch-106  80 batches\tloss 0.7907 (0.7872)\taccu 100.000 (99.082)\n",
      "Epoch-106 100 batches\tloss 0.7641 (0.7871)\taccu 98.438 (99.109)\n",
      "Epoch-106 120 batches\tloss 0.8144 (0.7865)\taccu 98.438 (99.154)\n",
      "Epoch-106 140 batches\tloss 0.7763 (0.7856)\taccu 100.000 (99.185)\n",
      "Epoch-106 160 batches\tloss 0.7952 (0.7873)\taccu 98.438 (99.150)\n",
      "Epoch-106 180 batches\tloss 0.8120 (0.7881)\taccu 98.438 (99.193)\n",
      "Epoch-106 200 batches\tloss 0.7645 (0.7875)\taccu 100.000 (99.250)\n",
      "Epoch-106 220 batches\tloss 0.7625 (0.7878)\taccu 100.000 (99.254)\n",
      "Epoch-106 240 batches\tloss 0.7700 (0.7875)\taccu 100.000 (99.245)\n",
      "Epoch-106 260 batches\tloss 0.7711 (0.7878)\taccu 100.000 (99.243)\n",
      "Epoch-106 280 batches\tloss 0.8047 (0.7873)\taccu 100.000 (99.275)\n",
      "Epoch-106 300 batches\tloss 0.7735 (0.7872)\taccu 100.000 (99.286)\n",
      "Epoch-106 320 batches\tloss 0.7875 (0.7872)\taccu 100.000 (99.292)\n",
      "Epoch-106 340 batches\tloss 0.7858 (0.7870)\taccu 98.438 (99.283)\n",
      "Epoch-106 360 batches\tloss 0.7797 (0.7865)\taccu 100.000 (99.301)\n",
      "Epoch-106 380 batches\tloss 0.8132 (0.7865)\taccu 96.875 (99.301)\n",
      "Epoch-106 400 batches\tloss 0.8047 (0.7870)\taccu 98.438 (99.301)\n",
      "Epoch-106 420 batches\tloss 0.8249 (0.7870)\taccu 96.875 (99.304)\n",
      "Epoch-106 440 batches\tloss 0.7648 (0.7871)\taccu 100.000 (99.279)\n",
      "Epoch-106 460 batches\tloss 0.8262 (0.7871)\taccu 100.000 (99.293)\n",
      "Epoch-106 480 batches\tloss 0.7427 (0.7869)\taccu 100.000 (99.307)\n",
      "Epoch-106 89.0s\tTrain: loss 0.7871\taccu 99.2929\tValid: loss 0.9725\taccu 92.0991\n",
      "Epoch 106: val_acc did not improve\n",
      "106 1e-05\n",
      "Epoch-107  20 batches\tloss 0.7802 (0.7826)\taccu 100.000 (99.531)\n",
      "Epoch-107  40 batches\tloss 0.8044 (0.7824)\taccu 100.000 (99.609)\n",
      "Epoch-107  60 batches\tloss 0.7729 (0.7845)\taccu 100.000 (99.531)\n",
      "Epoch-107  80 batches\tloss 0.7848 (0.7876)\taccu 100.000 (99.336)\n",
      "Epoch-107 100 batches\tloss 0.7942 (0.7860)\taccu 100.000 (99.359)\n",
      "Epoch-107 120 batches\tloss 0.7698 (0.7864)\taccu 100.000 (99.310)\n",
      "Epoch-107 140 batches\tloss 0.8061 (0.7859)\taccu 98.438 (99.330)\n",
      "Epoch-107 160 batches\tloss 0.7910 (0.7854)\taccu 100.000 (99.346)\n",
      "Epoch-107 180 batches\tloss 0.7785 (0.7852)\taccu 98.438 (99.340)\n",
      "Epoch-107 200 batches\tloss 0.8420 (0.7855)\taccu 96.875 (99.305)\n",
      "Epoch-107 220 batches\tloss 0.7814 (0.7848)\taccu 100.000 (99.318)\n",
      "Epoch-107 240 batches\tloss 0.7827 (0.7844)\taccu 100.000 (99.329)\n",
      "Epoch-107 260 batches\tloss 0.8041 (0.7851)\taccu 100.000 (99.297)\n",
      "Epoch-107 280 batches\tloss 0.8099 (0.7853)\taccu 100.000 (99.319)\n",
      "Epoch-107 300 batches\tloss 0.7975 (0.7850)\taccu 96.875 (99.339)\n",
      "Epoch-107 320 batches\tloss 0.8155 (0.7855)\taccu 98.438 (99.326)\n",
      "Epoch-107 340 batches\tloss 0.7898 (0.7858)\taccu 98.438 (99.297)\n",
      "Epoch-107 360 batches\tloss 0.7756 (0.7862)\taccu 100.000 (99.280)\n",
      "Epoch-107 380 batches\tloss 0.8312 (0.7862)\taccu 98.438 (99.297)\n",
      "Epoch-107 400 batches\tloss 0.7817 (0.7863)\taccu 100.000 (99.293)\n",
      "Epoch-107 420 batches\tloss 0.7768 (0.7861)\taccu 98.438 (99.301)\n",
      "Epoch-107 440 batches\tloss 0.7771 (0.7866)\taccu 100.000 (99.286)\n",
      "Epoch-107 460 batches\tloss 0.7730 (0.7868)\taccu 100.000 (99.287)\n",
      "Epoch-107 480 batches\tloss 0.7834 (0.7870)\taccu 100.000 (99.284)\n",
      "Epoch-107 89.1s\tTrain: loss 0.7868\taccu 99.2898\tValid: loss 0.9738\taccu 92.2907\n",
      "Epoch 107: val_acc did not improve\n",
      "107 1e-05\n",
      "Epoch-108  20 batches\tloss 0.8464 (0.7933)\taccu 98.438 (99.453)\n",
      "Epoch-108  40 batches\tloss 0.7734 (0.7911)\taccu 100.000 (99.453)\n",
      "Epoch-108  60 batches\tloss 0.7954 (0.7929)\taccu 100.000 (99.297)\n",
      "Epoch-108  80 batches\tloss 0.7940 (0.7944)\taccu 100.000 (99.277)\n",
      "Epoch-108 100 batches\tloss 0.7998 (0.7930)\taccu 96.875 (99.250)\n",
      "Epoch-108 120 batches\tloss 0.7643 (0.7913)\taccu 100.000 (99.297)\n",
      "Epoch-108 140 batches\tloss 0.7869 (0.7897)\taccu 98.438 (99.330)\n",
      "Epoch-108 160 batches\tloss 0.7969 (0.7891)\taccu 98.438 (99.307)\n",
      "Epoch-108 180 batches\tloss 0.8100 (0.7886)\taccu 95.312 (99.271)\n",
      "Epoch-108 200 batches\tloss 0.7623 (0.7879)\taccu 100.000 (99.289)\n",
      "Epoch-108 220 batches\tloss 0.7760 (0.7879)\taccu 100.000 (99.304)\n",
      "Epoch-108 240 batches\tloss 0.8243 (0.7876)\taccu 98.438 (99.310)\n",
      "Epoch-108 260 batches\tloss 0.8247 (0.7880)\taccu 98.438 (99.321)\n",
      "Epoch-108 280 batches\tloss 0.7557 (0.7879)\taccu 100.000 (99.308)\n",
      "Epoch-108 300 batches\tloss 0.7671 (0.7875)\taccu 100.000 (99.333)\n",
      "Epoch-108 320 batches\tloss 0.7873 (0.7876)\taccu 100.000 (99.331)\n",
      "Epoch-108 340 batches\tloss 0.7711 (0.7874)\taccu 100.000 (99.343)\n",
      "Epoch-108 360 batches\tloss 0.8528 (0.7883)\taccu 98.438 (99.323)\n",
      "Epoch-108 380 batches\tloss 0.7520 (0.7879)\taccu 100.000 (99.326)\n",
      "Epoch-108 400 batches\tloss 0.7708 (0.7881)\taccu 100.000 (99.312)\n",
      "Epoch-108 420 batches\tloss 0.7865 (0.7883)\taccu 98.438 (99.297)\n",
      "Epoch-108 440 batches\tloss 0.7875 (0.7885)\taccu 98.438 (99.272)\n",
      "Epoch-108 460 batches\tloss 0.8414 (0.7888)\taccu 96.875 (99.260)\n",
      "Epoch-108 480 batches\tloss 0.7669 (0.7892)\taccu 100.000 (99.261)\n",
      "Epoch-108 89.2s\tTrain: loss 0.7890\taccu 99.2677\tValid: loss 0.9726\taccu 92.0843\n",
      "Epoch 108: val_acc did not improve\n",
      "108 1e-05\n",
      "Epoch-109  20 batches\tloss 0.8551 (0.8115)\taccu 95.312 (98.203)\n",
      "Epoch-109  40 batches\tloss 0.7467 (0.7971)\taccu 100.000 (98.789)\n",
      "Epoch-109  60 batches\tloss 0.7880 (0.7929)\taccu 98.438 (99.036)\n",
      "Epoch-109  80 batches\tloss 0.7892 (0.7948)\taccu 98.438 (99.043)\n",
      "Epoch-109 100 batches\tloss 0.7743 (0.7927)\taccu 100.000 (99.109)\n",
      "Epoch-109 120 batches\tloss 0.7699 (0.7920)\taccu 100.000 (99.141)\n",
      "Epoch-109 140 batches\tloss 0.8368 (0.7917)\taccu 100.000 (99.174)\n",
      "Epoch-109 160 batches\tloss 0.7785 (0.7903)\taccu 100.000 (99.209)\n",
      "Epoch-109 180 batches\tloss 0.8311 (0.7895)\taccu 98.438 (99.253)\n",
      "Epoch-109 200 batches\tloss 0.8337 (0.7898)\taccu 96.875 (99.234)\n",
      "Epoch-109 220 batches\tloss 0.7801 (0.7893)\taccu 98.438 (99.254)\n",
      "Epoch-109 240 batches\tloss 0.8020 (0.7889)\taccu 100.000 (99.251)\n",
      "Epoch-109 260 batches\tloss 0.7784 (0.7884)\taccu 100.000 (99.255)\n",
      "Epoch-109 280 batches\tloss 0.7785 (0.7876)\taccu 100.000 (99.286)\n",
      "Epoch-109 300 batches\tloss 0.8919 (0.7878)\taccu 92.188 (99.271)\n",
      "Epoch-109 320 batches\tloss 0.8154 (0.7877)\taccu 95.312 (99.263)\n",
      "Epoch-109 340 batches\tloss 0.7474 (0.7879)\taccu 100.000 (99.256)\n",
      "Epoch-109 360 batches\tloss 0.8170 (0.7885)\taccu 98.438 (99.240)\n",
      "Epoch-109 380 batches\tloss 0.7700 (0.7886)\taccu 100.000 (99.243)\n",
      "Epoch-109 400 batches\tloss 0.8186 (0.7885)\taccu 100.000 (99.246)\n",
      "Epoch-109 420 batches\tloss 0.7717 (0.7883)\taccu 100.000 (99.249)\n",
      "Epoch-109 440 batches\tloss 0.7759 (0.7880)\taccu 100.000 (99.254)\n",
      "Epoch-109 460 batches\tloss 0.8212 (0.7876)\taccu 98.438 (99.249)\n",
      "Epoch-109 480 batches\tloss 0.7526 (0.7875)\taccu 100.000 (99.255)\n",
      "Epoch-109 89.3s\tTrain: loss 0.7877\taccu 99.2582\tValid: loss 0.9745\taccu 92.1138\n",
      "Epoch 109: val_acc did not improve\n",
      "109 1e-05\n",
      "Epoch-110  20 batches\tloss 0.8091 (0.7840)\taccu 98.438 (99.375)\n",
      "Epoch-110  40 batches\tloss 0.7718 (0.7880)\taccu 100.000 (99.062)\n",
      "Epoch-110  60 batches\tloss 0.7670 (0.7872)\taccu 100.000 (99.323)\n",
      "Epoch-110  80 batches\tloss 0.8076 (0.7885)\taccu 98.438 (99.199)\n",
      "Epoch-110 100 batches\tloss 0.7685 (0.7877)\taccu 100.000 (99.219)\n",
      "Epoch-110 120 batches\tloss 0.7750 (0.7871)\taccu 100.000 (99.180)\n",
      "Epoch-110 140 batches\tloss 0.7822 (0.7859)\taccu 100.000 (99.196)\n",
      "Epoch-110 160 batches\tloss 0.8315 (0.7859)\taccu 96.875 (99.199)\n",
      "Epoch-110 180 batches\tloss 0.7620 (0.7845)\taccu 100.000 (99.262)\n",
      "Epoch-110 200 batches\tloss 0.8513 (0.7851)\taccu 98.438 (99.250)\n",
      "Epoch-110 220 batches\tloss 0.8014 (0.7851)\taccu 100.000 (99.304)\n",
      "Epoch-110 240 batches\tloss 0.7561 (0.7850)\taccu 100.000 (99.290)\n",
      "Epoch-110 260 batches\tloss 0.7652 (0.7849)\taccu 100.000 (99.309)\n",
      "Epoch-110 280 batches\tloss 0.8192 (0.7857)\taccu 96.875 (99.291)\n",
      "Epoch-110 300 batches\tloss 0.7959 (0.7853)\taccu 98.438 (99.292)\n",
      "Epoch-110 320 batches\tloss 0.7820 (0.7849)\taccu 100.000 (99.312)\n",
      "Epoch-110 340 batches\tloss 0.8374 (0.7857)\taccu 96.875 (99.269)\n",
      "Epoch-110 360 batches\tloss 0.7953 (0.7859)\taccu 98.438 (99.258)\n",
      "Epoch-110 380 batches\tloss 0.8248 (0.7854)\taccu 98.438 (99.272)\n",
      "Epoch-110 400 batches\tloss 0.8120 (0.7853)\taccu 98.438 (99.273)\n",
      "Epoch-110 420 batches\tloss 0.7805 (0.7851)\taccu 100.000 (99.293)\n",
      "Epoch-110 440 batches\tloss 0.7820 (0.7849)\taccu 100.000 (99.304)\n",
      "Epoch-110 460 batches\tloss 0.8094 (0.7851)\taccu 96.875 (99.290)\n",
      "Epoch-110 480 batches\tloss 0.7949 (0.7849)\taccu 100.000 (99.300)\n",
      "Epoch-110 89.1s\tTrain: loss 0.7848\taccu 99.3119\tValid: loss 0.9737\taccu 92.2317\n",
      "Epoch 110: val_acc did not improve\n",
      "110 1.0000000000000002e-06\n",
      "Epoch-111  20 batches\tloss 0.8076 (0.7920)\taccu 98.438 (99.141)\n",
      "Epoch-111  40 batches\tloss 0.8228 (0.7913)\taccu 96.875 (99.141)\n",
      "Epoch-111  60 batches\tloss 0.7664 (0.7907)\taccu 98.438 (99.245)\n",
      "Epoch-111  80 batches\tloss 0.7811 (0.7925)\taccu 100.000 (99.180)\n",
      "Epoch-111 100 batches\tloss 0.7674 (0.7910)\taccu 100.000 (99.219)\n",
      "Epoch-111 120 batches\tloss 0.8554 (0.7907)\taccu 96.875 (99.180)\n",
      "Epoch-111 140 batches\tloss 0.7890 (0.7892)\taccu 98.438 (99.208)\n",
      "Epoch-111 160 batches\tloss 0.7731 (0.7878)\taccu 100.000 (99.277)\n",
      "Epoch-111 180 batches\tloss 0.7473 (0.7869)\taccu 100.000 (99.297)\n",
      "Epoch-111 200 batches\tloss 0.7732 (0.7871)\taccu 100.000 (99.289)\n",
      "Epoch-111 220 batches\tloss 0.7964 (0.7872)\taccu 98.438 (99.290)\n",
      "Epoch-111 240 batches\tloss 0.7944 (0.7881)\taccu 100.000 (99.225)\n",
      "Epoch-111 260 batches\tloss 0.7744 (0.7879)\taccu 100.000 (99.249)\n",
      "Epoch-111 280 batches\tloss 0.8396 (0.7877)\taccu 98.438 (99.269)\n",
      "Epoch-111 300 batches\tloss 0.7821 (0.7877)\taccu 98.438 (99.276)\n",
      "Epoch-111 320 batches\tloss 0.7677 (0.7878)\taccu 100.000 (99.253)\n",
      "Epoch-111 340 batches\tloss 0.7869 (0.7882)\taccu 98.438 (99.251)\n",
      "Epoch-111 360 batches\tloss 0.7795 (0.7876)\taccu 100.000 (99.275)\n",
      "Epoch-111 380 batches\tloss 0.7817 (0.7878)\taccu 100.000 (99.268)\n",
      "Epoch-111 400 batches\tloss 0.7774 (0.7878)\taccu 100.000 (99.285)\n",
      "Epoch-111 420 batches\tloss 0.7686 (0.7878)\taccu 100.000 (99.282)\n",
      "Epoch-111 440 batches\tloss 0.7782 (0.7879)\taccu 100.000 (99.272)\n",
      "Epoch-111 460 batches\tloss 0.7719 (0.7877)\taccu 98.438 (99.273)\n",
      "Epoch-111 480 batches\tloss 0.8233 (0.7878)\taccu 96.875 (99.248)\n",
      "Epoch-111 89.1s\tTrain: loss 0.7882\taccu 99.2235\tValid: loss 0.9712\taccu 92.3349\n",
      "Epoch 111: val_acc did not improve\n",
      "111 1.0000000000000002e-06\n",
      "Epoch-112  20 batches\tloss 0.7944 (0.7893)\taccu 100.000 (99.375)\n",
      "Epoch-112  40 batches\tloss 0.7713 (0.7903)\taccu 100.000 (99.258)\n",
      "Epoch-112  60 batches\tloss 0.7784 (0.7895)\taccu 100.000 (99.271)\n",
      "Epoch-112  80 batches\tloss 0.7626 (0.7895)\taccu 100.000 (99.277)\n",
      "Epoch-112 100 batches\tloss 0.7601 (0.7893)\taccu 100.000 (99.234)\n",
      "Epoch-112 120 batches\tloss 0.7824 (0.7885)\taccu 98.438 (99.206)\n",
      "Epoch-112 140 batches\tloss 0.8302 (0.7872)\taccu 98.438 (99.263)\n",
      "Epoch-112 160 batches\tloss 0.8024 (0.7870)\taccu 98.438 (99.336)\n",
      "Epoch-112 180 batches\tloss 0.8468 (0.7873)\taccu 96.875 (99.297)\n",
      "Epoch-112 200 batches\tloss 0.8384 (0.7877)\taccu 98.438 (99.281)\n",
      "Epoch-112 220 batches\tloss 0.7778 (0.7879)\taccu 100.000 (99.261)\n",
      "Epoch-112 240 batches\tloss 0.7963 (0.7872)\taccu 100.000 (99.290)\n",
      "Epoch-112 260 batches\tloss 0.7777 (0.7870)\taccu 100.000 (99.291)\n",
      "Epoch-112 280 batches\tloss 0.7712 (0.7869)\taccu 100.000 (99.302)\n",
      "Epoch-112 300 batches\tloss 0.8469 (0.7880)\taccu 98.438 (99.281)\n",
      "Epoch-112 320 batches\tloss 0.8084 (0.7879)\taccu 98.438 (99.277)\n",
      "Epoch-112 340 batches\tloss 0.7857 (0.7881)\taccu 100.000 (99.260)\n",
      "Epoch-112 360 batches\tloss 0.7778 (0.7877)\taccu 100.000 (99.262)\n",
      "Epoch-112 380 batches\tloss 0.7654 (0.7878)\taccu 100.000 (99.260)\n",
      "Epoch-112 400 batches\tloss 0.7736 (0.7873)\taccu 100.000 (99.277)\n",
      "Epoch-112 420 batches\tloss 0.7850 (0.7867)\taccu 98.438 (99.293)\n",
      "Epoch-112 440 batches\tloss 0.8288 (0.7871)\taccu 96.875 (99.276)\n",
      "Epoch-112 460 batches\tloss 0.8079 (0.7872)\taccu 100.000 (99.276)\n",
      "Epoch-112 480 batches\tloss 0.7852 (0.7869)\taccu 98.438 (99.281)\n",
      "Epoch-112 89.0s\tTrain: loss 0.7869\taccu 99.2929\tValid: loss 0.9744\taccu 92.1875\n",
      "Epoch 112: val_acc did not improve\n",
      "112 1.0000000000000002e-06\n",
      "Epoch-113  20 batches\tloss 0.7895 (0.7835)\taccu 100.000 (99.453)\n",
      "Epoch-113  40 batches\tloss 0.8300 (0.7880)\taccu 96.875 (99.297)\n",
      "Epoch-113  60 batches\tloss 0.8411 (0.7906)\taccu 95.312 (99.141)\n",
      "Epoch-113  80 batches\tloss 0.7893 (0.7887)\taccu 98.438 (99.219)\n",
      "Epoch-113 100 batches\tloss 0.7782 (0.7885)\taccu 100.000 (99.219)\n",
      "Epoch-113 120 batches\tloss 0.7606 (0.7876)\taccu 100.000 (99.206)\n",
      "Epoch-113 140 batches\tloss 0.7829 (0.7878)\taccu 100.000 (99.208)\n",
      "Epoch-113 160 batches\tloss 0.7734 (0.7870)\taccu 100.000 (99.219)\n",
      "Epoch-113 180 batches\tloss 0.7993 (0.7875)\taccu 96.875 (99.193)\n",
      "Epoch-113 200 batches\tloss 0.7856 (0.7881)\taccu 100.000 (99.203)\n",
      "Epoch-113 220 batches\tloss 0.7898 (0.7878)\taccu 98.438 (99.240)\n",
      "Epoch-113 240 batches\tloss 0.7708 (0.7881)\taccu 100.000 (99.245)\n",
      "Epoch-113 260 batches\tloss 0.8113 (0.7888)\taccu 100.000 (99.261)\n",
      "Epoch-113 280 batches\tloss 0.7782 (0.7891)\taccu 100.000 (99.247)\n",
      "Epoch-113 300 batches\tloss 0.8080 (0.7890)\taccu 98.438 (99.234)\n",
      "Epoch-113 320 batches\tloss 0.8058 (0.7891)\taccu 100.000 (99.229)\n",
      "Epoch-113 340 batches\tloss 0.7563 (0.7890)\taccu 100.000 (99.237)\n",
      "Epoch-113 360 batches\tloss 0.7621 (0.7884)\taccu 100.000 (99.240)\n",
      "Epoch-113 380 batches\tloss 0.7939 (0.7882)\taccu 98.438 (99.248)\n",
      "Epoch-113 400 batches\tloss 0.7680 (0.7876)\taccu 100.000 (99.273)\n",
      "Epoch-113 420 batches\tloss 0.7595 (0.7877)\taccu 100.000 (99.275)\n",
      "Epoch-113 440 batches\tloss 0.7625 (0.7876)\taccu 100.000 (99.279)\n",
      "Epoch-113 460 batches\tloss 0.8131 (0.7877)\taccu 96.875 (99.276)\n",
      "Epoch-113 480 batches\tloss 0.9367 (0.7880)\taccu 92.188 (99.261)\n",
      "Epoch-113 89.2s\tTrain: loss 0.7882\taccu 99.2645\tValid: loss 0.9730\taccu 92.1875\n",
      "Epoch 113: val_acc did not improve\n",
      "113 1.0000000000000002e-06\n",
      "Epoch-114  20 batches\tloss 0.7753 (0.7829)\taccu 100.000 (99.688)\n",
      "Epoch-114  40 batches\tloss 0.7618 (0.7850)\taccu 100.000 (99.531)\n",
      "Epoch-114  60 batches\tloss 0.7824 (0.7877)\taccu 100.000 (99.453)\n",
      "Epoch-114  80 batches\tloss 0.7859 (0.7887)\taccu 100.000 (99.453)\n",
      "Epoch-114 100 batches\tloss 0.8369 (0.7892)\taccu 96.875 (99.359)\n",
      "Epoch-114 120 batches\tloss 0.7603 (0.7874)\taccu 100.000 (99.388)\n",
      "Epoch-114 140 batches\tloss 0.8243 (0.7871)\taccu 98.438 (99.386)\n",
      "Epoch-114 160 batches\tloss 0.7598 (0.7893)\taccu 100.000 (99.307)\n",
      "Epoch-114 180 batches\tloss 0.7683 (0.7894)\taccu 100.000 (99.262)\n",
      "Epoch-114 200 batches\tloss 0.7823 (0.7888)\taccu 98.438 (99.281)\n",
      "Epoch-114 220 batches\tloss 0.7751 (0.7888)\taccu 100.000 (99.268)\n",
      "Epoch-114 240 batches\tloss 0.7816 (0.7887)\taccu 100.000 (99.251)\n",
      "Epoch-114 260 batches\tloss 0.7584 (0.7885)\taccu 100.000 (99.261)\n",
      "Epoch-114 280 batches\tloss 0.7549 (0.7891)\taccu 100.000 (99.241)\n",
      "Epoch-114 300 batches\tloss 0.7816 (0.7887)\taccu 100.000 (99.250)\n",
      "Epoch-114 320 batches\tloss 0.7535 (0.7881)\taccu 100.000 (99.268)\n",
      "Epoch-114 340 batches\tloss 0.7442 (0.7881)\taccu 100.000 (99.278)\n",
      "Epoch-114 360 batches\tloss 0.7781 (0.7880)\taccu 100.000 (99.293)\n",
      "Epoch-114 380 batches\tloss 0.7949 (0.7880)\taccu 98.438 (99.280)\n",
      "Epoch-114 400 batches\tloss 0.7983 (0.7879)\taccu 98.438 (99.270)\n",
      "Epoch-114 420 batches\tloss 0.7643 (0.7880)\taccu 100.000 (99.267)\n",
      "Epoch-114 440 batches\tloss 0.7580 (0.7879)\taccu 100.000 (99.261)\n",
      "Epoch-114 460 batches\tloss 0.7858 (0.7878)\taccu 100.000 (99.270)\n",
      "Epoch-114 480 batches\tloss 0.7556 (0.7875)\taccu 100.000 (99.268)\n",
      "Epoch-114 89.2s\tTrain: loss 0.7878\taccu 99.2614\tValid: loss 0.9720\taccu 91.9811\n",
      "Epoch 114: val_acc did not improve\n",
      "114 1.0000000000000002e-06\n",
      "Epoch-115  20 batches\tloss 0.7629 (0.7818)\taccu 100.000 (99.688)\n",
      "Epoch-115  40 batches\tloss 0.7675 (0.7866)\taccu 100.000 (99.219)\n",
      "Epoch-115  60 batches\tloss 0.7778 (0.7851)\taccu 100.000 (99.271)\n",
      "Epoch-115  80 batches\tloss 0.8326 (0.7837)\taccu 98.438 (99.395)\n",
      "Epoch-115 100 batches\tloss 0.7575 (0.7848)\taccu 100.000 (99.359)\n",
      "Epoch-115 120 batches\tloss 0.7686 (0.7827)\taccu 100.000 (99.453)\n",
      "Epoch-115 140 batches\tloss 0.7769 (0.7823)\taccu 100.000 (99.431)\n",
      "Epoch-115 160 batches\tloss 0.8070 (0.7830)\taccu 98.438 (99.404)\n",
      "Epoch-115 180 batches\tloss 0.7648 (0.7838)\taccu 100.000 (99.401)\n",
      "Epoch-115 200 batches\tloss 0.7984 (0.7837)\taccu 98.438 (99.406)\n",
      "Epoch-115 220 batches\tloss 0.7741 (0.7848)\taccu 100.000 (99.354)\n",
      "Epoch-115 240 batches\tloss 0.7859 (0.7856)\taccu 98.438 (99.323)\n",
      "Epoch-115 260 batches\tloss 0.7968 (0.7852)\taccu 100.000 (99.345)\n",
      "Epoch-115 280 batches\tloss 0.7555 (0.7857)\taccu 100.000 (99.314)\n",
      "Epoch-115 300 batches\tloss 0.7560 (0.7852)\taccu 100.000 (99.323)\n",
      "Epoch-115 320 batches\tloss 0.7725 (0.7853)\taccu 100.000 (99.326)\n",
      "Epoch-115 340 batches\tloss 0.7913 (0.7853)\taccu 98.438 (99.324)\n",
      "Epoch-115 360 batches\tloss 0.7961 (0.7852)\taccu 98.438 (99.340)\n",
      "Epoch-115 380 batches\tloss 0.7812 (0.7850)\taccu 100.000 (99.338)\n",
      "Epoch-115 400 batches\tloss 0.8113 (0.7853)\taccu 98.438 (99.316)\n",
      "Epoch-115 420 batches\tloss 0.7559 (0.7855)\taccu 100.000 (99.319)\n",
      "Epoch-115 440 batches\tloss 0.7758 (0.7855)\taccu 100.000 (99.336)\n",
      "Epoch-115 460 batches\tloss 0.7893 (0.7859)\taccu 98.438 (99.334)\n",
      "Epoch-115 480 batches\tloss 0.7930 (0.7859)\taccu 100.000 (99.336)\n",
      "Epoch-115 89.3s\tTrain: loss 0.7861\taccu 99.3245\tValid: loss 0.9738\taccu 92.1285\n",
      "Epoch 115: val_acc did not improve\n",
      "115 1.0000000000000002e-06\n",
      "Epoch-116  20 batches\tloss 0.8339 (0.7804)\taccu 95.312 (99.297)\n",
      "Epoch-116  40 batches\tloss 0.8065 (0.7826)\taccu 96.875 (99.375)\n",
      "Epoch-116  60 batches\tloss 0.7852 (0.7818)\taccu 100.000 (99.401)\n",
      "Epoch-116  80 batches\tloss 0.7847 (0.7826)\taccu 98.438 (99.434)\n",
      "Epoch-116 100 batches\tloss 0.7585 (0.7831)\taccu 100.000 (99.438)\n",
      "Epoch-116 120 batches\tloss 0.7941 (0.7828)\taccu 100.000 (99.427)\n",
      "Epoch-116 140 batches\tloss 0.7699 (0.7829)\taccu 100.000 (99.464)\n",
      "Epoch-116 160 batches\tloss 0.7628 (0.7833)\taccu 98.438 (99.424)\n",
      "Epoch-116 180 batches\tloss 0.7538 (0.7845)\taccu 100.000 (99.418)\n",
      "Epoch-116 200 batches\tloss 0.7874 (0.7847)\taccu 98.438 (99.406)\n",
      "Epoch-116 220 batches\tloss 0.7756 (0.7863)\taccu 100.000 (99.361)\n",
      "Epoch-116 240 batches\tloss 0.7704 (0.7859)\taccu 100.000 (99.362)\n",
      "Epoch-116 260 batches\tloss 0.7602 (0.7864)\taccu 100.000 (99.345)\n",
      "Epoch-116 280 batches\tloss 0.7794 (0.7859)\taccu 100.000 (99.353)\n",
      "Epoch-116 300 batches\tloss 0.7818 (0.7863)\taccu 100.000 (99.354)\n",
      "Epoch-116 320 batches\tloss 0.7495 (0.7864)\taccu 100.000 (99.336)\n",
      "Epoch-116 340 batches\tloss 0.8148 (0.7868)\taccu 98.438 (99.334)\n",
      "Epoch-116 360 batches\tloss 0.7943 (0.7863)\taccu 98.438 (99.340)\n",
      "Epoch-116 380 batches\tloss 0.7847 (0.7863)\taccu 100.000 (99.326)\n",
      "Epoch-116 400 batches\tloss 0.8134 (0.7862)\taccu 98.438 (99.332)\n",
      "Epoch-116 420 batches\tloss 0.7869 (0.7868)\taccu 100.000 (99.308)\n",
      "Epoch-116 440 batches\tloss 0.7731 (0.7866)\taccu 98.438 (99.322)\n",
      "Epoch-116 460 batches\tloss 0.7664 (0.7865)\taccu 100.000 (99.331)\n",
      "Epoch-116 480 batches\tloss 0.7728 (0.7864)\taccu 100.000 (99.339)\n",
      "Epoch-116 89.1s\tTrain: loss 0.7863\taccu 99.3403\tValid: loss 0.9738\taccu 92.2317\n",
      "Epoch 116: val_acc did not improve\n",
      "116 1.0000000000000002e-06\n",
      "Epoch-117  20 batches\tloss 0.7725 (0.7886)\taccu 100.000 (99.609)\n",
      "Epoch-117  40 batches\tloss 0.7659 (0.7867)\taccu 100.000 (99.453)\n",
      "Epoch-117  60 batches\tloss 0.7944 (0.7832)\taccu 98.438 (99.479)\n",
      "Epoch-117  80 batches\tloss 0.7941 (0.7862)\taccu 98.438 (99.297)\n",
      "Epoch-117 100 batches\tloss 0.7813 (0.7848)\taccu 100.000 (99.359)\n",
      "Epoch-117 120 batches\tloss 0.7509 (0.7845)\taccu 100.000 (99.375)\n",
      "Epoch-117 140 batches\tloss 0.7739 (0.7851)\taccu 100.000 (99.319)\n",
      "Epoch-117 160 batches\tloss 0.8015 (0.7854)\taccu 98.438 (99.326)\n",
      "Epoch-117 180 batches\tloss 0.8075 (0.7857)\taccu 98.438 (99.332)\n",
      "Epoch-117 200 batches\tloss 0.8027 (0.7851)\taccu 98.438 (99.336)\n",
      "Epoch-117 220 batches\tloss 0.7734 (0.7855)\taccu 100.000 (99.325)\n",
      "Epoch-117 240 batches\tloss 0.7803 (0.7861)\taccu 100.000 (99.297)\n",
      "Epoch-117 260 batches\tloss 0.7700 (0.7863)\taccu 100.000 (99.279)\n",
      "Epoch-117 280 batches\tloss 0.7722 (0.7862)\taccu 100.000 (99.302)\n",
      "Epoch-117 300 batches\tloss 0.7948 (0.7866)\taccu 100.000 (99.302)\n",
      "Epoch-117 320 batches\tloss 0.8478 (0.7872)\taccu 96.875 (99.287)\n",
      "Epoch-117 340 batches\tloss 0.7718 (0.7870)\taccu 100.000 (99.278)\n",
      "Epoch-117 360 batches\tloss 0.7544 (0.7868)\taccu 100.000 (99.288)\n",
      "Epoch-117 380 batches\tloss 0.7695 (0.7861)\taccu 100.000 (99.313)\n",
      "Epoch-117 400 batches\tloss 0.7548 (0.7863)\taccu 100.000 (99.289)\n",
      "Epoch-117 420 batches\tloss 0.8147 (0.7863)\taccu 98.438 (99.297)\n",
      "Epoch-117 440 batches\tloss 0.7861 (0.7865)\taccu 100.000 (99.286)\n",
      "Epoch-117 460 batches\tloss 0.7554 (0.7867)\taccu 100.000 (99.260)\n",
      "Epoch-117 480 batches\tloss 0.8123 (0.7863)\taccu 100.000 (99.264)\n",
      "Epoch-117 89.5s\tTrain: loss 0.7864\taccu 99.2772\tValid: loss 0.9734\taccu 92.1138\n",
      "Epoch 117: val_acc did not improve\n",
      "117 1.0000000000000002e-06\n",
      "Epoch-118  20 batches\tloss 0.8265 (0.7823)\taccu 98.438 (99.141)\n",
      "Epoch-118  40 batches\tloss 0.7717 (0.7823)\taccu 100.000 (99.258)\n",
      "Epoch-118  60 batches\tloss 0.7821 (0.7822)\taccu 100.000 (99.245)\n",
      "Epoch-118  80 batches\tloss 0.8070 (0.7825)\taccu 98.438 (99.316)\n",
      "Epoch-118 100 batches\tloss 0.7597 (0.7828)\taccu 100.000 (99.312)\n",
      "Epoch-118 120 batches\tloss 0.7668 (0.7846)\taccu 100.000 (99.219)\n",
      "Epoch-118 140 batches\tloss 0.7909 (0.7853)\taccu 100.000 (99.252)\n",
      "Epoch-118 160 batches\tloss 0.8028 (0.7841)\taccu 100.000 (99.316)\n",
      "Epoch-118 180 batches\tloss 0.7598 (0.7853)\taccu 100.000 (99.288)\n",
      "Epoch-118 200 batches\tloss 0.7713 (0.7856)\taccu 100.000 (99.312)\n",
      "Epoch-118 220 batches\tloss 0.7878 (0.7850)\taccu 98.438 (99.318)\n",
      "Epoch-118 240 batches\tloss 0.7723 (0.7851)\taccu 100.000 (99.316)\n",
      "Epoch-118 260 batches\tloss 0.7793 (0.7848)\taccu 100.000 (99.339)\n",
      "Epoch-118 280 batches\tloss 0.7725 (0.7846)\taccu 100.000 (99.342)\n",
      "Epoch-118 300 batches\tloss 0.7858 (0.7851)\taccu 100.000 (99.349)\n",
      "Epoch-118 320 batches\tloss 0.7529 (0.7851)\taccu 100.000 (99.360)\n",
      "Epoch-118 340 batches\tloss 0.8887 (0.7851)\taccu 96.875 (99.370)\n",
      "Epoch-118 360 batches\tloss 0.7933 (0.7852)\taccu 98.438 (99.362)\n",
      "Epoch-118 380 batches\tloss 0.8087 (0.7851)\taccu 98.438 (99.367)\n",
      "Epoch-118 400 batches\tloss 0.8556 (0.7855)\taccu 96.875 (99.348)\n",
      "Epoch-118 420 batches\tloss 0.7657 (0.7852)\taccu 100.000 (99.349)\n",
      "Epoch-118 440 batches\tloss 0.7819 (0.7853)\taccu 100.000 (99.354)\n",
      "Epoch-118 460 batches\tloss 0.8176 (0.7849)\taccu 96.875 (99.331)\n",
      "Epoch-118 480 batches\tloss 0.7762 (0.7849)\taccu 100.000 (99.329)\n",
      "Epoch-118 89.4s\tTrain: loss 0.7853\taccu 99.3056\tValid: loss 0.9739\taccu 92.1875\n",
      "Epoch 118: val_acc did not improve\n",
      "118 1.0000000000000002e-06\n",
      "Epoch-119  20 batches\tloss 0.7822 (0.7801)\taccu 100.000 (99.375)\n",
      "Epoch-119  40 batches\tloss 0.7530 (0.7818)\taccu 100.000 (99.336)\n",
      "Epoch-119  60 batches\tloss 0.7926 (0.7804)\taccu 98.438 (99.349)\n",
      "Epoch-119  80 batches\tloss 0.7715 (0.7811)\taccu 100.000 (99.355)\n",
      "Epoch-119 100 batches\tloss 0.7780 (0.7825)\taccu 100.000 (99.312)\n",
      "Epoch-119 120 batches\tloss 0.7571 (0.7837)\taccu 100.000 (99.284)\n",
      "Epoch-119 140 batches\tloss 0.7682 (0.7841)\taccu 100.000 (99.230)\n",
      "Epoch-119 160 batches\tloss 0.7525 (0.7842)\taccu 100.000 (99.258)\n",
      "Epoch-119 180 batches\tloss 0.7867 (0.7846)\taccu 100.000 (99.271)\n",
      "Epoch-119 200 batches\tloss 0.8029 (0.7849)\taccu 98.438 (99.258)\n",
      "Epoch-119 220 batches\tloss 0.7750 (0.7858)\taccu 98.438 (99.254)\n",
      "Epoch-119 240 batches\tloss 0.8084 (0.7850)\taccu 100.000 (99.290)\n",
      "Epoch-119 260 batches\tloss 0.7923 (0.7844)\taccu 100.000 (99.309)\n",
      "Epoch-119 280 batches\tloss 0.7741 (0.7841)\taccu 100.000 (99.319)\n",
      "Epoch-119 300 batches\tloss 0.7806 (0.7846)\taccu 100.000 (99.292)\n",
      "Epoch-119 320 batches\tloss 0.7748 (0.7849)\taccu 100.000 (99.287)\n",
      "Epoch-119 340 batches\tloss 0.7658 (0.7849)\taccu 100.000 (99.288)\n",
      "Epoch-119 360 batches\tloss 0.7625 (0.7848)\taccu 100.000 (99.297)\n",
      "Epoch-119 380 batches\tloss 0.7740 (0.7846)\taccu 100.000 (99.326)\n",
      "Epoch-119 400 batches\tloss 0.7942 (0.7848)\taccu 100.000 (99.328)\n",
      "Epoch-119 420 batches\tloss 0.7944 (0.7845)\taccu 98.438 (99.334)\n",
      "Epoch-119 440 batches\tloss 0.7714 (0.7845)\taccu 100.000 (99.322)\n",
      "Epoch-119 460 batches\tloss 0.8020 (0.7845)\taccu 98.438 (99.327)\n",
      "Epoch-119 480 batches\tloss 0.7609 (0.7844)\taccu 100.000 (99.329)\n",
      "Epoch-119 89.0s\tTrain: loss 0.7844\taccu 99.3308\tValid: loss 0.9729\taccu 92.2612\n",
      "Epoch 119: val_acc did not improve\n",
      "119 1.0000000000000002e-06\n",
      "Epoch-120  20 batches\tloss 0.7674 (0.7901)\taccu 100.000 (99.531)\n",
      "Epoch-120  40 batches\tloss 0.7454 (0.7870)\taccu 100.000 (99.375)\n",
      "Epoch-120  60 batches\tloss 0.7641 (0.7867)\taccu 100.000 (99.427)\n",
      "Epoch-120  80 batches\tloss 0.7785 (0.7874)\taccu 100.000 (99.375)\n",
      "Epoch-120 100 batches\tloss 0.7777 (0.7892)\taccu 100.000 (99.281)\n",
      "Epoch-120 120 batches\tloss 0.7671 (0.7885)\taccu 100.000 (99.271)\n",
      "Epoch-120 140 batches\tloss 0.7586 (0.7878)\taccu 100.000 (99.297)\n",
      "Epoch-120 160 batches\tloss 0.8017 (0.7874)\taccu 96.875 (99.307)\n",
      "Epoch-120 180 batches\tloss 0.7746 (0.7880)\taccu 100.000 (99.271)\n",
      "Epoch-120 200 batches\tloss 0.7883 (0.7873)\taccu 100.000 (99.289)\n",
      "Epoch-120 220 batches\tloss 0.7745 (0.7877)\taccu 100.000 (99.247)\n",
      "Epoch-120 240 batches\tloss 0.7718 (0.7872)\taccu 100.000 (99.277)\n",
      "Epoch-120 260 batches\tloss 0.7578 (0.7864)\taccu 100.000 (99.303)\n",
      "Epoch-120 280 batches\tloss 0.8344 (0.7862)\taccu 96.875 (99.280)\n",
      "Epoch-120 300 batches\tloss 0.7686 (0.7868)\taccu 100.000 (99.255)\n",
      "Epoch-120 320 batches\tloss 0.7627 (0.7867)\taccu 100.000 (99.258)\n",
      "Epoch-120 340 batches\tloss 0.8207 (0.7866)\taccu 98.438 (99.274)\n",
      "Epoch-120 360 batches\tloss 0.7729 (0.7868)\taccu 100.000 (99.288)\n",
      "Epoch-120 380 batches\tloss 0.7512 (0.7860)\taccu 100.000 (99.305)\n",
      "Epoch-120 400 batches\tloss 0.7646 (0.7857)\taccu 100.000 (99.312)\n",
      "Epoch-120 420 batches\tloss 0.8463 (0.7858)\taccu 98.438 (99.323)\n",
      "Epoch-120 440 batches\tloss 0.7882 (0.7855)\taccu 98.438 (99.315)\n",
      "Epoch-120 460 batches\tloss 0.7849 (0.7858)\taccu 100.000 (99.300)\n",
      "Epoch-120 480 batches\tloss 0.7730 (0.7861)\taccu 100.000 (99.297)\n",
      "Epoch-120 89.1s\tTrain: loss 0.7861\taccu 99.2898\tValid: loss 0.9713\taccu 92.3054\n",
      "Epoch 120: val_acc did not improve\n",
      "Best val_acc: 92.4528 from epoch-95\n",
      "Save train and validation log into into ./SGN/pretrained/SGN\\0_log.csv\n",
      "Test: accuracy 92.600, time: 18.63s\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, do_train)\n",
    "\n",
    "    total = get_n_params(model)\n",
    "    # print(model)\n",
    "    print('The number of parameters: ', total)\n",
    "    print('The modes is:', network)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('It is using GPU!')\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = LabelSmoothingLoss(num_classes, smoothing=0.1).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "\n",
    "    if monitor == 'val_acc':\n",
    "        mode = 'max'\n",
    "        monitor_op = np.greater\n",
    "        best = -np.Inf\n",
    "        str_op = 'improve'\n",
    "    elif monitor == 'val_loss':\n",
    "        mode = 'min'\n",
    "        monitor_op = np.less\n",
    "        best = np.Inf\n",
    "        str_op = 'reduce'\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[60, 90, 110], gamma=0.1)\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, case, seg=seg, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    train_loader = ntu_loaders.get_train_loader(batch_size, workers)\n",
    "    val_loader = ntu_loaders.get_val_loader(batch_size, workers)\n",
    "    train_size = ntu_loaders.get_train_size()\n",
    "    val_size = ntu_loaders.get_val_size()\n",
    "\n",
    "    test_loader = ntu_loaders.get_test_loader(32, workers)\n",
    "\n",
    "    print('Train on %d samples, validate on %d samples' %\n",
    "          (train_size, val_size))\n",
    "\n",
    "    best_epoch = 0\n",
    "    output_dir = make_dir(dataset)\n",
    "\n",
    "    save_path = os.path.join(output_dir, network)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    checkpoint = osp.join(save_path, '%s_best.pth' % case)\n",
    "    earlystop_cnt = 0\n",
    "    csv_file = osp.join(save_path, '%s_log.csv' % case)\n",
    "    log_res = list()\n",
    "\n",
    "    lable_path = osp.join(save_path, '%s_lable.txt' % case)\n",
    "    pred_path = osp.join(save_path, '%s_pred.txt' % case)\n",
    "\n",
    "    # Training\n",
    "    if do_train == 1:\n",
    "        for epoch in range(start_epoch, max_epochs):\n",
    "\n",
    "            print(epoch, optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            t_start = time.time()\n",
    "            train_loss, train_acc = train(\n",
    "                train_loader, model, criterion, optimizer, epoch)\n",
    "            val_loss, val_acc = validate(val_loader, model, criterion)\n",
    "            log_res += [[train_loss, train_acc.cpu().numpy(),\n",
    "                         val_loss, val_acc.cpu().numpy()]]\n",
    "\n",
    "            print('Epoch-{:<3d} {:.1f}s\\t'\n",
    "                  'Train: loss {:.4f}\\taccu {:.4f}\\tValid: loss {:.4f}\\taccu {:.4f}'\n",
    "                  .format(epoch + 1, time.time() - t_start, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "            current = val_loss if mode == 'min' else val_acc\n",
    "\n",
    "            # store tensor in cpu\n",
    "            current = current.cpu()\n",
    "\n",
    "            if monitor_op(current, best):\n",
    "                print('Epoch %d: %s %sd from %.4f to %.4f, '\n",
    "                      'saving model to %s'\n",
    "                      % (epoch + 1, monitor, str_op, best, current, checkpoint))\n",
    "                best = current\n",
    "                best_epoch = epoch + 1\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best': best,\n",
    "                    'monitor': monitor,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "                earlystop_cnt = 0\n",
    "            else:\n",
    "                print('Epoch %d: %s did not %s' % (epoch + 1, monitor, str_op))\n",
    "                earlystop_cnt += 1\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        print('Best %s: %.4f from epoch-%d' % (monitor, best, best_epoch))\n",
    "        with open(csv_file, 'w') as fw:\n",
    "            cw = csv.writer(fw)\n",
    "            cw.writerow(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "            cw.writerows(log_res)\n",
    "        print('Save train and validation log into into %s' % csv_file)\n",
    "\n",
    "    # Test\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, 0)\n",
    "    model = model.cuda()\n",
    "    test(test_loader, model, checkpoint, lable_path, pred_path)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
