{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Paper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=8, global_pool=None, convpool=None, compress=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        model = []\n",
    "        acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        nr_layer = len(channels) - 2 if compress else len(channels) - 1\n",
    "\n",
    "        for i in range(nr_layer):\n",
    "            if convpool is None:\n",
    "                pad = (kernel_size - 2) // 2\n",
    "                model.append(nn.ReflectionPad1d(pad))\n",
    "                model.append(nn.Conv1d(channels[i], channels[i+1],\n",
    "                                   kernel_size=kernel_size, stride=2))\n",
    "                model.append(acti)\n",
    "            else:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "                model.append(nn.ReflectionPad1d(pad))\n",
    "                model.append(nn.Conv1d(channels[i], channels[i+1],\n",
    "                                       kernel_size=kernel_size, stride=1))\n",
    "                model.append(acti)\n",
    "                model.append(convpool(kernel_size=2, stride=2))\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "        self.compress = compress\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        if self.compress:\n",
    "            self.conv1x1 = nn.Conv1d(channels[-2], channels[-1], kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        if self.global_pool is not None:\n",
    "            ks = x.shape[-1]\n",
    "            x = self.global_pool(x, ks)\n",
    "            if self.compress:\n",
    "                x = self.conv1x1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=7):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        model = []\n",
    "        pad = (kernel_size - 1) // 2\n",
    "        acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(channels) - 1):\n",
    "            model.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "            model.append(nn.ReflectionPad1d(pad))\n",
    "            model.append(nn.Conv1d(channels[i], channels[i + 1],\n",
    "                                            kernel_size=kernel_size, stride=1))\n",
    "            if i == 0 or i == 1:\n",
    "                model.append(nn.Dropout(p=0.2))\n",
    "            if not i == len(channels) - 2:\n",
    "                model.append(acti)          # whether to add tanh a last?\n",
    "                #model.append(nn.Dropout(p=0.2))\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class AutoEncoder2x(nn.Module):\n",
    "    def __init__(self, mot_en_channels, body_en_channels, de_channels, global_pool=None, convpool=None, compress=False):\n",
    "        super(AutoEncoder2x, self).__init__()\n",
    "        assert mot_en_channels[0] == de_channels[-1] and \\\n",
    "               mot_en_channels[-1] + body_en_channels[-1] == de_channels[0]\n",
    "\n",
    "        self.mot_encoder = Encoder(mot_en_channels)\n",
    "        self.static_encoder = Encoder(body_en_channels, kernel_size=7, global_pool=global_pool, convpool=convpool, compress=compress)\n",
    "        self.decoder = Decoder(de_channels)\n",
    "\n",
    "    def cross(self, x1, x2):\n",
    "        m1 = self.mot_encoder(x1)\n",
    "        b1 = self.static_encoder(x1[:, :-2, :])\n",
    "        m2 = self.mot_encoder(x2)\n",
    "        b2 = self.static_encoder(x2[:, :-2, :])\n",
    "\n",
    "        out1 = self.decoder(torch.cat([m1, b1.repeat(1, 1, m1.shape[-1])], dim=1))\n",
    "        out2 = self.decoder(torch.cat([m2, b2.repeat(1, 1, m2.shape[-1])], dim=1))\n",
    "        out12 = self.decoder(torch.cat([m1, b2.repeat(1, 1, m1.shape[-1])], dim=1))\n",
    "        out21 = self.decoder(torch.cat([m2, b1.repeat(1, 1, m2.shape[-1])], dim=1))\n",
    "\n",
    "        return out1, out2, out12, out21\n",
    "\n",
    "    def transfer(self, x1, x2):\n",
    "        m1 = self.mot_encoder(x1)\n",
    "        b2 = self.static_encoder(x2[:, :-2, :]).repeat(1, 1, m1.shape[-1])\n",
    "\n",
    "        out12 = self.decoder(torch.cat([m1, b2], dim=1))\n",
    "\n",
    "        return out12\n",
    "\n",
    "    def cross_with_triplet(self, x1, x2, x12, x21):\n",
    "        m1 = self.mot_encoder(x1)\n",
    "        b1 = self.static_encoder(x1[:, :-2, :])\n",
    "        m2 = self.mot_encoder(x2)\n",
    "        b2 = self.static_encoder(x2[:, :-2, :])\n",
    "\n",
    "        out1 = self.decoder(torch.cat([m1, b1.repeat(1, 1, m1.shape[-1])], dim=1))\n",
    "        out2 = self.decoder(torch.cat([m2, b2.repeat(1, 1, m2.shape[-1])], dim=1))\n",
    "        out12 = self.decoder(torch.cat([m1, b2.repeat(1, 1, m1.shape[-1])], dim=1))\n",
    "        out21 = self.decoder(torch.cat([m2, b1.repeat(1, 1, m2.shape[-1])], dim=1))\n",
    "\n",
    "        m12 = self.mot_encoder(x12)\n",
    "        b12 = self.static_encoder(x12[:, :-2, :])\n",
    "        m21 = self.mot_encoder(x21)\n",
    "        b21 = self.static_encoder(x21[:, :-2, :])\n",
    "\n",
    "        outputs = [out1, out2, out12, out21]\n",
    "        motionvecs = [m1.reshape(m1.shape[0], -1),\n",
    "                      m2.reshape(m2.shape[0], -1),\n",
    "                      m12.reshape(m12.shape[0], -1),\n",
    "                      m21.reshape(m21.shape[0], -1)]\n",
    "        bodyvecs = [b1.reshape(b1.shape[0], -1),\n",
    "                      b2.reshape(b2.shape[0], -1),\n",
    "                      b21.reshape(b21.shape[0], -1),\n",
    "                      b12.reshape(b12.shape[0], -1)]\n",
    "\n",
    "        return outputs, motionvecs, bodyvecs\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = self.mot_encoder(x)\n",
    "        b = self.static_encoder(x)\n",
    "        b = b.repeat(1, 1, m.shape[-1])\n",
    "        d = torch.cat([m, b], dim=1)\n",
    "        d = self.decoder(d)\n",
    "        return d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ntu/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to delete 28814\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "to_del = []\n",
    "for file in X:\n",
    "    if type(X[file]) == list:\n",
    "        to_del.append(file)\n",
    "print('to delete', len(to_del))\n",
    "for file in to_del:\n",
    "    del X[file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad or trim data to 75 frames. when padding, repeat the last frame\n",
    "# input is of shape (frames, 75)\n",
    "T = 75\n",
    "for file in X:\n",
    "    if X[file].shape[0] < T:\n",
    "        X[file] = np.pad(X[file], ((0, T - X[file].shape[0]), (0, 0)), mode='edge')\n",
    "    elif X[file].shape[0] > T:\n",
    "        X[file] = X[file][:T, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in X:\n",
    "    X[file] = torch.tensor(X[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "p = {}\n",
    "for file in X:\n",
    "    if file[16:20] not in a:\n",
    "        a[file[16:20]] = {}\n",
    "    if file[8:12] not in a[file[16:20]]:\n",
    "        a[file[16:20]][file[8:12]] = []\n",
    "    a[file[16:20]][file[8:12]].append(file)\n",
    "    \n",
    "    if file[8:12] not in p:\n",
    "        p[file[8:12]] = set()\n",
    "    p[file[8:12]].add(file[16:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 500\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for _ in range(samples):\n",
    "    # sample two random p\n",
    "    p1, p2 = random.sample(list(p.keys()), 2)\n",
    "    # find overlapping a\n",
    "    a1 = p[p1]\n",
    "    a2 = p[p2]\n",
    "    a12 = a1.intersection(a2)\n",
    "    if len(a12) == 0:\n",
    "        continue\n",
    "    # sample two random a\n",
    "    a1, a2 = random.sample(list(a12), 2)\n",
    "    # sample x and y\n",
    "    x1 = random.sample(a[a1][p1], 1)[0]\n",
    "    x2 = random.sample(a[a2][p2], 1)[0]\n",
    "    y1 = random.sample(a[a1][p2], 1)[0]\n",
    "    y2 = random.sample(a[a2][p1], 1)[0]\n",
    "    train_x.append([x1, x2])\n",
    "    train_y.append([y1, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return X[self.X[index][0]], X[self.X[index][1]], X[self.y[index][0]],  X[self.y[index][1]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(train_x, train_y)\n",
    "dl = DataLoader(data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_en_channels = [75, 64, 96, 128]\n",
    "body_en_channels = [75, 32, 48, 64]\n",
    "de_channels = [mot_en_channels[-1] + body_en_channels[-1], 128, 64, 75]\n",
    "model = AutoEncoder2x(mot_en_channels, body_en_channels, de_channels, global_pool=F.max_pool1d, convpool=nn.MaxPool1d, compress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([16, 75, 72])\n",
      "torch.Size([7, 75, 72])\n"
     ]
    }
   ],
   "source": [
    "for (x1, x2, y1, y2) in dl:\n",
    "    print(model(x1.float()).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
