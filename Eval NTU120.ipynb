{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import plotly\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SGN.model import SGN\n",
    "from SGN.data import NTUDataLoaders, AverageMeter\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import mlflow\n",
    "import time\n",
    "_=mlflow.set_experiment(\"Privacy Retargeting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "only_use_pos = True # True uses SGN preprocessing, False uses my preprocessing\n",
    "remove_two_actor_actions = True\n",
    "one_dimension_conv = False\n",
    "ntu_120 = False\n",
    "only_ntu_120 = False\n",
    "seperate_train_test = True\n",
    "sgn_eval_after_each_stage = False\n",
    "binary_data = False\n",
    "train_cameras = [2, 3]\n",
    "test_cameras = [1]\n",
    "train_actors = [1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38, 45, 46, 47, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 70, 74, 78, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 94, 95, 97, 98, 100, 103] #https://ar5iv.labs.arxiv.org/html/1905.04757\n",
    "T = 75\n",
    "k = 5\n",
    "setting = 'cv'\n",
    "dataset = 'NTU'\n",
    "metric = 'val_utility_acc_coop'\n",
    "matric_minimize = False\n",
    "device = torch.device('cuda:0')\n",
    "seg = 20\n",
    "lr = 1e-5\n",
    "adv_lr = 1e-5\n",
    "util_classifier_alpha = 10\n",
    "priv_classifier_alpha = .1\n",
    "if ntu_120:\n",
    "    utility_classes = 120\n",
    "    privacy_classes = 106\n",
    "else:\n",
    "    utility_classes = 60\n",
    "    privacy_classes = 40\n",
    "validation_acc_freq = -1 #-1 to disable\n",
    "emb_clf_update_per_epoch_paired = 1\n",
    "emb_clf_update_per_epoch_unpaired = 3\n",
    "encoded_channels = (128, 16) # default\n",
    "dmr_encoded_channels = (256, 32) # dmr\n",
    "batch_size = 32\n",
    "workers=0\n",
    "cross_samples_train = 50000\n",
    "cross_samples_test = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate parameters\n",
    "assert len(train_cameras) > 0 and len(test_cameras) > 0\n",
    "assert emb_clf_update_per_epoch_paired > 0 and emb_clf_update_per_epoch_unpaired > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Organize\n",
    "\n",
    "X = (frames, joints, pos + orientation)\n",
    "    \n",
    "    (frames, 25, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "if only_use_pos:\n",
    "    with open('ntu/SGN/X_full.pkl', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "else:\n",
    "    with open('ntu/X.pkl', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "\n",
    "# pad/trim data to T frames and convert to tensor\n",
    "for file, value in X.items():\n",
    "    # If SGN preprocessing, remove zero padding\n",
    "    if only_use_pos:\n",
    "        first_zero_index = value.shape[0]\n",
    "        for i in range(value.shape[0]):\n",
    "            if np.all(value[i] == 0):\n",
    "                first_zero_index = i\n",
    "                break\n",
    "        value = value[:first_zero_index]\n",
    "\n",
    "    num_frames = value.shape[0]\n",
    "\n",
    "    # Pad or trim\n",
    "    if num_frames < T:\n",
    "        if only_use_pos: padding = np.repeat(value[-1][np.newaxis, :], T - num_frames, axis=0)\n",
    "        else: padding = np.repeat(value[-1][np.newaxis, :, :], T - num_frames, axis=0)\n",
    "        value = np.concatenate((value, padding), axis=0)\n",
    "    elif num_frames > T:\n",
    "        # Randomly sample T frames\n",
    "        start = random.randint(0, num_frames - T)\n",
    "        value = value[start:start+T]\n",
    "    \n",
    "    # Convert to tensor and store back\n",
    "    X[file] = torch.from_numpy(value).float()\n",
    "\n",
    "if not ntu_120:\n",
    "    to_rem = []\n",
    "    for file in X.keys():\n",
    "        if int(str(file).split('A')[1][:3]) > 60:\n",
    "            to_rem.append(file)\n",
    "    for file in to_rem:\n",
    "        del X[file]\n",
    "\n",
    "if only_ntu_120:\n",
    "    to_rem = []\n",
    "    for file in X.keys():\n",
    "        if int(str(file).split('A')[1][:3]) <= 60:\n",
    "            to_rem.append(file)\n",
    "    for file in to_rem:\n",
    "        del X[file]\n",
    "\n",
    "# chop off second actor and convert to 3d\n",
    "if only_use_pos:\n",
    "    for file, value in X.items():\n",
    "        value = value[:, :75]\n",
    "        value = value.view(-1, 25, 3)\n",
    "        X[file] = value\n",
    "\n",
    "# remove two actor actions\n",
    "two_action_files = set([50,51,52,53,54,55,56,57,58,59,60,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120])\n",
    "if remove_two_actor_actions:\n",
    "    to_rem = []\n",
    "    for file in X.keys():\n",
    "        if int(str(file).split('A')[1][:3]) in two_action_files:\n",
    "            to_rem.append(file)\n",
    "    for file in to_rem:\n",
    "        del X[file]\n",
    "\n",
    "\n",
    "def parse_file_name(file_name):\n",
    "    \"\"\"Parses the filename into a dictionary of parts.\"\"\"\n",
    "    file_name = str(file_name)\n",
    "    if file_name[0] == 'b': # SGN preprocessing\n",
    "        S = int(file_name[3:6])\n",
    "        C = int(file_name[7:10])\n",
    "        P = int(file_name[11:14])\n",
    "        R = int(file_name[15:18])\n",
    "        A = int(file_name[19:22])\n",
    "    else:\n",
    "        S = int(file_name[1:4])\n",
    "        C = int(file_name[5:8])\n",
    "        P = int(file_name[9:12])\n",
    "        R = int(file_name[13:16])\n",
    "        A = int(file_name[17:20])\n",
    "    return {'S': S, 'C': C, 'P': P, 'R': R, 'A': A}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is size of latent space\n",
    "class Adversary_Emb(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Adversary_Emb, self).__init__()\n",
    "        self.channels = [encoded_channels[0], 128, 256, 512]\n",
    "        self.conv1 = nn.ConvTranspose1d(self.channels[0], self.channels[1], 3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(self.channels[1], self.channels[2], 3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv3 = nn.ConvTranspose1d(self.channels[2], self.channels[3], 3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.channels[1])\n",
    "        self.bn2 = nn.BatchNorm1d(self.channels[2])\n",
    "        self.bn3 = nn.BatchNorm1d(self.channels[3])\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(self.channels[3], 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=0.5, training=self.training)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), p=0.5, training=self.training)\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        # x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module): # 1 = real, 0 = fake\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv1d(in_channels=T, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    "        self.fc1 = nn.Linear(80, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        #flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Retargeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv1d(in_channels=T, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv1d(in_channels=512, out_channels=encoded_channels[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(encoded_channels[0], encoded_channels[0] * encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1) \n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder1D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose1d(in_channels=encoded_channels[0]*2, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(in_channels=96, out_channels=T, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    " \n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder2D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv2d(in_channels=T, out_channels=12, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv2d(in_channels=32, out_channels=encoded_channels[0], kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(encoded_channels[0], encoded_channels[0] * encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder2D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose2d(in_channels=encoded_channels[0]*2, out_channels=256, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(in_channels=128, out_channels=96, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose2d(in_channels=96, out_channels=75, kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(3)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMR_Encoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Encoder1D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv1d(in_channels=T, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv1d(in_channels=512, out_channels=dmr_encoded_channels[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(dmr_encoded_channels[0], dmr_encoded_channels[0] * dmr_encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1) \n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *dmr_encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DMR_Decoder1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Decoder1D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose1d(in_channels=dmr_encoded_channels[0]*2, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(in_channels=96, out_channels=T, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    " \n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        return x\n",
    "    \n",
    "class DMR_Encoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Encoder2D, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv2d(in_channels=T, out_channels=12, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv2d(in_channels=32, out_channels=dmr_encoded_channels[0], kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(dmr_encoded_channels[0], dmr_encoded_channels[0] * dmr_encoded_channels[1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, *dmr_encoded_channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DMR_Decoder2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DMR_Decoder2D, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose2d(in_channels=dmr_encoded_channels[0]*2, out_channels=256, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(in_channels=128, out_channels=96, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose2d(in_channels=96, out_channels=75, kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "        self.ref = nn.ReflectionPad2d(3)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DMR(nn.Module):\n",
    "    def __init__(self, adv_lr=1e-4, use_adv=False):\n",
    "        super(DMR, self).__init__()\n",
    "\n",
    "        # AutoEncoder Models\n",
    "        if one_dimension_conv:\n",
    "            self.static_encoder = DMR_Encoder1D()\n",
    "            self.advamic_encoder = DMR_Encoder1D()\n",
    "            self.decoder = DMR_Decoder1D()\n",
    "        else:\n",
    "            self.static_encoder = DMR_Encoder2D()\n",
    "            self.dynamic_encoder = DMR_Encoder2D()\n",
    "            self.decoder = DMR_Decoder1D()\n",
    "\n",
    "        # Adversarial Models\n",
    "        self.use_adv = use_adv\n",
    "        if use_adv:\n",
    "            self.priv_adv = Adversary_Emb(privacy_classes).to(device) # input = dynamic embedding, output = privacy class\n",
    "            self.priv_coop = Adversary_Emb(privacy_classes).to(device) # input = static embedding, output = privacy class\n",
    "            self.util_adv = Adversary_Emb(utility_classes).to(device) # input = static embedding, output = utility class\n",
    "            self.util_coop = Adversary_Emb(utility_classes).to(device) # input = dynamic embedding, output = utility class\n",
    "            self.discriminator = Discriminator().to(device)\n",
    "\n",
    "            self.priv_optim = torch.optim.AdamW(self.priv_adv.parameters(), lr=adv_lr)\n",
    "            self.priv_coop_optim = torch.optim.AdamW(self.priv_coop.parameters(), lr=adv_lr)\n",
    "            self.util_optim = torch.optim.AdamW(self.util_adv.parameters(), lr=adv_lr)\n",
    "            self.util_coop_optim = torch.optim.AdamW(self.util_coop.parameters(), lr=adv_lr)\n",
    "            self.discriminator_optim = torch.optim.AdamW(self.discriminator.parameters(), lr=adv_lr)\n",
    "\n",
    "            # Freeze Adversarial Models\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "\n",
    "        # Loss Functions\n",
    "        self.triplet_loss = nn.TripletMarginLoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Info for loss functions\n",
    "        self.end_effectors = torch.tensor([19, 15, 23, 24, 21, 22, 3]).to(device) * 3\n",
    "        self.chain_lengths = torch.tensor([5, 5, 8, 8, 8, 8, 5]).to(device)\n",
    "\n",
    "        # Lambdas for discounted loss\n",
    "        self.lambda_rec = 2\n",
    "        self.lambda_cross = 0.1\n",
    "        self.lambda_ee = 1\n",
    "        self.lambda_smoothing = 3\n",
    "        self.lambda_trip = 1\n",
    "        self.lambda_latent = 10\n",
    "        self.lambda_adv_util_coop = util_classifier_alpha\n",
    "        self.lambda_adv_priv_coop = priv_classifier_alpha\n",
    "        self.lambda_adv_util_adv = util_classifier_alpha\n",
    "        self.lambda_adv_priv_adv = priv_classifier_alpha\n",
    "        self.lambda_adv_disc = 1\n",
    "\n",
    "        # Loss Toggles\n",
    "        self.use_rec_loss = True\n",
    "        self.use_cross_loss = True\n",
    "        self.use_ee_loss = True \n",
    "        self.use_trip_loss_paired = True \n",
    "        self.use_trip_loss_unpaired = True\n",
    "        self.use_smoothing_loss = True\n",
    "        self.use_latent_consistency = True\n",
    "\n",
    "    def get_loss_params(self):\n",
    "        return {\n",
    "            'lambda_rec': self.lambda_rec,\n",
    "            'lambda_cross': self.lambda_cross,\n",
    "            'lambda_ee': self.lambda_ee,\n",
    "            'lambda_trip': self.lambda_trip,\n",
    "            'lambda_latent': self.lambda_latent,\n",
    "            'lambda_adv_util_coop': self.lambda_adv_util_coop,\n",
    "            'lambda_adv_priv_coop': self.lambda_adv_priv_coop,\n",
    "            'lambda_adv_util_adv': self.lambda_adv_util_adv,\n",
    "            'lambda_adv_priv_adv': self.lambda_adv_priv_adv,\n",
    "            'lambda_adv_disc': self.lambda_adv_disc,\n",
    "            'use_rec_loss': self.use_rec_loss,\n",
    "            'use_cross_loss': self.use_cross_loss,\n",
    "            'use_ee_loss': self.use_ee_loss,\n",
    "            'use_trip_loss_paired': self.use_trip_loss_paired,\n",
    "            'use_trip_loss_unpaired': self.use_trip_loss_unpaired,\n",
    "            'use_smoothing_loss': self.use_smoothing_loss,\n",
    "            'use_latent_consistency': self.use_latent_consistency\n",
    "        }\n",
    "\n",
    "    def cross(self, x1, x1_rot, x2, x2_rot):\n",
    "        d1 = self.dynamic_encoder(x1_rot)\n",
    "        d2 = self.dynamic_encoder(x2_rot)\n",
    "        s1 = self.static_encoder(x1)\n",
    "        s2 = self.static_encoder(x2)\n",
    "        \n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1))\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1))\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1))\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1))\n",
    "\n",
    "        return x1_hat, x2_hat, y1_hat, y2_hat\n",
    "    \n",
    "    def eval(self, x1_rot, x2):\n",
    "        dynamic = self.dynamic_encoder(x1_rot)\n",
    "        static = self.static_encoder(x2)\n",
    "        return self.decoder(torch.cat((dynamic, static), dim=1))\n",
    "\n",
    "    def rec_loss(self, x, x_rot):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "        if not one_dimension_conv:\n",
    "            x_ = x.reshape(x.size(0), T, -1)\n",
    "        return self.reconstruction_loss(x_, x_hat)\n",
    "    \n",
    "    def loss_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, cross = True, reconstruction = True, emb_adv = True, discrim_adv = True, verbose = False):\n",
    "        d1 = self.dynamic_encoder(x1_rot) # A1\n",
    "        d2 = self.dynamic_encoder(x2_rot) # A2\n",
    "        s1 = self.static_encoder(x1) # P1\n",
    "        s2 = self.static_encoder(x2) # P2\n",
    "\n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1)) # P1, A1\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1)) # P2, A2\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1)) # P2, A1\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1)) # P1, A2\n",
    "\n",
    "        d12 = self.dynamic_encoder(y1_rot) # A1\n",
    "        d21 = self.dynamic_encoder(y2_rot) # A2\n",
    "        s12 = self.static_encoder(y1) # P2\n",
    "        s21 = self.static_encoder(y2) # P1\n",
    "\n",
    "        x1_hat_ = self.decoder(torch.cat((d12, s21), dim=1)) # P1, A1\n",
    "        x2_hat_ = self.decoder(torch.cat((d21, s12), dim=1)) # P2, A2\n",
    "        y1_hat_ = self.decoder(torch.cat((d12, s12), dim=1)) # P2, A1\n",
    "        y2_hat_ = self.decoder(torch.cat((d21, s21), dim=1)) # P1, A2\n",
    "\n",
    "        # x1_hat is reconstruction of x1\n",
    "        # x2_hat is reconstruction of x2\n",
    "        # y1_hat is cross reconstruction from x1 and x2\n",
    "        # y2_hat is cross reconstruction from x2 and x1\n",
    "        # x1_hat_ is cross reconstruction from y1 and y2\n",
    "        # x2_hat_ is cross reconstruction from y2 and y1\n",
    "        # y1_hat_ is reconstruction of y1\n",
    "        # y2_hat_ is reconstruction of y2\n",
    "        # d1 = A1\n",
    "        # d2 = A2\n",
    "        # d12 = A1\n",
    "        # d21 = A2\n",
    "        # s1 = P1\n",
    "        # s2 = P2\n",
    "        # s12 = P2\n",
    "        # s21 = P1\n",
    "\n",
    "        # flatten data if 2D\n",
    "        if not one_dimension_conv:\n",
    "            x1 = x1.view(x1.size(0), T, -1)\n",
    "            x2 = x2.view(x2.size(0), T, -1)\n",
    "            y1 = y1.view(y1.size(0), T, -1)\n",
    "            y2 = y2.view(y2.size(0), T, -1)\n",
    "        \n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        cross_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        latent_consistency_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "                        \n",
    "        # reconstruction loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = (self.reconstruction_loss(x1, x1_hat) + self.reconstruction_loss(x2, x2_hat) + self.reconstruction_loss(y1, y1_hat_) + self.reconstruction_loss(y2, y2_hat_)) / 4\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "        \n",
    "        # cross reconstruction loss\n",
    "        if self.use_cross_loss and cross:\n",
    "            # could move this to its own function, but since cross is basically reconstruction, its fine like this\n",
    "            cross_loss = (self.reconstruction_loss(y1, y1_hat) + self.reconstruction_loss(y2, y2_hat) + self.reconstruction_loss(x1, x1_hat_) + self.reconstruction_loss(x2, x2_hat_)) / 4\n",
    "            if verbose: print('Cross Reconstruction Loss: ', cross_loss.item())\n",
    "        \n",
    "        # end effector loss\n",
    "        if self.use_ee_loss:\n",
    "            if reconstruction:\n",
    "                end_effector_loss += (self.end_effector_loss(x1_hat, x1) + self.end_effector_loss(x2_hat, x2)) / 2\n",
    "            if cross:\n",
    "                end_effector_loss += (self.end_effector_loss(y1_hat, y1) + self.end_effector_loss(y2_hat, y2)) / 2\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # triplet loss\n",
    "        if self.use_trip_loss_paired: # anchor, positive, negative\n",
    "            # d1 = A1, d2 = A2, d12 = A1, d21 = A2\n",
    "            # s1 = P1, s2 = P2, s12 = P2, s21 = P1\n",
    "            # d12,s12 = y1, d21,s21 = y2\n",
    "            # y1 = jk, y2 = il\n",
    "            triplet_loss = self.triplet_loss(d12, d1, d2) \\\n",
    "                            + self.triplet_loss(d21, d2, d1) \\\n",
    "                            + self.triplet_loss(s12, s2, s1) \\\n",
    "                            + self.triplet_loss(s21, s1, s2) \n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = (self.smoothing_loss(x1, x1_hat) + self.smoothing_loss(x2, x2_hat) + self.smoothing_loss(y1, y1_hat_) + self.smoothing_loss(y2, y2_hat_) + \\\n",
    "                                self.smoothing_loss(x1, x1_hat_) + self.smoothing_loss(x2, x2_hat_) + self.smoothing_loss(y1, y1_hat) + self.smoothing_loss(y2, y2_hat)) / 8\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # latent consistency loss\n",
    "        if self.use_latent_consistency:\n",
    "            latent_consistency_loss = (self.latent_consistency_loss(d1, d12) + self.latent_consistency_loss(d2, d21) + self.latent_consistency_loss(s1, s21) + self.latent_consistency_loss(s2, s12)) / 4\n",
    "            if verbose: print('Latent Consistency Loss: ', latent_consistency_loss.item())\n",
    "\n",
    "        # adversarial loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y1, actor_y2 = actors[0] - 1, actors[1] - 1\n",
    "            actor_y1, actor_y2 = torch.eye(privacy_classes)[actor_y1.long()].to(device), torch.eye(privacy_classes)[actor_y2.long()].to(device)\n",
    "            action_y1, action_y2 = actions[0] - 1, actions[1] - 1\n",
    "            action_y1, action_y2 = torch.eye(utility_classes)[action_y1.long()].to(device), torch.eye(utility_classes)[action_y2.long()].to(device)\n",
    "\n",
    "            # x1 => d1 s1\n",
    "            # x2 => d2 s2\n",
    "\n",
    "            # d1 => p1\n",
    "            # d2 => p2\n",
    "            # s1 => a1\n",
    "            # s2 => a2\n",
    "            \n",
    "            # actor_y1 = p1\n",
    "            # actor_y2 = p2\n",
    "\n",
    "            # action_y1 = a1\n",
    "            # action_y2 = a2\n",
    "\n",
    "\n",
    "            # privacy loss (adversarial)\n",
    "            privacy_loss_adv = (-self.adv_loss(self.priv_adv, d1, actor_y1) -self.adv_loss(self.priv_adv, d2, actor_y2))/2\n",
    "            privacy_acc_adv = (self.adv_accuracy(self.priv_adv, d1, actor_y1) + self.adv_accuracy(self.priv_adv, d2, actor_y2))/2\n",
    "\n",
    "            # privacy loss (coop)\n",
    "            privacy_loss_coop = (self.adv_loss(self.priv_coop, s1, actor_y1) + self.adv_loss(self.priv_coop, s2, actor_y2))/2\n",
    "            privacy_acc_coop = (self.adv_accuracy(self.priv_coop, s1, actor_y1) + self.adv_accuracy(self.priv_coop, s2, actor_y2))/2\n",
    "\n",
    "            # utility loss (adversarial)\n",
    "            utility_loss_adv = (-self.adv_loss(self.util_adv, s1, action_y1) -self.adv_loss(self.util_adv, s2, action_y2))/2\n",
    "            utility_acc_adv = (self.adv_accuracy(self.util_adv, s1, action_y1) + self.adv_accuracy(self.util_adv, s2, action_y2))/2\n",
    "\n",
    "            # utility loss (coop)\n",
    "            utility_loss_coop = (self.adv_loss(self.util_coop, d1, action_y1) + self.adv_loss(self.util_coop, d2, action_y2))/2\n",
    "            utility_acc_coop = (self.adv_accuracy(self.util_coop, d1, action_y1) + self.adv_accuracy(self.util_coop, d2, action_y2))/2\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss (Adversarial): ', privacy_loss_adv.item(), '\\tPrivacy Loss (Coop): ', privacy_loss_coop.item())\n",
    "                print('Utility Loss (Adversarial): ', utility_loss_adv.item(), '\\tUtility Loss (Coop): ', utility_loss_coop.item())\n",
    "                print('Privacy Accuracy (Adversarial): ', privacy_acc_adv.item(), '\\tPrivacy Accuracy (Coop): ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy (Adversarial): ', utility_acc_adv.item(), '\\tUtility Accuracy (Coop): ', utility_acc_coop.item())\n",
    "            \n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat, x1_hat_, x2_hat_, y1_hat_, y2_hat_)))\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (8 * batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'cross_loss': cross_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'latent_consistency_loss': latent_consistency_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + cross_loss * self.lambda_cross \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + latent_consistency_loss * self.lambda_latent \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x1_hat, x2_hat, y1_hat, y2_hat, losses\n",
    "\n",
    "    def loss_unpaired(self, x_pos, x_rot, actors, actions, reconstruction = True, emb_adv = False, discrim_adv = False, ee = False, triplet = False, verbose = False):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x_pos)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "        if not one_dimension_conv:\n",
    "            x = x_pos.reshape(x_pos.size(0), T, -1)\n",
    "\n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = self.reconstruction_loss(x, x_hat)\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "\n",
    "        # End Effector Loss\n",
    "        if self.use_ee_loss and ee:\n",
    "            end_effector_loss = self.end_effector_loss(x_hat, x)\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # Triplet Loss\n",
    "        if self.use_trip_loss_unpaired and triplet: # anchor, positive, negative\n",
    "            triplet_loss = (self.triplet_loss(d, d, s) + self.triplet_loss(s, s, d)) / 2\n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        # Smoothing Loss\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = self.smoothing_loss(x, x_hat)\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # Adversarial Loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y = actors - 1\n",
    "            actor_y = torch.eye(privacy_classes)[actor_y.long()].to(device)\n",
    "            action_y = actions - 1\n",
    "            action_y = torch.eye(utility_classes)[action_y.long()].to(device)\n",
    "\n",
    "            # latent privacy loss (adv)\n",
    "            privacy_loss_adv = -self.adv_loss(self.priv_adv, d, actor_y)\n",
    "            privacy_acc_adv = self.adv_accuracy(self.priv_adv, d, actor_y)\n",
    "\n",
    "            # latent privacy loss (coop)\n",
    "            privacy_loss_coop = self.adv_loss(self.priv_coop, s, actor_y)\n",
    "            privacy_acc_coop = self.adv_accuracy(self.priv_coop, s, actor_y)\n",
    "\n",
    "            # latent utility loss (adv)\n",
    "            utility_loss_adv = -self.adv_loss(self.util_adv, s, action_y)\n",
    "            utility_acc_adv = self.adv_accuracy(self.util_adv, s, action_y)\n",
    "\n",
    "            # latent utility loss (coop)\n",
    "            utility_loss_coop = self.adv_loss(self.util_coop, d, action_y)\n",
    "            utility_acc_coop = self.adv_accuracy(self.util_coop, d, action_y)\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss Adv: ', privacy_loss_adv.item(), '\\tPrivacy Loss Coop: ', privacy_loss_coop.item(), '\\tPrivacy Loss: ', privacy_loss.item())\n",
    "                print('Utility Loss Adv: ', utility_loss_adv.item(), '\\tUtility Loss Coop: ', utility_loss_coop.item(), '\\tUtility Loss: ', utility_loss.item())\n",
    "                print('Privacy Accuracy Adv: ', privacy_acc_adv.item(), '\\tPrivacy Accuracy Coop: ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy Adv: ', utility_acc_adv.item(), '\\tUtility Accuracy Coop: ', utility_acc_coop.item())\n",
    "\n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x_hat, losses\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        # return F.mse_loss(x, y)\n",
    "        return torch.square(torch.norm(x - y, dim=1)).mean()\n",
    "    \n",
    "    def latent_consistency_loss(self, x, y):\n",
    "        return F.mse_loss(x, y)\n",
    "    \n",
    "    def end_effector_loss(self, x, y):\n",
    "        # slice to get the end effector joints\n",
    "        x_ee = x[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)] \n",
    "        y_ee = y[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)]\n",
    "\n",
    "        # calculate velocities\n",
    "        x_vel = torch.norm(x_ee[:, 1:] - x_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        y_vel = torch.norm(y_ee[:, 1:] - y_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        \n",
    "        # compute mse loss for each joint\n",
    "        losses = F.mse_loss(x_vel, y_vel, reduction='none')\n",
    "\n",
    "        # take sum over end effectors\n",
    "        loss = losses.sum(dim=1)\n",
    "\n",
    "        # take mean over batch\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def smoothing_loss(self, y, y_pred):\n",
    "        # (batch, T, 75)\n",
    "        # Calculate the squared sum of differences for y and y_pred\n",
    "        diff_y = torch.sum(y[:, :-1] - y[:, 1:], dim=2) ** 2\n",
    "        diff_y_pred = torch.sum(y_pred[:, :-1] - y_pred[:, 1:], dim=2) ** 2\n",
    "\n",
    "        # Calculate the absolute difference\n",
    "        abs_diff = torch.abs(diff_y - diff_y_pred)\n",
    "\n",
    "        # Sum over all batches and sequence elements\n",
    "        loss = torch.sum(abs_diff)\n",
    "\n",
    "        # Normalize by the total number of elements (batch_size * sequence_length)\n",
    "        total_loss = torch.sqrt(loss) / (y.size(0) * y.size(1))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def adv_loss(self, model, x, y):\n",
    "        return self.cross_entropy(model(x), y)#.long().to(device))\n",
    "    \n",
    "    def adv_accuracy(self, model, x, y):\n",
    "        return (model(x).argmax(dim=1) == y.argmax(dim=1).to(device)).float().mean()\n",
    "\n",
    "    def forward(self, x, x_rot):\n",
    "        dyn = self.dynamic_encoder(x_rot)\n",
    "        sta = self.static_encoder(x)\n",
    "        x = self.decoder(torch.cat((dyn, sta), dim=1))\n",
    "        return x\n",
    "    \n",
    "    def set_eval(self, eval=True):\n",
    "        if eval:\n",
    "            self.static_encoder.eval()\n",
    "            self.dynamic_encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "        else:\n",
    "            self.static_encoder.train()\n",
    "            self.dynamic_encoder.train()\n",
    "            self.decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, adv_lr=1e-4, use_adv=True):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # AutoEncoder Models\n",
    "        if one_dimension_conv:\n",
    "            self.static_encoder = Encoder1D()\n",
    "            self.advamic_encoder = Encoder1D()\n",
    "            self.decoder = Decoder1D()\n",
    "        else:\n",
    "            self.static_encoder = Encoder2D()\n",
    "            self.dynamic_encoder = Encoder2D()\n",
    "            self.decoder = Decoder1D()\n",
    "\n",
    "        # Adversarial Models\n",
    "        self.use_adv = use_adv\n",
    "        if use_adv:\n",
    "            self.priv_adv = Adversary_Emb(privacy_classes).to(device) # input = dynamic embedding, output = privacy class\n",
    "            self.priv_coop = Adversary_Emb(privacy_classes).to(device) # input = static embedding, output = privacy class\n",
    "            self.util_adv = Adversary_Emb(utility_classes).to(device) # input = static embedding, output = utility class\n",
    "            self.util_coop = Adversary_Emb(utility_classes).to(device) # input = dynamic embedding, output = utility class\n",
    "            self.discriminator = Discriminator().to(device)\n",
    "\n",
    "            self.priv_optim = torch.optim.AdamW(self.priv_adv.parameters(), lr=adv_lr)\n",
    "            self.priv_coop_optim = torch.optim.AdamW(self.priv_coop.parameters(), lr=adv_lr)\n",
    "            self.util_optim = torch.optim.AdamW(self.util_adv.parameters(), lr=adv_lr)\n",
    "            self.util_coop_optim = torch.optim.AdamW(self.util_coop.parameters(), lr=adv_lr)\n",
    "            self.discriminator_optim = torch.optim.AdamW(self.discriminator.parameters(), lr=adv_lr)\n",
    "\n",
    "            # Freeze Adversarial Models\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "\n",
    "        # Loss Functions\n",
    "        self.triplet_loss = nn.TripletMarginLoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Info for loss functions\n",
    "        self.end_effectors = torch.tensor([19, 15, 23, 24, 21, 22, 3]).to(device) * 3\n",
    "        self.chain_lengths = torch.tensor([5, 5, 8, 8, 8, 8, 5]).to(device)\n",
    "\n",
    "        # Lambdas for discounted loss\n",
    "        self.lambda_rec = 2\n",
    "        self.lambda_cross = 0.1\n",
    "        self.lambda_ee = 1\n",
    "        self.lambda_smoothing = 3\n",
    "        self.lambda_trip = 1\n",
    "        self.lambda_latent = 10\n",
    "        self.lambda_adv_util_coop = util_classifier_alpha\n",
    "        self.lambda_adv_priv_coop = priv_classifier_alpha\n",
    "        self.lambda_adv_util_adv = util_classifier_alpha\n",
    "        self.lambda_adv_priv_adv = priv_classifier_alpha\n",
    "        self.lambda_adv_disc = 1\n",
    "\n",
    "        # Loss Toggles\n",
    "        self.use_rec_loss = True\n",
    "        self.use_cross_loss = True\n",
    "        self.use_ee_loss = True \n",
    "        self.use_trip_loss_paired = True \n",
    "        self.use_trip_loss_unpaired = True\n",
    "        self.use_smoothing_loss = True\n",
    "        self.use_latent_consistency = True\n",
    "\n",
    "    def get_loss_params(self):\n",
    "        return {\n",
    "            'lambda_rec': self.lambda_rec,\n",
    "            'lambda_cross': self.lambda_cross,\n",
    "            'lambda_ee': self.lambda_ee,\n",
    "            'lambda_trip': self.lambda_trip,\n",
    "            'lambda_latent': self.lambda_latent,\n",
    "            'lambda_adv_util_coop': self.lambda_adv_util_coop,\n",
    "            'lambda_adv_priv_coop': self.lambda_adv_priv_coop,\n",
    "            'lambda_adv_util_adv': self.lambda_adv_util_adv,\n",
    "            'lambda_adv_priv_adv': self.lambda_adv_priv_adv,\n",
    "            'lambda_adv_disc': self.lambda_adv_disc,\n",
    "            'use_rec_loss': self.use_rec_loss,\n",
    "            'use_cross_loss': self.use_cross_loss,\n",
    "            'use_ee_loss': self.use_ee_loss,\n",
    "            'use_trip_loss_paired': self.use_trip_loss_paired,\n",
    "            'use_trip_loss_unpaired': self.use_trip_loss_unpaired,\n",
    "            'use_smoothing_loss': self.use_smoothing_loss,\n",
    "            'use_latent_consistency': self.use_latent_consistency\n",
    "        }\n",
    "\n",
    "    def cross(self, x1, x1_rot, x2, x2_rot):\n",
    "        d1 = self.dynamic_encoder(x1_rot)\n",
    "        d2 = self.dynamic_encoder(x2_rot)\n",
    "        s1 = self.static_encoder(x1)\n",
    "        s2 = self.static_encoder(x2)\n",
    "        \n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1))\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1))\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1))\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1))\n",
    "\n",
    "        return x1_hat, x2_hat, y1_hat, y2_hat\n",
    "    \n",
    "    def eval(self, x1_rot, x2):\n",
    "        dynamic = self.dynamic_encoder(x1_rot)\n",
    "        static = self.static_encoder(x2)\n",
    "        return self.decoder(torch.cat((dynamic, static), dim=1))\n",
    "\n",
    "    def rec_loss(self, x, x_rot):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "        if not one_dimension_conv:\n",
    "            x_ = x.reshape(x.size(0), T, -1)\n",
    "        return self.reconstruction_loss(x_, x_hat)\n",
    "    \n",
    "    def loss_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, cross = True, reconstruction = True, emb_adv = True, discrim_adv = True, verbose = False):\n",
    "        d1 = self.dynamic_encoder(x1_rot) # A1\n",
    "        d2 = self.dynamic_encoder(x2_rot) # A2\n",
    "        s1 = self.static_encoder(x1) # P1\n",
    "        s2 = self.static_encoder(x2) # P2\n",
    "\n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1)) # P1, A1\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1)) # P2, A2\n",
    "        y1_hat = self.decoder(torch.cat((d1, s2), dim=1)) # P2, A1\n",
    "        y2_hat = self.decoder(torch.cat((d2, s1), dim=1)) # P1, A2\n",
    "\n",
    "        d12 = self.dynamic_encoder(y1_rot) # A1\n",
    "        d21 = self.dynamic_encoder(y2_rot) # A2\n",
    "        s12 = self.static_encoder(y1) # P2\n",
    "        s21 = self.static_encoder(y2) # P1\n",
    "\n",
    "        x1_hat_ = self.decoder(torch.cat((d12, s21), dim=1)) # P1, A1\n",
    "        x2_hat_ = self.decoder(torch.cat((d21, s12), dim=1)) # P2, A2\n",
    "        y1_hat_ = self.decoder(torch.cat((d12, s12), dim=1)) # P2, A1\n",
    "        y2_hat_ = self.decoder(torch.cat((d21, s21), dim=1)) # P1, A2\n",
    "\n",
    "        # x1_hat is reconstruction of x1\n",
    "        # x2_hat is reconstruction of x2\n",
    "        # y1_hat is cross reconstruction from x1 and x2\n",
    "        # y2_hat is cross reconstruction from x2 and x1\n",
    "        # x1_hat_ is cross reconstruction from y1 and y2\n",
    "        # x2_hat_ is cross reconstruction from y2 and y1\n",
    "        # y1_hat_ is reconstruction of y1\n",
    "        # y2_hat_ is reconstruction of y2\n",
    "        # d1 = A1\n",
    "        # d2 = A2\n",
    "        # d12 = A1\n",
    "        # d21 = A2\n",
    "        # s1 = P1\n",
    "        # s2 = P2\n",
    "        # s12 = P2\n",
    "        # s21 = P1\n",
    "\n",
    "        # flatten data if 2D\n",
    "        if not one_dimension_conv:\n",
    "            x1 = x1.view(x1.size(0), T, -1)\n",
    "            x2 = x2.view(x2.size(0), T, -1)\n",
    "            y1 = y1.view(y1.size(0), T, -1)\n",
    "            y2 = y2.view(y2.size(0), T, -1)\n",
    "        \n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        cross_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        latent_consistency_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "                        \n",
    "        # reconstruction loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = (self.reconstruction_loss(x1, x1_hat) + self.reconstruction_loss(x2, x2_hat) + self.reconstruction_loss(y1, y1_hat_) + self.reconstruction_loss(y2, y2_hat_)) / 4\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "        \n",
    "        # cross reconstruction loss\n",
    "        if self.use_cross_loss and cross:\n",
    "            # could move this to its own function, but since cross is basically reconstruction, its fine like this\n",
    "            cross_loss = (self.reconstruction_loss(y1, y1_hat) + self.reconstruction_loss(y2, y2_hat) + self.reconstruction_loss(x1, x1_hat_) + self.reconstruction_loss(x2, x2_hat_)) / 4\n",
    "            if verbose: print('Cross Reconstruction Loss: ', cross_loss.item())\n",
    "        \n",
    "        # end effector loss\n",
    "        if self.use_ee_loss:\n",
    "            if reconstruction:\n",
    "                end_effector_loss += (self.end_effector_loss(x1_hat, x1) + self.end_effector_loss(x2_hat, x2)) / 2\n",
    "            if cross:\n",
    "                end_effector_loss += (self.end_effector_loss(y1_hat, y1) + self.end_effector_loss(y2_hat, y2)) / 2\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # triplet loss\n",
    "        if self.use_trip_loss_paired: # anchor, positive, negative\n",
    "            # d1 = A1, d2 = A2, d12 = A1, d21 = A2\n",
    "            # s1 = P1, s2 = P2, s12 = P2, s21 = P1\n",
    "            # d12,s12 = y1, d21,s21 = y2\n",
    "            # y1 = jk, y2 = il\n",
    "            triplet_loss = self.triplet_loss(d12, d1, d2) \\\n",
    "                            + self.triplet_loss(d21, d2, d1) \\\n",
    "                            + self.triplet_loss(s12, s2, s1) \\\n",
    "                            + self.triplet_loss(s21, s1, s2) \n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = (self.smoothing_loss(x1, x1_hat) + self.smoothing_loss(x2, x2_hat) + self.smoothing_loss(y1, y1_hat_) + self.smoothing_loss(y2, y2_hat_) + \\\n",
    "                                self.smoothing_loss(x1, x1_hat_) + self.smoothing_loss(x2, x2_hat_) + self.smoothing_loss(y1, y1_hat) + self.smoothing_loss(y2, y2_hat)) / 8\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # latent consistency loss\n",
    "        if self.use_latent_consistency:\n",
    "            latent_consistency_loss = (self.latent_consistency_loss(d1, d12) + self.latent_consistency_loss(d2, d21) + self.latent_consistency_loss(s1, s21) + self.latent_consistency_loss(s2, s12)) / 4\n",
    "            if verbose: print('Latent Consistency Loss: ', latent_consistency_loss.item())\n",
    "\n",
    "        # adversarial loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y1, actor_y2 = actors[0] - 1, actors[1] - 1\n",
    "            actor_y1, actor_y2 = torch.eye(privacy_classes)[actor_y1.long()].to(device), torch.eye(privacy_classes)[actor_y2.long()].to(device)\n",
    "            action_y1, action_y2 = actions[0] - 1, actions[1] - 1\n",
    "            action_y1, action_y2 = torch.eye(utility_classes)[action_y1.long()].to(device), torch.eye(utility_classes)[action_y2.long()].to(device)\n",
    "\n",
    "            # x1 => d1 s1\n",
    "            # x2 => d2 s2\n",
    "\n",
    "            # d1 => p1\n",
    "            # d2 => p2\n",
    "            # s1 => a1\n",
    "            # s2 => a2\n",
    "            \n",
    "            # actor_y1 = p1\n",
    "            # actor_y2 = p2\n",
    "\n",
    "            # action_y1 = a1\n",
    "            # action_y2 = a2\n",
    "\n",
    "\n",
    "            # privacy loss (adversarial)\n",
    "            privacy_loss_adv = (-self.adv_loss(self.priv_adv, d1, actor_y1) -self.adv_loss(self.priv_adv, d2, actor_y2))/2\n",
    "            privacy_acc_adv = (self.adv_accuracy(self.priv_adv, d1, actor_y1) + self.adv_accuracy(self.priv_adv, d2, actor_y2))/2\n",
    "\n",
    "            # privacy loss (coop)\n",
    "            privacy_loss_coop = (self.adv_loss(self.priv_coop, s1, actor_y1) + self.adv_loss(self.priv_coop, s2, actor_y2))/2\n",
    "            privacy_acc_coop = (self.adv_accuracy(self.priv_coop, s1, actor_y1) + self.adv_accuracy(self.priv_coop, s2, actor_y2))/2\n",
    "\n",
    "            # utility loss (adversarial)\n",
    "            utility_loss_adv = (-self.adv_loss(self.util_adv, s1, action_y1) -self.adv_loss(self.util_adv, s2, action_y2))/2\n",
    "            utility_acc_adv = (self.adv_accuracy(self.util_adv, s1, action_y1) + self.adv_accuracy(self.util_adv, s2, action_y2))/2\n",
    "\n",
    "            # utility loss (coop)\n",
    "            utility_loss_coop = (self.adv_loss(self.util_coop, d1, action_y1) + self.adv_loss(self.util_coop, d2, action_y2))/2\n",
    "            utility_acc_coop = (self.adv_accuracy(self.util_coop, d1, action_y1) + self.adv_accuracy(self.util_coop, d2, action_y2))/2\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss (Adversarial): ', privacy_loss_adv.item(), '\\tPrivacy Loss (Coop): ', privacy_loss_coop.item())\n",
    "                print('Utility Loss (Adversarial): ', utility_loss_adv.item(), '\\tUtility Loss (Coop): ', utility_loss_coop.item())\n",
    "                print('Privacy Accuracy (Adversarial): ', privacy_acc_adv.item(), '\\tPrivacy Accuracy (Coop): ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy (Adversarial): ', utility_acc_adv.item(), '\\tUtility Accuracy (Coop): ', utility_acc_coop.item())\n",
    "            \n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat, x1_hat_, x2_hat_, y1_hat_, y2_hat_)))\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (8 * batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'cross_loss': cross_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'latent_consistency_loss': latent_consistency_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + cross_loss * self.lambda_cross \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + latent_consistency_loss * self.lambda_latent \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x1_hat, x2_hat, y1_hat, y2_hat, losses\n",
    "\n",
    "    def loss_unpaired(self, x_pos, x_rot, actors, actions, reconstruction = True, emb_adv = False, discrim_adv = False, ee = False, triplet = False, verbose = False):\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x_pos)\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "        if not one_dimension_conv:\n",
    "            x = x_pos.reshape(x_pos.size(0), T, -1)\n",
    "\n",
    "        # initialize all losses to 0 tensor\n",
    "        rec_loss = torch.zeros(1).to(device)\n",
    "        end_effector_loss = torch.zeros(1).to(device)\n",
    "        triplet_loss = torch.zeros(1).to(device)\n",
    "        smoothing_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss = torch.zeros(1).to(device)\n",
    "        privacy_loss_adv = torch.zeros(1).to(device)\n",
    "        privacy_loss_coop = torch.zeros(1).to(device)\n",
    "        utility_loss = torch.zeros(1).to(device)\n",
    "        utility_loss_adv = torch.zeros(1).to(device)\n",
    "        utility_loss_coop = torch.zeros(1).to(device)\n",
    "        privacy_acc_adv = torch.zeros(1).to(device)\n",
    "        privacy_acc_coop = torch.zeros(1).to(device)\n",
    "        utility_acc_adv = torch.zeros(1).to(device)\n",
    "        utility_acc_coop = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        if self.use_rec_loss and reconstruction:\n",
    "            rec_loss = self.reconstruction_loss(x, x_hat)\n",
    "            if verbose: print('Reconstruction Loss: ', rec_loss.item())\n",
    "\n",
    "        # End Effector Loss\n",
    "        if self.use_ee_loss and ee:\n",
    "            end_effector_loss = self.end_effector_loss(x_hat, x)\n",
    "            if verbose: print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # Triplet Loss\n",
    "        if self.use_trip_loss_unpaired and triplet: # anchor, positive, negative\n",
    "            triplet_loss = (self.triplet_loss(d, d, s) + self.triplet_loss(s, s, d)) / 2\n",
    "            if verbose: print('Triplet Loss: ', triplet_loss.item())\n",
    "\n",
    "        # Smoothing Loss\n",
    "        if self.use_smoothing_loss:\n",
    "            smoothing_loss = self.smoothing_loss(x, x_hat)\n",
    "            if verbose: print('Smoothing Loss: ', smoothing_loss.item())\n",
    "\n",
    "        # Adversarial Loss\n",
    "        if self.use_adv and emb_adv:\n",
    "            actor_y = actors - 1\n",
    "            actor_y = torch.eye(privacy_classes)[actor_y.long()].to(device)\n",
    "            action_y = actions - 1\n",
    "            action_y = torch.eye(utility_classes)[action_y.long()].to(device)\n",
    "\n",
    "            # latent privacy loss (adv)\n",
    "            privacy_loss_adv = -self.adv_loss(self.priv_adv, d, actor_y)\n",
    "            privacy_acc_adv = self.adv_accuracy(self.priv_adv, d, actor_y)\n",
    "\n",
    "            # latent privacy loss (coop)\n",
    "            privacy_loss_coop = self.adv_loss(self.priv_coop, s, actor_y)\n",
    "            privacy_acc_coop = self.adv_accuracy(self.priv_coop, s, actor_y)\n",
    "\n",
    "            # latent utility loss (adv)\n",
    "            utility_loss_adv = -self.adv_loss(self.util_adv, s, action_y)\n",
    "            utility_acc_adv = self.adv_accuracy(self.util_adv, s, action_y)\n",
    "\n",
    "            # latent utility loss (coop)\n",
    "            utility_loss_coop = self.adv_loss(self.util_coop, d, action_y)\n",
    "            utility_acc_coop = self.adv_accuracy(self.util_coop, d, action_y)\n",
    "\n",
    "            privacy_loss = privacy_loss_adv * self.lambda_adv_priv_adv + privacy_loss_coop * self.lambda_adv_priv_coop\n",
    "            utility_loss = utility_loss_adv * self.lambda_adv_util_adv + utility_loss_coop * self.lambda_adv_util_coop\n",
    "\n",
    "            if verbose: \n",
    "                print('Privacy Loss Adv: ', privacy_loss_adv.item(), '\\tPrivacy Loss Coop: ', privacy_loss_coop.item(), '\\tPrivacy Loss: ', privacy_loss.item())\n",
    "                print('Utility Loss Adv: ', utility_loss_adv.item(), '\\tUtility Loss Coop: ', utility_loss_coop.item(), '\\tUtility Loss: ', utility_loss.item())\n",
    "                print('Privacy Accuracy Adv: ', privacy_acc_adv.item(), '\\tPrivacy Accuracy Coop: ', privacy_acc_coop.item())\n",
    "                print('Utility Accuracy Adv: ', utility_acc_adv.item(), '\\tUtility Accuracy Coop: ', utility_acc_coop.item())\n",
    "\n",
    "\n",
    "        if self.use_adv and discrim_adv:\n",
    "            # discrimnator (adversarial)\n",
    "            discrim_out_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(discrim_out_fake, torch.ones_like(discrim_out_fake))\n",
    "            discriminator_acc = torch.sum(torch.round(discrim_out_fake) == 0).float() / (batch_size)\n",
    "            if verbose: print('Discriminator Loss: ', discriminator_loss.item(), '\\tDiscriminator Accuracy: ', discriminator_acc.item())\n",
    "\n",
    "        losses = {\n",
    "            'rec_loss': rec_loss.item(),\n",
    "            'end_effector_loss': end_effector_loss.item(),\n",
    "            'triplet_loss': triplet_loss.item(),\n",
    "            'smoothing_loss': smoothing_loss.item(),\n",
    "            'privacy_loss': privacy_loss.item(),\n",
    "            'privacy_loss_adv': privacy_loss_adv.item(),\n",
    "            'privacy_loss_coop': privacy_loss_coop.item(),\n",
    "            'privacy_acc_adv': privacy_acc_adv.item(),\n",
    "            'privacy_acc_coop': privacy_acc_coop.item(),\n",
    "            'utility_loss': utility_loss.item(),\n",
    "            'utility_loss_adv': utility_loss_adv.item(),\n",
    "            'utility_loss_coop': utility_loss_coop.item(),\n",
    "            'utility_acc_adv': utility_acc_adv.item(),\n",
    "            'utility_acc_coop': utility_acc_coop.item(),\n",
    "            'discriminator_loss': discriminator_loss.item(),\n",
    "            'discriminator_acc': discriminator_acc.item()\n",
    "        }\n",
    "\n",
    "        return rec_loss * self.lambda_rec \\\n",
    "                + end_effector_loss * self.lambda_ee \\\n",
    "                + triplet_loss * self.lambda_trip \\\n",
    "                + privacy_loss \\\n",
    "                + utility_loss \\\n",
    "                + discriminator_loss * self.lambda_adv_disc \\\n",
    "                + smoothing_loss * self.lambda_smoothing, \\\n",
    "                x_hat, losses\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        # return F.mse_loss(x, y)\n",
    "        return torch.square(torch.norm(x - y, dim=1)).mean()\n",
    "    \n",
    "    def latent_consistency_loss(self, x, y):\n",
    "        return F.mse_loss(x, y)\n",
    "    \n",
    "    def end_effector_loss(self, x, y):\n",
    "        # slice to get the end effector joints\n",
    "        x_ee = x[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)] \n",
    "        y_ee = y[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).to(device)]\n",
    "\n",
    "        # calculate velocities\n",
    "        x_vel = torch.norm(x_ee[:, 1:] - x_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        y_vel = torch.norm(y_ee[:, 1:] - y_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        \n",
    "        # compute mse loss for each joint\n",
    "        losses = F.mse_loss(x_vel, y_vel, reduction='none')\n",
    "\n",
    "        # take sum over end effectors\n",
    "        loss = losses.sum(dim=1)\n",
    "\n",
    "        # take mean over batch\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def smoothing_loss(self, y, y_pred):\n",
    "        # (batch, T, 75)\n",
    "        # Calculate the squared sum of differences for y and y_pred\n",
    "        diff_y = torch.sum(y[:, :-1] - y[:, 1:], dim=2) ** 2\n",
    "        diff_y_pred = torch.sum(y_pred[:, :-1] - y_pred[:, 1:], dim=2) ** 2\n",
    "\n",
    "        # Calculate the absolute difference\n",
    "        abs_diff = torch.abs(diff_y - diff_y_pred)\n",
    "\n",
    "        # Sum over all batches and sequence elements\n",
    "        loss = torch.sum(abs_diff)\n",
    "\n",
    "        # Normalize by the total number of elements (batch_size * sequence_length)\n",
    "        total_loss = torch.sqrt(loss) / (y.size(0) * y.size(1))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def adv_loss(self, model, x, y):\n",
    "        return self.cross_entropy(model(x), y)#.long().to(device))\n",
    "    \n",
    "    def adv_accuracy(self, model, x, y):\n",
    "        return (model(x).argmax(dim=1) == y.argmax(dim=1).to(device)).float().mean()\n",
    "\n",
    "    def train_adv_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, train_emb = True, train_discrim = True):\n",
    "        if not self.use_adv: return 0,0\n",
    "        # freeze encoders/decoder\n",
    "        self.dynamic_encoder.eval()\n",
    "        self.static_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        # unfreeze adversaries\n",
    "        self.priv_adv.train()\n",
    "        self.util_adv.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        # zero out gradients\n",
    "        self.priv_optim.zero_grad()\n",
    "        self.util_optim.zero_grad()\n",
    "        self.discriminator_optim.zero_grad()\n",
    "\n",
    "        # encode\n",
    "        d1 = self.dynamic_encoder(x1_rot) # A1\n",
    "        d2 = self.dynamic_encoder(x2_rot) # A2\n",
    "        d3 = self.dynamic_encoder(y1_rot) # A2\n",
    "        d4 = self.dynamic_encoder(y2_rot) # A1\n",
    "        s1 = self.static_encoder(x1) # P1\n",
    "        s2 = self.static_encoder(x2) # P2\n",
    "        s3 = self.static_encoder(y1) # P1\n",
    "        s4 = self.static_encoder(y2) # P2\n",
    "\n",
    "        # decode\n",
    "        x1_hat = self.decoder(torch.cat((d1, s1), dim=1)) # P1, A1\n",
    "        x2_hat = self.decoder(torch.cat((d2, s2), dim=1)) # P2, A2\n",
    "        y1_hat = self.decoder(torch.cat((d3, s3), dim=1)) # P1, A2\n",
    "        y2_hat = self.decoder(torch.cat((d4, s4), dim=1)) # P2, A1\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            # train privacy adversary\n",
    "            p1, p2 = actors[0] - 1, actors[1] - 1\n",
    "            p1, p2 = torch.eye(privacy_classes)[p1.long()].to(device), torch.eye(privacy_classes)[p2.long()].to(device)\n",
    "            priv_loss = (self.cross_entropy(self.priv_adv(d1), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d2), p2) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d3), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d4), p2)) / 4\n",
    "            priv_acc = (self.adv_accuracy(self.priv_adv, d1, p1) + self.adv_accuracy(self.priv_adv, d2, p2) + self.adv_accuracy(self.priv_adv, d3, p1) + self.adv_accuracy(self.priv_adv, d4, p2)) / 4\n",
    "            priv_loss.backward(retain_graph=True)\n",
    "            self.priv_optim.step()\n",
    "\n",
    "            # train privacy cooperative\n",
    "            priv_coop_loss = (self.cross_entropy(self.priv_coop(s1), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s2), p2) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s3), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s4), p2)) / 4\n",
    "            priv_coop_acc = (self.adv_accuracy(self.priv_coop, s1, p1) + self.adv_accuracy(self.priv_coop, s2, p2) + self.adv_accuracy(self.priv_coop, s3, p1) + self.adv_accuracy(self.priv_coop, s4, p2)) / 4\n",
    "            priv_coop_loss.backward(retain_graph=True)\n",
    "            self.priv_coop_optim.step()\n",
    "                        \n",
    "            # train utility adversary\n",
    "            a1, a2 = actions[0] - 1, actions[1] - 1\n",
    "            a1, a2 = torch.eye(utility_classes)[a1.long()].to(device), torch.eye(utility_classes)[a2.long()].to(device)\n",
    "            util_loss = (self.cross_entropy(self.util_adv(s1), a1) + \\\n",
    "                        self.cross_entropy(self.util_adv(s2), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s3), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s4), a1)) / 4\n",
    "            util_acc = (self.adv_accuracy(self.util_adv, s1, a1) + self.adv_accuracy(self.util_adv, s2, a2) + self.adv_accuracy(self.util_adv, s3, a2) + self.adv_accuracy(self.util_adv, s4, a1)) / 4\n",
    "            util_loss.backward(retain_graph=True)\n",
    "            self.util_optim.step()\n",
    "\n",
    "            # train utility cooperative\n",
    "            util_coop_loss = (self.cross_entropy(self.util_coop(d1), a1) + \\\n",
    "                            self.cross_entropy(self.util_coop(d2), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d3), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d4), a1)) / 4\n",
    "            util_coop_acc = (self.adv_accuracy(self.util_coop, d1, a1) + self.adv_accuracy(self.util_coop, d2, a2) + self.adv_accuracy(self.util_coop, d3, a2) + self.adv_accuracy(self.util_coop, d4, a1)) / 4\n",
    "            util_coop_loss.backward(retain_graph=True)\n",
    "            self.util_coop_optim.step()\n",
    "\n",
    "\n",
    "        if train_discrim:\n",
    "            # train discriminator\n",
    "            output_real = self.discriminator(torch.cat((x1.view(x1.size(0), T, -1), x2.view(x2.size(0), T, -1), y1.view(y1.size(0), T, -1), y2.view(y1.size(0), T, -1))))\n",
    "            output_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat)))\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / (4 * batch_size)) + (torch.sum(torch.round(output_real) == 1).float() / (4 * batch_size))) / 2\n",
    "            discriminator_loss.backward()\n",
    "            self.discriminator_optim.step()\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.dynamic_encoder.train()\n",
    "        self.static_encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        # freeze adversaries\n",
    "        self.priv_adv.eval()\n",
    "        self.priv_coop.eval()\n",
    "        self.util_adv.eval()\n",
    "        self.util_coop.eval()\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def train_adv_unpaired(self, x_pos, x_rot, actor, action, train_emb = True, train_discrim = True):\n",
    "        # ensure one training method is enabled\n",
    "        assert train_emb or train_discrim, 'At least one training method must be enabled'\n",
    "\n",
    "        # freeze encoders/decoder\n",
    "        self.dynamic_encoder.eval()\n",
    "        self.static_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        # unfreeze adversaries\n",
    "        self.priv_adv.train()\n",
    "        self.priv_coop.train()\n",
    "        self.util_adv.train()\n",
    "        self.util_coop.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        # zero out gradients\n",
    "        self.priv_optim.zero_grad()\n",
    "        self.priv_coop_optim.zero_grad()\n",
    "        self.util_optim.zero_grad()\n",
    "        self.util_coop_optim.zero_grad()\n",
    "        self.discriminator_optim.zero_grad()\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            p = actor - 1\n",
    "            p = torch.eye(privacy_classes)[p.long()].to(device)\n",
    "            a = action - 1\n",
    "            a = torch.eye(utility_classes)[a.long()].to(device)\n",
    "\n",
    "            # train privacy adversary\n",
    "            priv_loss = self.adv_loss(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "            priv_acc = self.adv_accuracy(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "            priv_loss.backward()\n",
    "            self.priv_optim.step()\n",
    "\n",
    "            # tain privacy cooperative\n",
    "            priv_coop_loss = self.adv_loss(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            priv_coop_acc = self.adv_accuracy(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            priv_coop_loss.backward()\n",
    "            self.priv_coop_optim.step()\n",
    "            \n",
    "            # train utility adversary\n",
    "            util_loss = self.adv_loss(self.util_adv, self.static_encoder(x_pos), a)\n",
    "            util_acc = self.adv_accuracy(self.util_adv, self.static_encoder(x_pos), a)\n",
    "            util_loss.backward()\n",
    "            self.util_optim.step()\n",
    "\n",
    "            # train utility cooperative\n",
    "            util_coop_loss = self.adv_loss(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "            util_coop_acc = self.adv_accuracy(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "            util_coop_loss.backward()\n",
    "            self.util_coop_optim.step()\n",
    "\n",
    "        if train_discrim:\n",
    "            # encode\n",
    "            d = self.dynamic_encoder(x_rot)\n",
    "            s = self.static_encoder(x_pos)\n",
    "\n",
    "            # decode\n",
    "            x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "            # train discriminator\n",
    "            output_real = self.discriminator(x_pos.reshape(x_pos.size(0), T, -1))\n",
    "            output_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / batch_size) + (torch.sum(torch.round(output_real) == 1).float() / batch_size)) / 2\n",
    "            discriminator_loss.backward()\n",
    "            self.discriminator_optim.step()\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.dynamic_encoder.train()\n",
    "        self.static_encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        # freeze adversaries\n",
    "        self.priv_adv.eval()\n",
    "        self.priv_coop.eval()\n",
    "        self.util_adv.eval()\n",
    "        self.util_coop.eval()\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def val_adv_paired(self, x1, x1_rot, x2, x2_rot, y1, y1_rot, y2, y2_rot, actors, actions, train_emb = True, train_discrim = True):\n",
    "        if not self.use_adv: return 0,0\n",
    "\n",
    "        # freeze encoders/decoder\n",
    "        self.set_eval()\n",
    "\n",
    "        # Encode\n",
    "        d1, d2, d3, d4 = [self.dynamic_encoder(x) for x in [x1_rot, x2_rot, y1_rot, y2_rot]]\n",
    "        s1, s2, s3, s4 = [self.static_encoder(x) for x in [x1, x2, y1, y2]]\n",
    "\n",
    "        # Decode\n",
    "        x1_hat, x2_hat, y1_hat, y2_hat = [self.decoder(torch.cat((d, s), dim=1)) for d, s in zip([d1, d2, d3, d4], [s1, s2, s3, s4])]\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            # privacy adversary\n",
    "            p1, p2 = actors[0] - 1, actors[1] - 1\n",
    "            p1, p2 = torch.eye(privacy_classes)[p1.long()].to(device), torch.eye(privacy_classes)[p2.long()].to(device)\n",
    "            priv_loss = (self.cross_entropy(self.priv_adv(d1), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d2), p2) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d3), p1) + \\\n",
    "                        self.cross_entropy(self.priv_adv(d4), p2)) / 4\n",
    "            priv_acc = (self.adv_accuracy(self.priv_adv, d1, p1) + self.adv_accuracy(self.priv_adv, d2, p2) + self.adv_accuracy(self.priv_adv, d3, p1) + self.adv_accuracy(self.priv_adv, d4, p2)) / 4\n",
    "\n",
    "            # privacy cooperative\n",
    "            priv_coop_loss = (self.cross_entropy(self.priv_coop(s1), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s2), p2) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s3), p1) + \\\n",
    "                            self.cross_entropy(self.priv_coop(s4), p2)) / 4\n",
    "            priv_coop_acc = (self.adv_accuracy(self.priv_coop, s1, p1) + self.adv_accuracy(self.priv_coop, s2, p2) + self.adv_accuracy(self.priv_coop, s3, p1) + self.adv_accuracy(self.priv_coop, s4, p2)) / 4\n",
    "                        \n",
    "            # utility adversary\n",
    "            a1, a2 = actions[0] - 1, actions[1] - 1\n",
    "            a1, a2 = torch.eye(utility_classes)[a1.long()].to(device), torch.eye(utility_classes)[a2.long()].to(device)\n",
    "            util_loss = (self.cross_entropy(self.util_adv(s1), a1) + \\\n",
    "                        self.cross_entropy(self.util_adv(s2), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s3), a2) + \\\n",
    "                        self.cross_entropy(self.util_adv(s4), a1)) / 4\n",
    "            util_acc = (self.adv_accuracy(self.util_adv, s1, a1) + self.adv_accuracy(self.util_adv, s2, a2) + self.adv_accuracy(self.util_adv, s3, a2) + self.adv_accuracy(self.util_adv, s4, a1)) / 4\n",
    "\n",
    "            # utility cooperative\n",
    "            util_coop_loss = (self.cross_entropy(self.util_coop(d1), a1) + \\\n",
    "                            self.cross_entropy(self.util_coop(d2), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d3), a2) + \\\n",
    "                            self.cross_entropy(self.util_coop(d4), a1)) / 4\n",
    "            util_coop_acc = (self.adv_accuracy(self.util_coop, d1, a1) + self.adv_accuracy(self.util_coop, d2, a2) + self.adv_accuracy(self.util_coop, d3, a2) + self.adv_accuracy(self.util_coop, d4, a1)) / 4\n",
    "\n",
    "\n",
    "        if train_discrim:\n",
    "            # discriminator\n",
    "            output_real = self.discriminator(torch.cat((x1.view(x1.size(0), T, -1), x2.view(x2.size(0), T, -1), y1.view(y1.size(0), T, -1), y2.view(y1.size(0), T, -1))))\n",
    "            output_fake = self.discriminator(torch.cat((x1_hat, x2_hat, y1_hat, y2_hat)))\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / (4 * batch_size)) + (torch.sum(torch.round(output_real) == 1).float() / (4 * batch_size))) / 2\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.set_eval(False)\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def val_adv_unpaired(self, x_pos, x_rot, actor, action, train_emb = True, train_discrim = True):\n",
    "        # ensure one training method is enabled\n",
    "        assert train_emb or train_discrim, 'At least one training method must be enabled'\n",
    "\n",
    "        # freeze encoders/decoder\n",
    "        self.set_eval()\n",
    "\n",
    "        # Encode\n",
    "        d = self.dynamic_encoder(x_rot)\n",
    "        s = self.static_encoder(x_pos)\n",
    "\n",
    "        # Decode\n",
    "        x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "        # instantiate losses\n",
    "        priv_loss = torch.zeros(1).to(device)\n",
    "        priv_coop_loss = torch.zeros(1).to(device)\n",
    "        priv_acc = torch.zeros(1).to(device)\n",
    "        priv_coop_acc = torch.zeros(1).to(device)\n",
    "        util_loss = torch.zeros(1).to(device)\n",
    "        util_coop_loss = torch.zeros(1).to(device)\n",
    "        util_acc = torch.zeros(1).to(device)\n",
    "        util_coop_acc = torch.zeros(1).to(device)\n",
    "        discriminator_loss = torch.zeros(1).to(device)\n",
    "        discriminator_acc = torch.zeros(1).to(device)\n",
    "\n",
    "        if train_emb:\n",
    "            p = actor - 1\n",
    "            p = torch.eye(privacy_classes)[p.long()].to(device)\n",
    "            a = action - 1\n",
    "            a = torch.eye(utility_classes)[a.long()].to(device)\n",
    "\n",
    "            # privacy adversary\n",
    "            priv_loss = self.adv_loss(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "            priv_acc = self.adv_accuracy(self.priv_adv, self.dynamic_encoder(x_rot), p)\n",
    "\n",
    "            # privacy cooperative\n",
    "            priv_coop_loss = self.adv_loss(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            priv_coop_acc = self.adv_accuracy(self.priv_coop, self.static_encoder(x_pos), p)\n",
    "            \n",
    "            # utility adversary\n",
    "            util_loss = self.adv_loss(self.util_adv, self.static_encoder(x_pos), a)\n",
    "            util_acc = self.adv_accuracy(self.util_adv, self.static_encoder(x_pos), a)\n",
    "\n",
    "            # utility cooperative\n",
    "            util_coop_loss = self.adv_loss(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "            util_coop_acc = self.adv_accuracy(self.util_coop, self.dynamic_encoder(x_rot), a)\n",
    "\n",
    "        if train_discrim:\n",
    "            # encode\n",
    "            d = self.dynamic_encoder(x_rot)\n",
    "            s = self.static_encoder(x_pos)\n",
    "\n",
    "            # decode\n",
    "            x_hat = self.decoder(torch.cat((d, s), dim=1))\n",
    "\n",
    "            # train discriminator\n",
    "            output_real = self.discriminator(x_pos.reshape(x_pos.size(0), T, -1))\n",
    "            output_fake = self.discriminator(x_hat)\n",
    "            discriminator_loss = self.bce_loss(output_real, torch.ones_like(output_real)) + self.bce_loss(output_fake, torch.zeros_like(output_fake))\n",
    "            discriminator_acc = ((torch.sum(torch.round(output_fake) == 0).float() / batch_size) + (torch.sum(torch.round(output_real) == 1).float() / batch_size)) / 2\n",
    "\n",
    "        # unfreeze encoders/decoder\n",
    "        self.set_eval(False)\n",
    "\n",
    "        return priv_loss.item(), priv_coop_loss.item(), util_loss.item(), util_coop_loss.item(), discriminator_loss.item(), priv_acc.item(), util_acc.item(), priv_coop_acc.item(), util_coop_acc.item(), discriminator_acc.item()\n",
    "\n",
    "    def forward(self, x, x_rot):\n",
    "        dyn = self.dynamic_encoder(x_rot)\n",
    "        sta = self.static_encoder(x)\n",
    "        x = self.decoder(torch.cat((dyn, sta), dim=1))\n",
    "        return x\n",
    "    \n",
    "    def set_eval(self, eval=True):\n",
    "        if eval:\n",
    "            self.static_encoder.eval()\n",
    "            self.dynamic_encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            self.priv_adv.eval()\n",
    "            self.priv_coop.eval()\n",
    "            self.util_adv.eval()\n",
    "            self.util_coop.eval()\n",
    "            self.discriminator.eval()\n",
    "        else:\n",
    "            self.static_encoder.train()\n",
    "            self.dynamic_encoder.train()\n",
    "            self.decoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility/Privacy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, k=3):\n",
    "    acces = AverageMeter()\n",
    "    topk_acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "        topk_acc = top_k_accuracy(output.data, target.cuda(), k=k)\n",
    "        topk_acces.update(topk_acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "\n",
    "    label_index = np.argmax(label_output, axis=1)\n",
    "    pred_index = np.argmax(pred_output, axis=1)\n",
    "\n",
    "    f1 = f1_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "    precision = precision_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "    recall = recall_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "\n",
    "    return acces.avg, f1, precision, recall, topk_acces.avg\n",
    "    \n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)\n",
    "\n",
    "def top_k_accuracy(output, target, k=3):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(k, 1, True, True) \n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1) \n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "    return correct_k.mul_(100.0 / batch_size)\n",
    "    \n",
    "def run_sgn_eval(train_x, train_y, test_x, test_y, val_x, val_y, case, model, k=3):\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, case, seg=20, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    test_loader = ntu_loaders.get_test_loader(batch_size, 16)\n",
    "\n",
    "    # Test\n",
    "    return test(test_loader, model, k=k)\n",
    "\n",
    "def run_sgn_gender_eval(train_x, train_y, test_x, test_y, val_x, val_y, model, k=1):\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, 0, seg=20, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    test_loader = ntu_loaders.get_test_loader(batch_size, 16)\n",
    "\n",
    "    acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "\n",
    "    label_index = np.argmax(label_output, axis=1)\n",
    "    pred_index = np.argmax(pred_output, axis=1)\n",
    "\n",
    "    f1 = f1_score(label_index, pred_index, average='macro', zero_division=0)\n",
    "\n",
    "    return acces.avg, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(adv_lr=adv_lr).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_29036\\273552988.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('pretrained/MR.pt'))\n"
     ]
    }
   ],
   "source": [
    "load_model = True\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load('pretrained/MR.pt'))\n",
    "    # model.load_state_dict(torch.load('final_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_29036\\471309301.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if remove_two_actor_actions: sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60_sgnpt_no_two_actor.pt')['state_dict'])\n",
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_29036\\471309301.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_60_sgnpt.pt')['state_dict'])\n"
     ]
    }
   ],
   "source": [
    "load_util = True\n",
    "sgn_ar = SGN(utility_classes, None, seg, batch_size, 0).to(device)\n",
    "sgn_priv = SGN(privacy_classes, None, seg, batch_size, 0).to(device)\n",
    "\n",
    "if ntu_120:\n",
    "    if only_use_pos: # Assumes SGN preprocessing\n",
    "        raise NotImplementedError\n",
    "        sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_sgnpt.pt')['state_dict'])\n",
    "        sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_sgnpt.pt')['state_dict'])\n",
    "    else:\n",
    "        sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy.pt')['state_dict'])\n",
    "        sgn_ar.load_state_dict(torch.load('SGN/pretrained/action.pt')['state_dict'])\n",
    "else:\n",
    "    if only_use_pos:\n",
    "        if remove_two_actor_actions: sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60_sgnpt_no_two_actor.pt')['state_dict'])\n",
    "        else: sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60_sgnpt.pt')['state_dict'])\n",
    "        sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_60_sgnpt.pt')['state_dict'])\n",
    "    else: \n",
    "        sgn_priv.load_state_dict(torch.load('SGN/pretrained/privacy_60.pt')['state_dict'])\n",
    "        sgn_ar.load_state_dict(torch.load('SGN/pretrained/action_60.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retargeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_29036\\3005026026.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dmr.load_state_dict(torch.load('pretrained/DMR.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmr = DMR().to(device)\n",
    "dmr.load_state_dict(torch.load('pretrained/DMR.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_29036\\2023599903.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load('pretrained/MR.pt')\n"
     ]
    }
   ],
   "source": [
    "val_model = AutoEncoder(use_adv=False).to(device)\n",
    "weights = torch.load('pretrained/MR.pt')\n",
    "keys_to_rem = [\"priv_adv.conv1.weight\", \"priv_adv.conv1.bias\", \"priv_adv.conv2.weight\", \"priv_adv.conv2.bias\", \"priv_adv.conv3.weight\", \"priv_adv.conv3.bias\", \"priv_adv.bn1.weight\", \"priv_adv.bn1.bias\", \"priv_adv.bn1.running_mean\", \"priv_adv.bn1.running_var\", \"priv_adv.bn1.num_batches_tracked\", \"priv_adv.bn2.weight\", \"priv_adv.bn2.bias\", \"priv_adv.bn2.running_mean\", \"priv_adv.bn2.running_var\", \"priv_adv.bn2.num_batches_tracked\", \"priv_adv.bn3.weight\", \"priv_adv.bn3.bias\", \"priv_adv.bn3.running_mean\", \"priv_adv.bn3.running_var\", \"priv_adv.bn3.num_batches_tracked\", \"priv_adv.fc1.weight\", \"priv_adv.fc1.bias\", \"priv_adv.fc2.weight\", \"priv_adv.fc2.bias\", \"priv_adv.fc3.weight\", \"priv_adv.fc3.bias\", \"priv_coop.conv1.weight\", \"priv_coop.conv1.bias\", \"priv_coop.conv2.weight\", \"priv_coop.conv2.bias\", \"priv_coop.conv3.weight\", \"priv_coop.conv3.bias\", \"priv_coop.bn1.weight\", \"priv_coop.bn1.bias\", \"priv_coop.bn1.running_mean\", \"priv_coop.bn1.running_var\", \"priv_coop.bn1.num_batches_tracked\", \"priv_coop.bn2.weight\", \"priv_coop.bn2.bias\", \"priv_coop.bn2.running_mean\", \"priv_coop.bn2.running_var\", \"priv_coop.bn2.num_batches_tracked\", \"priv_coop.bn3.weight\", \"priv_coop.bn3.bias\", \"priv_coop.bn3.running_mean\", \"priv_coop.bn3.running_var\", \"priv_coop.bn3.num_batches_tracked\", \"priv_coop.fc1.weight\", \"priv_coop.fc1.bias\", \"priv_coop.fc2.weight\", \"priv_coop.fc2.bias\", \"priv_coop.fc3.weight\", \"priv_coop.fc3.bias\", \"util_adv.conv1.weight\", \"util_adv.conv1.bias\", \"util_adv.conv2.weight\", \"util_adv.conv2.bias\", \"util_adv.conv3.weight\", \"util_adv.conv3.bias\", \"util_adv.bn1.weight\", \"util_adv.bn1.bias\", \"util_adv.bn1.running_mean\", \"util_adv.bn1.running_var\", \"util_adv.bn1.num_batches_tracked\", \"util_adv.bn2.weight\", \"util_adv.bn2.bias\", \"util_adv.bn2.running_mean\", \"util_adv.bn2.running_var\", \"util_adv.bn2.num_batches_tracked\", \"util_adv.bn3.weight\", \"util_adv.bn3.bias\", \"util_adv.bn3.running_mean\", \"util_adv.bn3.running_var\", \"util_adv.bn3.num_batches_tracked\", \"util_adv.fc1.weight\", \"util_adv.fc1.bias\", \"util_adv.fc2.weight\", \"util_adv.fc2.bias\", \"util_adv.fc3.weight\", \"util_adv.fc3.bias\", \"util_coop.conv1.weight\", \"util_coop.conv1.bias\", \"util_coop.conv2.weight\", \"util_coop.conv2.bias\", \"util_coop.conv3.weight\", \"util_coop.conv3.bias\", \"util_coop.bn1.weight\", \"util_coop.bn1.bias\", \"util_coop.bn1.running_mean\", \"util_coop.bn1.running_var\", \"util_coop.bn1.num_batches_tracked\", \"util_coop.bn2.weight\", \"util_coop.bn2.bias\", \"util_coop.bn2.running_mean\", \"util_coop.bn2.running_var\", \"util_coop.bn2.num_batches_tracked\", \"util_coop.bn3.weight\", \"util_coop.bn3.bias\", \"util_coop.bn3.running_mean\", \"util_coop.bn3.running_var\", \"util_coop.bn3.num_batches_tracked\", \"util_coop.fc1.weight\", \"util_coop.fc1.bias\", \"util_coop.fc2.weight\", \"util_coop.fc2.bias\", \"util_coop.fc3.weight\", \"util_coop.fc3.bias\", \"discriminator.enc1.weight\", \"discriminator.enc1.bias\", \"discriminator.enc2.weight\", \"discriminator.enc2.bias\", \"discriminator.enc3.weight\", \"discriminator.enc3.bias\", \"discriminator.enc4.weight\", \"discriminator.enc4.bias\", \"discriminator.fc1.weight\", \"discriminator.fc1.bias\", \"discriminator.fc2.weight\", \"discriminator.fc2.bias\"]\n",
    "for key in keys_to_rem:\n",
    "    del weights[key]\n",
    "val_model.load_state_dict(weights)\n",
    "model = val_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retarget_random_action():\n",
    "    X_hat_random = {}\n",
    "    X_hat_constant = {}\n",
    "\n",
    "    # const = random.sample(list(X.keys()), 1)[0]\n",
    "    const = 'S007C001P025R001A045'.encode('utf-8')\n",
    "    x2_const = X[const].float().cuda().unsqueeze(0)\n",
    "    print(const)\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for file in tqdm(X):\n",
    "            x1 = X[file].unsqueeze(0)\n",
    "            while True:\n",
    "                sample = random.sample(list(X.keys()), 1)[0]\n",
    "                # ensure different actor\n",
    "                if sample.decode('utf-8')[9:12] != file.decode('utf-8')[9:12]:\n",
    "                    break\n",
    "            x2_random = X[sample].unsqueeze(0)\n",
    "            start = time.time()\n",
    "            print(x1.shape)\n",
    "            X_hat_random[file] = val_model.eval(x1.float().cuda(), x2_random.float().cuda()).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            start = time.time()\n",
    "            X_hat_constant[file] = val_model.eval(x1.float().cuda(), x2_const).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            # render_video(X_hat_random[file])\n",
    "            # render_video(X_hat_constant[file])\n",
    "\n",
    "    print(f'Average time: {np.mean(times)}')\n",
    "    \n",
    "    # Save results\n",
    "    # with open('results/DMR_X_hat_random_RA.pkl', 'wb') as f:\n",
    "    #     pickle.dump(X_hat_random, f)\n",
    "    # with open(f'results/DMR_X_hat_constant_RA.pkl', 'wb') as f:\n",
    "    #     pickle.dump(X_hat_constant, f)\n",
    "\n",
    "def retarget_constant_action():\n",
    "    X_hat_random = {}\n",
    "    X_hat_constant = {}\n",
    "\n",
    "    const = 8\n",
    "    # x2_const = X[const].float().cuda().unsqueeze(0)\n",
    "    times = []\n",
    "\n",
    "    const_dict = {}\n",
    "    # Get a sample of each action from the constant actor\n",
    "    for file in X:\n",
    "        info = parse_file_name(file)\n",
    "        if info['P'] == const and info['A'] not in const_dict:\n",
    "            const_dict[info['A']] = X[file].float().cuda().unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for file in tqdm(X):\n",
    "            x1 = X[file].unsqueeze(0)\n",
    "            info = parse_file_name(file)\n",
    "            while True:\n",
    "                sample = random.sample(list(X.keys()), 1)[0]\n",
    "                # ensure different actor and same action\n",
    "                info_ = parse_file_name(sample)\n",
    "                if info_['P'] != info['P'] and info_['A'] == info['A']:\n",
    "                    break\n",
    "            x2_random = X[sample].unsqueeze(0)\n",
    "            start = time.time()\n",
    "            X_hat_random[file] = val_model.eval(x1.float().cuda(), x2_random.float().cuda()).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            \n",
    "            start = time.time()\n",
    "            X_hat_constant[file] = val_model.eval(x1.float().cuda(), const_dict[info['A']]).cpu().numpy().squeeze()\n",
    "            times.append(time.time() - start)\n",
    "            \n",
    "            # render_video(X_hat_random[file])\n",
    "            # render_video(X_hat_constant[file])\n",
    "\n",
    "    print(f'Average time: {np.mean(times)}')\n",
    "    \n",
    "    # Save results\n",
    "    with open('results/DMR_X_hat_random_CA.pkl', 'wb') as f:\n",
    "        pickle.dump(X_hat_random, f)\n",
    "    with open(f'results/DMR_X_hat_constant_CA.pkl', 'wb') as f:\n",
    "        pickle.dump(X_hat_constant, f)\n",
    "\n",
    "def retarget_specific(dummy, reference = None, just_render = False, use_dmr = False):\n",
    "    X_hat = {}\n",
    "    X_hat_dmr = {}\n",
    "\n",
    "    x2_const = X[dummy].float().cuda().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        if reference is None:\n",
    "            for file in X:\n",
    "                x1 = X[file].unsqueeze(0)\n",
    "                X_hat[file] = val_model.eval(x1.float().cuda(), x2_const).cpu().numpy().squeeze()\n",
    "                if use_dmr: X_hat_dmr[file] = dmr.eval(x1.float().cuda(), x2_const).cpu().numpy().squeeze()\n",
    "                if just_render: \n",
    "                    render_video(X_hat[file])\n",
    "                    if use_dmr: render_video(X_hat_dmr[file])\n",
    "                break\n",
    "        else:\n",
    "            X_hat = model.eval(X[reference].unsqueeze(0).float().cuda(), X[dummy].unsqueeze(0).float().cuda()).cpu().numpy().squeeze()\n",
    "            if use_dmr: X_hat_dmr = dmr.eval(X[reference].unsqueeze(0).float().cuda(), X[dummy].unsqueeze(0).float().cuda()).cpu().numpy().squeeze()\n",
    "            if just_render: \n",
    "                render_video(X_hat)\n",
    "                if use_dmr: render_video(X_hat_dmr)\n",
    "            else:\n",
    "                render_video(X_hat, gif=f'pmr_{dummy}')\n",
    "                if use_dmr: render_video(X_hat_dmr, gif=f'dmr_{dummy}')\n",
    "\n",
    "    # Save results\n",
    "    # with open(f'results/X_hat_{dummy}.pkl', 'wb') as f:\n",
    "    #     pickle.dump(X_hat, f)\n",
    "\n",
    "    # if use_dmr:\n",
    "    #     with open(f'results/X_hat_dmr_{dummy}.pkl', 'wb') as f:\n",
    "    #         pickle.dump(X_hat_dmr, f)\n",
    "\n",
    "    return X_hat\n",
    "\n",
    "# retarget_random_action()\n",
    "# retarget_constant_action()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate utility of other baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrt\\AppData\\Local\\Temp\\ipykernel_29036\\3495056763.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sgn_gender.load_state_dict(torch.load('SGN/pretrained/gender.pt')['state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load SGN gender classification\n",
    "sgn_gender = SGN(2, None, seg, batch_size, 0).to(device)\n",
    "sgn_gender.load_state_dict(torch.load('SGN/pretrained/gender.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgn_train_x, sgn_train_y, sgn_val_x, sgn_val_y = np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1)), np.zeros((batch_size, 300, 150)), np.zeros((batch_size, 1))\n",
    "\n",
    "eval_renders_str_skeleton = ['S006C003P017R001A011.skeleton',\n",
    "                             'S015C003P019R001A011.skeleton']\n",
    "eval_renders_str = [x[:-9] for x in eval_renders_str_skeleton]\n",
    "eval_render_byte = [x.encode('utf-8') for x in eval_renders_str]\n",
    "genders = pd.read_csv('NTU\\SGN\\statistics\\Genders.csv').replace('M', 1).replace('F', 0)\n",
    "\n",
    "def anonymizer_to_sgn(t, max_frames=300):\n",
    "    xyz, frames, joints, actors = t.shape\n",
    "    \n",
    "    # Pre-allocate memory for the output array\n",
    "    X = np.zeros((max_frames, xyz * joints * actors), dtype=np.float32)\n",
    "    \n",
    "    # Reshape the input array for easier manipulation\n",
    "    t_reshaped = t.reshape((frames, -1))\n",
    "    \n",
    "    # Copy over the reshaped data to the pre-allocated output\n",
    "    X[:frames, :t_reshaped.shape[1]] = t_reshaped\n",
    "    \n",
    "    return X\n",
    "\n",
    "def eval(X_dict, gif_name=None, cameras=None, just_render=False):\n",
    "    # Remove NTU120 if needed\n",
    "    if not ntu_120:\n",
    "        X_dict = {k: v for k, v in X_dict.items() if int(k[17:20]) <= 60}\n",
    "\n",
    "    if only_ntu_120:\n",
    "        X_dict = {k: v for k, v in X_dict.items() if int(k[17:20]) > 60}\n",
    "\n",
    "    # Remove cameras if needed\n",
    "    if cameras is not None:\n",
    "        X_dict = {k: v for k, v in X_dict.items() if int(k[7]) in cameras}\n",
    "\n",
    "    print(f'Number of files: {len(X_dict)}')\n",
    "\n",
    "    if not just_render:\n",
    "        # Calculate MSE\n",
    "        anon = torch.zeros((len(X_dict), 300, 75))\n",
    "        raw = torch.zeros((len(X_dict), 75, 25, 3))\n",
    "        rem=0\n",
    "        for i, file in enumerate(X_dict):\n",
    "            if only_use_pos:\n",
    "                if type(file) != np.bytes_: file_byte = file.split('.')[0].encode('utf-8')\n",
    "                else: file_byte = file\n",
    "            else: file_byte = file\n",
    "            \n",
    "            # Ensure file exists in both dicts\n",
    "            if file_byte not in X or file not in X_dict:\n",
    "                rem+=1\n",
    "                continue\n",
    "            anon[i] = torch.tensor(X_dict[file])\n",
    "            if only_use_pos:\n",
    "                raw[i] = X[file_byte]\n",
    "            else:\n",
    "                raw[i] = X[file_byte][:, :, :3]\n",
    "\n",
    "        # Remove non existent files\n",
    "        anon = anon[:len(X_dict)-rem]\n",
    "        raw = raw[:len(X_dict)-rem]\n",
    "\n",
    "        # Remove zeros from the end of the sequence\n",
    "        for i in range(anon.shape[1]):\n",
    "            if not torch.all(anon[:, i] == 0):\n",
    "                anon = anon[:, :i+1]\n",
    "                raw = raw[:, :i+1]\n",
    "                break\n",
    "\n",
    "        # Reshape anon to be 75, 25, 3\n",
    "        if anon.shape[1] > 75: anon = anon[:, :75, :]\n",
    "        anon = anon[:, :, :75]\n",
    "        anon = anon.reshape((anon.shape[0], anon.shape[1], 25, 3))\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = torch.mean((anon - raw)**2, dim=3)\n",
    "        l2 = torch.mean(torch.sqrt(torch.sum((anon-raw)**2, dim=3)))\n",
    "        print(f'MSE:\\t\\t\\t\\t{torch.mean(mse)}\\nL2:\\t\\t\\t\\t{l2}\\n')\n",
    "\n",
    "        # Pre-allocate memory for the output array\n",
    "        x = np.zeros((len(X_dict), 300, 150), dtype=np.float32)\n",
    "        y_util = np.zeros(len(X_dict))\n",
    "        y_priv = np.zeros(len(X_dict))\n",
    "        y_gender = np.zeros(len(X_dict))\n",
    "\n",
    "        for i, file in enumerate(X_dict):\n",
    "            if X_dict[file].shape[1] == 75:\n",
    "                X_dict[file] = np.pad(X_dict[file], ((0, 0), (0, 75)), 'constant')\n",
    "\n",
    "            x[i] = np.array(X_dict[file], dtype=np.float32)\n",
    "            y_util[i] = int(file[17:20])\n",
    "            y_priv[i] = int(file[9:12])\n",
    "            y_gender[i] = genders.loc[y_priv[i]-1, 'Gender']\n",
    "\n",
    "        y_util = y_util - 1\n",
    "        y_priv = y_priv - 1\n",
    "        y_util = np.eye(utility_classes)[y_util.astype(int)]\n",
    "        y_priv = np.eye(privacy_classes)[y_priv.astype(int)]\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        acc, f1, prec, recall, topk = run_sgn_eval(sgn_train_x, sgn_train_y, x, y_util, sgn_val_x, sgn_val_y, 1, sgn_ar, k=k)\n",
    "        print(f'Utility Accuracy:\\t\\t{acc}\\nUtility F1:\\t\\t\\t{f1*100}\\nUtility Precision:\\t\\t{prec*100}\\nUtility Recall:\\t\\t\\t{recall*100}\\nTop-{k} Accuracy:\\t\\t\\t{topk}\\n')\n",
    "\n",
    "        ar_acc = acc\n",
    "\n",
    "        acc, f1, prec, recall, topk = run_sgn_eval(sgn_train_x, sgn_train_y, x, y_priv, sgn_val_x, sgn_val_y, 1, sgn_priv, k=k)\n",
    "        print(f'Privacy Accuracy:\\t\\t{acc}\\nPrivacy F1:\\t\\t\\t{f1*100}\\nPrivacy Precision:\\t\\t{prec*100}\\nPrivacy Recall:\\t\\t\\t{recall*100}\\nTop-{k} Accuracy:\\t\\t\\t{topk}\\n')\n",
    "\n",
    "        ri_acc = acc\n",
    "\n",
    "        # Gender classification\n",
    "        acc, f1 = run_sgn_gender_eval(sgn_train_x, sgn_train_y, x, y_priv, sgn_val_x, sgn_val_y, sgn_priv)\n",
    "        print(f'Gender Classificiation Accuracy:\\t{acc}\\nF1:\\t\\t\\t\\t{f1*100}\\n')\n",
    "\n",
    "        gc_acc = acc\n",
    "    \n",
    "        # Return accuracies\n",
    "        return ar_acc, ri_acc, gc_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x_pkl, from_moon=False, pad_data=True, gif_name='', cameras=None, just_render=False):\n",
    "    with open(x_pkl, 'rb') as f:\n",
    "        test_x = pickle.load(f)\n",
    "\n",
    "    if from_moon:\n",
    "        test_x = {k: v[0] for k, v in test_x.items()}\n",
    "        for file in test_x:\n",
    "            # Assuming anonymizer_to_sgn is a predefined function you have\n",
    "            test_x[file] = anonymizer_to_sgn(test_x[file])[:, :75]\n",
    "\n",
    "    if pad_data:\n",
    "        for file in test_x:\n",
    "            if test_x[file].shape[0] == 1:\n",
    "                test_x[file] = test_x[file][0]\n",
    "            test_x[file] = np.pad(test_x[file], ((0, 300-test_x[file].shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "    # If keys are bytes, convert to string\n",
    "    if type(list(test_x.keys())[0]) == np.bytes_: test_x = {k.decode('utf-8'): v for k, v in test_x.items()}\n",
    "\n",
    "    return eval(test_x, gif_name=gif_name, cameras=cameras, just_render=just_render)\n",
    "\n",
    "datasets = {\n",
    "    # 'pmr_constant_CA': ('results/X_hat_constant_CA.pkl', False, True),\n",
    "    # 'dmr_constant_CA': ('results/DMR_X_hat_constant_CA.pkl', False, True),\n",
    "    # 'moon_unet': ('C:\\\\Users\\\\Carrt\\\\OneDrive\\\\Code\\\\Linkage Attack\\\\External Repositories\\\\Skeleton-anonymization\\\\X_unet_file.pkl', True, False),\n",
    "    # 'moon_resnet': ('C:\\\\Users\\\\Carrt\\\\OneDrive\\\\Code\\\\Linkage Attack\\\\External Repositories\\\\Skeleton-anonymization\\\\X_resnet_file.pkl', True, False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all datasets\n",
    "for gif_name, (x_pkl, from_moon, pad_data) in datasets.items():\n",
    "    print(f'Processing {gif_name}')\n",
    "    ar_, ri_, gc_ = [], [], []\n",
    "    for i in range(5):\n",
    "        ar, ri, gc = process_data(x_pkl, from_moon=from_moon, pad_data=pad_data, gif_name=gif_name, just_render=False)\n",
    "        ar_.append(ar)\n",
    "        ri_.append(ri)\n",
    "        gc_.append(gc)\n",
    "    ar_ = np.array(ar_)\n",
    "    ri_ = np.array(ri_)\n",
    "    gc_ = np.array(gc_)\n",
    "    \n",
    "    print(f'AR: {np.mean(ar_)}\\t{np.std(ar_)}')\n",
    "    print(f'RI: {np.mean(ri_)}\\t{np.std(ri_)}')\n",
    "    print(f'GC: {np.mean(gc_)}\\t{np.std(gc_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw\n",
    "if only_use_pos:\n",
    "    with open('ntu/SGN/X_full.pkl', 'rb') as f:\n",
    "        raw = pickle.load(f)\n",
    "else:\n",
    "    with open('ntu/X.pkl', 'rb') as f:\n",
    "        raw = pickle.load(f)\n",
    "\n",
    "for file in raw:\n",
    "    if only_use_pos:\n",
    "        # chop from (300, 150) to (300, 75)\n",
    "        raw[file] = raw[file][:, :75]\n",
    "    else:\n",
    "        # reshape from (frame, 25, 3) to (frame, 75)\n",
    "        raw[file] = raw[file][:, :, :3].reshape((raw[file].shape[0], 75))\n",
    "        # pad data to 300 frames\n",
    "        if raw[file].shape[0] == 300: continue\n",
    "        raw[file] = np.pad(raw[file], ((0, 300-raw[file].shape[0]), (0, 0)), 'constant')\n",
    "        if raw[file].shape != (300, 75): \n",
    "            print(file, raw[file].shape)\n",
    "            del raw[file]\n",
    "ar_, ri_, gc_ = [], [], []\n",
    "for i in range(5):\n",
    "    ar, ri, gc = eval(raw, gif_name=None)\n",
    "    ar_.append(ar)\n",
    "    ri_.append(ri)\n",
    "    gc_.append(gc)\n",
    "\n",
    "ar_ = np.array(ar_)\n",
    "ri_ = np.array(ri_)\n",
    "gc_ = np.array(gc_)\n",
    "print(f'AR: {np.mean(ar_)}\\t{np.std(ar_)}')\n",
    "print(f'RI: {np.mean(ri_)}\\t{np.std(ri_)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
