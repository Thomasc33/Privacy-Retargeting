{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ntu/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to delete 28814\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "to_del = []\n",
    "for file in X:\n",
    "    if type(X[file]) == list:\n",
    "        to_del.append(file)\n",
    "print('to delete', len(to_del))\n",
    "for file in to_del:\n",
    "    del X[file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad or trim data to 75 frames. when padding, repeat the last frame\n",
    "# input is of shape (frames, 75)\n",
    "T = 75\n",
    "for file in X:\n",
    "    if X[file].shape[0] < T:\n",
    "        X[file] = np.pad(X[file], ((0, T - X[file].shape[0]), (0, 0)), mode='edge')\n",
    "    elif X[file].shape[0] > T:\n",
    "        X[file] = X[file][:T, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in X:\n",
    "    X[file] = torch.tensor(X[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "p = {}\n",
    "for file in X:\n",
    "    if file[16:20] not in a:\n",
    "        a[file[16:20]] = {}\n",
    "    if file[8:12] not in a[file[16:20]]:\n",
    "        a[file[16:20]][file[8:12]] = []\n",
    "    a[file[16:20]][file[8:12]].append(file)\n",
    "    \n",
    "    if file[8:12] not in p:\n",
    "        p[file[8:12]] = set()\n",
    "    p[file[8:12]].add(file[16:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(samples):\n",
    "    x, y = [], []\n",
    "    for _ in range(samples):\n",
    "        # sample two random p\n",
    "        p1, p2 = random.sample(list(p.keys()), 2)\n",
    "        # find overlapping a\n",
    "        a1 = p[p1]\n",
    "        a2 = p[p2]\n",
    "        a12 = a1.intersection(a2)\n",
    "        if len(a12) == 0:\n",
    "            continue\n",
    "        # sample two random a\n",
    "        a1, a2 = random.sample(list(a12), 2)\n",
    "        # sample x and y\n",
    "        x1 = random.sample(a[a1][p1], 1)[0]\n",
    "        x2 = random.sample(a[a2][p2], 1)[0]\n",
    "        y1 = random.sample(a[a1][p2], 1)[0]\n",
    "        y2 = random.sample(a[a2][p1], 1)[0]\n",
    "        x.append([x1, x2])\n",
    "        y.append([y1, y2])\n",
    "    return x, y\n",
    "\n",
    "batch_size = 16\n",
    "train_x, train_y = gen_samples(1000)\n",
    "val_x, val_y = gen_samples(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        actors = [self.X[index][0][8:12], self.X[index][1][8:12]]\n",
    "        actions = [self.X[index][0][16:20], self.X[index][1][16:20]]\n",
    "        return X[self.X[index][0]], X[self.X[index][1]], X[self.y[index][0]],  X[self.y[index][1]], actors, actions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data(train_x, train_y)\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = Data(val_x, val_y)\n",
    "val_dl = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'NTU'\n",
    "seg = 75\n",
    "lr = 1e-4\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SGN.model import SGN\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Conv1d(in_channels=75, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc2 = nn.Conv1d(in_channels=96, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc3 = nn.Conv1d(in_channels=128, out_channels=160, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc4 = nn.Conv1d(in_channels=160, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.enc4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.ConvTranspose1d(in_channels=384, out_channels=160, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose1d(in_channels=160, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(in_channels=96, out_channels=75, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.ref1 = nn.ReflectionPad1d(3)\n",
    "        self.ref2 = nn.ReflectionPad1d(3)\n",
    "        self.ref3 = nn.ReflectionPad1d(3)\n",
    "        self.ref4 = nn.ReflectionPad1d(3)\n",
    " \n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up75 = nn.Upsample(size=75, mode='nearest') \n",
    "\n",
    "        self.acti = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ref1(x)\n",
    "        x = self.acti(self.dec1(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref2(x)\n",
    "        x = self.acti(self.dec2(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref3(x)\n",
    "        x = self.acti(self.dec3(x))\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = self.ref4(x)\n",
    "        x = self.acti(self.dec4(x))\n",
    "        x = self.up75(x)\n",
    "        return x\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.static_encoder = Encoder()\n",
    "        self.dynamic_encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        # Adversarial Models\n",
    "        # self.utility = SGN(120, dataset, seg, batch_size, 1) # Action Recognition\n",
    "        # self.privacy = SGN(106, dataset, seg, batch_size, 1) # Re-Identification\n",
    "\n",
    "        self.end_effectors_ = [ # from ntu, (joint idx-1, kinematic chain length)\n",
    "            (19, 5), # left foot\n",
    "            (15, 5), # right foot\n",
    "            (23, 8), # left hand tip\n",
    "            (24, 8), # left thumb\n",
    "            (21, 8), # right hand tip\n",
    "            (22, 8), # right thumb\n",
    "            (3, 5)   # head\n",
    "        ]\n",
    "\n",
    "        self.end_effectors = torch.tensor([19, 15, 23, 24, 21, 22, 3]).cuda() * 3\n",
    "        self.chain_lengths = torch.tensor([5, 5, 8, 8, 8, 8, 5]).cuda()\n",
    "        self.weights = torch.tensor([1./5, 1./5, 1./8, 1./8, 1./8, 1./8, 1./5]).cuda()\n",
    "\n",
    "    def cross(self, x1, x2):\n",
    "        d1 = self.dynamic_encoder(x1)\n",
    "        d2 = self.dynamic_encoder(x2)\n",
    "        s1 = self.static_encoder(x1)\n",
    "        s2 = self.static_encoder(x2)\n",
    "        \n",
    "        out1 = self.decoder(torch.cat((d1, s1), dim=1))\n",
    "        out2 = self.decoder(torch.cat((d2, s2), dim=1))\n",
    "        out12 = self.decoder(torch.cat((d1, s2), dim=1))\n",
    "        out21 = self.decoder(torch.cat((d2, s1), dim=1))\n",
    "\n",
    "        return out1, out2, out12, out21\n",
    "    \n",
    "    def loss(self, x1, x2, y1, y2, actors, actions):\n",
    "        d1 = self.dynamic_encoder(x1)\n",
    "        d2 = self.dynamic_encoder(x2)\n",
    "        s1 = self.static_encoder(x1)\n",
    "        s2 = self.static_encoder(x2)\n",
    "\n",
    "        out1 = self.decoder(torch.cat((d1, s1), dim=1))\n",
    "        out2 = self.decoder(torch.cat((d2, s2), dim=1))\n",
    "        out12 = self.decoder(torch.cat((d1, s2), dim=1))\n",
    "        out21 = self.decoder(torch.cat((d2, s1), dim=1))\n",
    "        \n",
    "        # reconstruction loss\n",
    "        rec_loss = self.reconstruction_loss(x1, out1) + self.reconstruction_loss(x2, out2)\n",
    "        # print('Reconstruction Loss: ', rec_loss.item())\n",
    "        \n",
    "        # cross reconstruction loss\n",
    "        cross_loss = self.cross_reconstruction_loss(x1, x2, out12, out21)\n",
    "        # print('Cross Reconstruction Loss: ', cross_loss.item())\n",
    "        \n",
    "        # end effector loss\n",
    "        end_effector_loss = self.end_effector_loss(out1, x1) + self.end_effector_loss(out2, x2) + self.end_effector_loss(out12, x2) + self.end_effector_loss(out21, x1)\n",
    "        # print('End Effector Loss: ', end_effector_loss.item())\n",
    "\n",
    "        # triplet loss\n",
    "        # sgn latent privacy loss (adversarial)\n",
    "        # sgn latent utility loss (adversarial)\n",
    "\n",
    "        return rec_loss + cross_loss + end_effector_loss\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        # return torch.square(torch.norm(x - y, dim=1)).mean()\n",
    "        return F.mse_loss(x, y)\n",
    "    \n",
    "    def cross_reconstruction_loss(self, x1, x2, y1, y2):\n",
    "        return F.mse_loss(x1, y2) + F.mse_loss(x2, y1)\n",
    "        # return torch.square(torch.norm(x1 - y2, dim=1)).mean() + torch.square(torch.norm(x2 - y1, dim=1)).mean()\n",
    "    \n",
    "    def end_effector_loss(self, x, y):\n",
    "        # slice to get the end effector joints\n",
    "        x_ee = x[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).cuda()] \n",
    "        y_ee = y[:, :, self.end_effectors.unsqueeze(-1) + torch.arange(3).cuda()]\n",
    "\n",
    "        # calculate velocities\n",
    "        x_vel = torch.norm(x_ee[:, 1:] - x_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        y_vel = torch.norm(y_ee[:, 1:] - y_ee[:, :-1], dim=-1) / self.chain_lengths.unsqueeze(0)\n",
    "        \n",
    "        # compute mse loss for each joint\n",
    "        losses = F.mse_loss(x_vel, y_vel, reduction='none')\n",
    "\n",
    "        # take sum over end effectors\n",
    "        loss = losses.sum(dim=1)\n",
    "\n",
    "        # take mean over batch\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        dyn = self.dynamic_encoder(x)   \n",
    "        sta = self.static_encoder(x)\n",
    "        x = self.decoder(torch.cat((dyn, sta), dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Loss: 0.1145985740072587\n",
      "Epoch 1/100 Validation Loss: 0.0939013184979558\n",
      "Epoch 2/100 Loss: 0.09222336814684026\n",
      "Epoch 2/100 Validation Loss: 0.10371694155037403\n",
      "Epoch 3/100 Loss: 0.0923240180401241\n",
      "Epoch 3/100 Validation Loss: 0.08904875256121159\n",
      "Epoch 4/100 Loss: 0.08755642274285064\n",
      "Epoch 4/100 Validation Loss: 0.09087808709591627\n",
      "Epoch 5/100 Loss: 0.08544388161424328\n",
      "Epoch 5/100 Validation Loss: 0.0916650490835309\n",
      "Epoch 6/100 Loss: 0.08607951924204826\n",
      "Epoch 6/100 Validation Loss: 0.14908325299620628\n",
      "Epoch 7/100 Loss: 0.11639690508737284\n",
      "Epoch 7/100 Validation Loss: 0.09761720057576895\n",
      "Epoch 8/100 Loss: 0.08506756626507815\n",
      "Epoch 8/100 Validation Loss: 0.08533747680485249\n",
      "Epoch 9/100 Loss: 0.08162144505802323\n",
      "Epoch 9/100 Validation Loss: 0.08846228383481503\n",
      "Epoch 10/100 Loss: 0.08067770378992838\n",
      "Epoch 10/100 Validation Loss: 0.08669823966920376\n",
      "Epoch 11/100 Loss: 0.0803046391948181\n",
      "Epoch 11/100 Validation Loss: 0.08417336083948612\n",
      "Epoch 12/100 Loss: 0.07762494562741588\n",
      "Epoch 12/100 Validation Loss: 0.08057351596653461\n",
      "Epoch 13/100 Loss: 0.07621990801656947\n",
      "Epoch 13/100 Validation Loss: 0.08408214244991541\n",
      "Epoch 14/100 Loss: 0.08209974757012199\n",
      "Epoch 14/100 Validation Loss: 0.0831857779994607\n",
      "Epoch 15/100 Loss: 0.07675064136000241\n",
      "Epoch 15/100 Validation Loss: 0.08186809904873371\n",
      "Epoch 16/100 Loss: 0.0798544964807875\n",
      "Epoch 16/100 Validation Loss: 0.08347674319520593\n",
      "Epoch 17/100 Loss: 0.08331483273821719\n",
      "Epoch 17/100 Validation Loss: 0.09194979816675186\n",
      "Epoch 18/100 Loss: 0.07659588447388481\n",
      "Epoch 18/100 Validation Loss: 0.07836240157485008\n",
      "Epoch 19/100 Loss: 0.07645943114424453\n",
      "Epoch 19/100 Validation Loss: 0.0793892489746213\n",
      "Epoch 20/100 Loss: 0.0772711907458656\n",
      "Epoch 20/100 Validation Loss: 0.08358775544911623\n",
      "Epoch 21/100 Loss: 0.0783695001155138\n",
      "Epoch 21/100 Validation Loss: 0.09975416492670774\n",
      "Epoch 22/100 Loss: 0.07829232371467001\n",
      "Epoch 22/100 Validation Loss: 0.08371866960078478\n",
      "Epoch 23/100 Loss: 0.07372436990194461\n",
      "Epoch 23/100 Validation Loss: 0.08245874103158712\n",
      "Epoch 24/100 Loss: 0.07465558849713382\n",
      "Epoch 24/100 Validation Loss: 0.07628214685246348\n",
      "Epoch 25/100 Loss: 0.07054323878358393\n",
      "Epoch 25/100 Validation Loss: 0.07620022632181644\n",
      "Epoch 26/100 Loss: 0.0713820517720545\n",
      "Epoch 26/100 Validation Loss: 0.07557047810405493\n",
      "Epoch 27/100 Loss: 0.07843590571599848\n",
      "Epoch 27/100 Validation Loss: 0.07477196771651506\n",
      "Epoch 28/100 Loss: 0.0713921170681715\n",
      "Epoch 28/100 Validation Loss: 0.08091123402118683\n",
      "Epoch 29/100 Loss: 0.07150973270044607\n",
      "Epoch 29/100 Validation Loss: 0.07391606410965323\n",
      "Epoch 30/100 Loss: 0.06839524078018525\n",
      "Epoch 30/100 Validation Loss: 0.07247353717684746\n",
      "Epoch 31/100 Loss: 0.06789019440903384\n",
      "Epoch 31/100 Validation Loss: 0.07197512267157435\n",
      "Epoch 32/100 Loss: 0.06855792004395933\n",
      "Epoch 32/100 Validation Loss: 0.07887824065983295\n",
      "Epoch 33/100 Loss: 0.07003479933037478\n",
      "Epoch 33/100 Validation Loss: 0.07123599341139197\n",
      "Epoch 34/100 Loss: 0.06805436488460093\n",
      "Epoch 34/100 Validation Loss: 0.06974557461217046\n",
      "Epoch 35/100 Loss: 0.06716407178079381\n",
      "Epoch 35/100 Validation Loss: 0.07313785701990128\n",
      "Epoch 36/100 Loss: 0.06862414628267288\n",
      "Epoch 36/100 Validation Loss: 0.07198299700394273\n",
      "Epoch 37/100 Loss: 0.07033467062694185\n",
      "Epoch 37/100 Validation Loss: 0.07450972637161613\n",
      "Epoch 38/100 Loss: 0.06874376688809956\n",
      "Epoch 38/100 Validation Loss: 0.07026630220934749\n",
      "Epoch 39/100 Loss: 0.0667136691729812\n",
      "Epoch 39/100 Validation Loss: 0.07084222882986069\n",
      "Epoch 40/100 Loss: 0.06696178150527618\n",
      "Epoch 40/100 Validation Loss: 0.07567412266507745\n",
      "Epoch 41/100 Loss: 0.0674744891550611\n",
      "Epoch 41/100 Validation Loss: 0.07094579143449664\n",
      "Epoch 42/100 Loss: 0.06612323542289875\n",
      "Epoch 42/100 Validation Loss: 0.07835003454238176\n",
      "Epoch 43/100 Loss: 0.06775426842710551\n",
      "Epoch 43/100 Validation Loss: 0.07083377381786704\n",
      "Epoch 44/100 Loss: 0.066554353955914\n",
      "Epoch 44/100 Validation Loss: 0.06967756291851401\n",
      "Epoch 45/100 Loss: 0.06574548408389091\n",
      "Epoch 45/100 Validation Loss: 0.07176077784970403\n",
      "Epoch 46/100 Loss: 0.06727856770157814\n",
      "Epoch 46/100 Validation Loss: 0.07180477911606431\n",
      "Epoch 47/100 Loss: 0.06550018675625324\n",
      "Epoch 47/100 Validation Loss: 0.07559927692636847\n",
      "Epoch 48/100 Loss: 0.06523012402741347\n",
      "Epoch 48/100 Validation Loss: 0.07456660224124789\n",
      "Epoch 49/100 Loss: 0.06569909260553472\n",
      "Epoch 49/100 Validation Loss: 0.068842105101794\n",
      "Epoch 50/100 Loss: 0.06404341571033001\n",
      "Epoch 50/100 Validation Loss: 0.06859742384403944\n",
      "Epoch 51/100 Loss: 0.06443436367108542\n",
      "Epoch 51/100 Validation Loss: 0.08377062482759356\n",
      "Epoch 52/100 Loss: 0.06590494097155683\n",
      "Epoch 52/100 Validation Loss: 0.06833305023610592\n",
      "Epoch 53/100 Loss: 0.06896232386284015\n",
      "Epoch 53/100 Validation Loss: 0.07815742958337069\n",
      "Epoch 54/100 Loss: 0.06832760649130624\n",
      "Epoch 54/100 Validation Loss: 0.06977616250514984\n",
      "Epoch 55/100 Loss: 0.06367243453860283\n",
      "Epoch 55/100 Validation Loss: 0.06859687063843012\n",
      "Epoch 56/100 Loss: 0.06254822771777124\n",
      "Epoch 56/100 Validation Loss: 0.068808957003057\n",
      "Epoch 57/100 Loss: 0.06215754764921525\n",
      "Epoch 57/100 Validation Loss: 0.06738463789224625\n",
      "Epoch 58/100 Loss: 0.06522308794014595\n",
      "Epoch 58/100 Validation Loss: 0.08120152447372675\n",
      "Epoch 59/100 Loss: 0.0657632768373279\n",
      "Epoch 59/100 Validation Loss: 0.07024302054196596\n",
      "Epoch 60/100 Loss: 0.06452026941320475\n",
      "Epoch 60/100 Validation Loss: 0.07161844568327069\n",
      "Epoch 61/100 Loss: 0.06395759312983822\n",
      "Epoch 61/100 Validation Loss: 0.0692089176736772\n",
      "Epoch 62/100 Loss: 0.061479875817894936\n",
      "Epoch 62/100 Validation Loss: 0.06561306957155466\n",
      "Epoch 63/100 Loss: 0.061482657623641634\n",
      "Epoch 63/100 Validation Loss: 0.06867165863513947\n",
      "Epoch 64/100 Loss: 0.06269361834753961\n",
      "Epoch 64/100 Validation Loss: 0.06767891813069582\n",
      "Epoch 65/100 Loss: 0.06273804013343419\n",
      "Epoch 65/100 Validation Loss: 0.07052935846149921\n",
      "Epoch 66/100 Loss: 0.06165893737445859\n",
      "Epoch 66/100 Validation Loss: 0.06624259008094668\n",
      "Epoch 67/100 Loss: 0.06283577597316574\n",
      "Epoch 67/100 Validation Loss: 0.07529092719778419\n",
      "Epoch 68/100 Loss: 0.07797417756827439\n",
      "Epoch 68/100 Validation Loss: 0.07297416171059012\n",
      "Epoch 69/100 Loss: 0.06430137639536578\n",
      "Epoch 69/100 Validation Loss: 0.06606324529275298\n",
      "Epoch 70/100 Loss: 0.06120722445056719\n",
      "Epoch 70/100 Validation Loss: 0.06498934561386704\n",
      "Epoch 71/100 Loss: 0.061486969099325293\n",
      "Epoch 71/100 Validation Loss: 0.06616499554365873\n",
      "Epoch 72/100 Loss: 0.06012916137628695\n",
      "Epoch 72/100 Validation Loss: 0.06416065711528063\n",
      "Epoch 73/100 Loss: 0.059135061195668054\n",
      "Epoch 73/100 Validation Loss: 0.06579063972458243\n",
      "Epoch 74/100 Loss: 0.05945139476919875\n",
      "Epoch 74/100 Validation Loss: 0.06421555997803807\n",
      "Epoch 75/100 Loss: 0.05985905975103378\n",
      "Epoch 75/100 Validation Loss: 0.06368324859067798\n",
      "Epoch 76/100 Loss: 0.05901447146692697\n",
      "Epoch 76/100 Validation Loss: 0.0637559536844492\n",
      "Epoch 77/100 Loss: 0.059073986595167834\n",
      "Epoch 77/100 Validation Loss: 0.06585378060117364\n",
      "Epoch 78/100 Loss: 0.05883851077626733\n",
      "Epoch 78/100 Validation Loss: 0.06825954467058182\n",
      "Epoch 79/100 Loss: 0.05896829879459213\n",
      "Epoch 79/100 Validation Loss: 0.06342606293037534\n",
      "Epoch 80/100 Loss: 0.05864378053914098\n",
      "Epoch 80/100 Validation Loss: 0.06444580480456352\n",
      "Epoch 81/100 Loss: 0.06004016441019142\n",
      "Epoch 81/100 Validation Loss: 0.0688158837147057\n",
      "Epoch 82/100 Loss: 0.05998424594016636\n",
      "Epoch 82/100 Validation Loss: 0.06583665637299418\n",
      "Epoch 83/100 Loss: 0.06092494265998111\n",
      "Epoch 83/100 Validation Loss: 0.06310574663802981\n",
      "Epoch 84/100 Loss: 0.058830116272849196\n",
      "Epoch 84/100 Validation Loss: 0.06817377917468548\n",
      "Epoch 85/100 Loss: 0.05910829903886599\n",
      "Epoch 85/100 Validation Loss: 0.06308183493092656\n",
      "Epoch 86/100 Loss: 0.057915679891319835\n",
      "Epoch 86/100 Validation Loss: 0.06448254734277725\n",
      "Epoch 87/100 Loss: 0.05843961940092199\n",
      "Epoch 87/100 Validation Loss: 0.06440800800919533\n",
      "Epoch 88/100 Loss: 0.05781452018110191\n",
      "Epoch 88/100 Validation Loss: 0.062214098405092955\n",
      "Epoch 89/100 Loss: 0.05976931827471537\n",
      "Epoch 89/100 Validation Loss: 0.06838054489344358\n",
      "Epoch 90/100 Loss: 0.0588444625849233\n",
      "Epoch 90/100 Validation Loss: 0.0647956826724112\n",
      "Epoch 91/100 Loss: 0.057500244720893746\n",
      "Epoch 91/100 Validation Loss: 0.06447487184777856\n",
      "Epoch 92/100 Loss: 0.0581028930404607\n",
      "Epoch 92/100 Validation Loss: 0.06484719086438417\n",
      "Epoch 93/100 Loss: 0.05831922601689311\n",
      "Epoch 93/100 Validation Loss: 0.06611961079761386\n",
      "Epoch 94/100 Loss: 0.05862767512307448\n",
      "Epoch 94/100 Validation Loss: 0.0631774952635169\n",
      "Epoch 95/100 Loss: 0.05755914692931315\n",
      "Epoch 95/100 Validation Loss: 0.06380260083824396\n",
      "Epoch 96/100 Loss: 0.05756798638578724\n",
      "Epoch 96/100 Validation Loss: 0.06195661844685674\n",
      "Epoch 97/100 Loss: 0.056427844854838705\n",
      "Epoch 97/100 Validation Loss: 0.06420717388391495\n",
      "Epoch 98/100 Loss: 0.05607066226794439\n",
      "Epoch 98/100 Validation Loss: 0.06506394501775503\n",
      "Epoch 99/100 Loss: 0.05754857477458084\n",
      "Epoch 99/100 Validation Loss: 0.06352361431345344\n",
      "Epoch 100/100 Loss: 0.05674457111779381\n",
      "Epoch 100/100 Validation Loss: 0.06351232714951038\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for (x1, x2, y1, y2, actors, actions) in train_dl:\n",
    "        # Move tensors to the configured device\n",
    "        x1, x2, y1, y2 = x1.float().cuda(), x2.float().cuda(), y1.float().cuda(), y2.float().cuda()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss = model.loss(x1, x2, y1, y2, actors, actions)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Decay learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the training loss\n",
    "    print(f'Epoch {epoch+1}/{epochs} Loss: {np.mean(losses)}')\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for (x1, x2, y1, y2, actors, actions) in val_dl:\n",
    "            x1, x2, y1, y2 = x1.float().cuda(), x2.float().cuda(), y1.float().cuda(), y2.float().cuda()\n",
    "            loss = model.loss(x1, x2, y1, y2, actors, actions)\n",
    "            val_losses.append(loss.item())\n",
    "        print(f'Epoch {epoch+1}/{epochs} Validation Loss: {np.mean(val_losses)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
